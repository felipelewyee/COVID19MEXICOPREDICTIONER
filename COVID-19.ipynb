{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red para COVID-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es una red secuencial simple para tratar de predecir el dato de número de contagiados de COVID19 en México que se da a las 19:00 horas por parte del gobierno.\n",
    "\n",
    "**Para ejecutar la predicción:**\n",
    "\n",
    "**1)** Ejecutar processing.sh, este script descarga el archivo time_series_covid19_confirmed_global.csv con datos de contagiados por país. Este archivo se actualiza en internet aproximadamente a las 6:30pm con datos del día anterior, hay que revisar que esté actualizado a la fecha del día de la predicción.\n",
    "\n",
    "chmod +x processing.sh\n",
    "\n",
    "./processing.sh\n",
    "\n",
    "**2)** Abrir jupyter notebook (se requiere keras, tensorflow, pandas y numpy instaldos).\n",
    "\n",
    "jupyter notebook COVID-19.ipynb\n",
    "\n",
    "**3)** Actualizar dias_a_usar y dia_a_predecir. Como referencia, el 30 de marzo fue el día 33 de infección en México, por lo que dia_a_predecir=33 y dias_a_usar=32.\n",
    "\n",
    "**4)** Ejecutar todos los cuadros.\n",
    "\n",
    "**Creado por Juan Felipe Huan Lew Yee, Neftalí Isaí Rodríguez Rojas y Jorge Martín del Campo Ramírez.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos el dia que queremos la prediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dia_a_predecir = 50\n",
    "dias_a_usar = 49"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la lista de paises que analizaremos. (Más abajo se filtrarán los que no tengan suficientes días)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_namelist = ['Afghanistan', 'Albania', 'Algeria', 'Angola', 'Andorra', 'Antigua and Barbuda', 'Argentina', 'Armenia', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Benin', 'Bhutan', 'Bolivia', 'Bosnia and Herzegovina', 'Brazil', 'Brunei', 'Bulgaria', 'Burkina Faso', 'Cabo Verde', 'Cambodia', 'Cameroon', 'Canada', 'Central African Republic', 'Chad', 'Chile', 'China', 'Colombia', 'Congo (Brazzaville)', 'Congo (Kinshasa)', 'Costa Rica', \"Cote d'Ivoire\", 'Croatia', 'Cuba', 'Cyprus', 'Czechia', 'Denmark', 'Djibouti', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Eswatini', 'Ethiopia', 'Fiji', 'Finland', 'France', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Greece', 'Guatemala', 'Guinea', 'Guyana', 'Haiti', 'Honduras', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran', 'Iraq', 'Ireland', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jordan', 'Kazakhstan', 'Kenya', 'Korea South', 'Kuwait', 'Kyrgyzstan', 'Latvia', 'Lebanon', 'Liberia', 'Lithuania', 'Luxembourg', 'Madagascar', 'Malaysia', 'Maldives', 'Malta', 'Mauritania', 'Mauritius', 'Moldova', 'Monaco', 'Mongolia', 'Montenegro', 'Morocco', 'Namibia', 'Nepal', 'Netherlands', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'North Macedonia', 'Norway', 'Oman', 'Pakistan', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', 'Poland', 'Portugal', 'Qatar', 'Romania', 'Russia', 'Rwanda', 'Saint Lucia', 'Saint Vincent and the Grenadines', 'San Marino', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Singapore', 'Slovakia', 'Slovenia', 'Somalia', 'South Africa', 'Spain', 'Sri Lanka', 'Sudan', 'Suriname', 'Sweden', 'Switzerland', 'Tanzania', 'Thailand', 'Togo', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'Uruguay', 'US', 'Uzbekistan', 'Venezuela', 'Vietnam', 'Zambia', 'Zimbabwe', 'Dominica', 'Grenada', 'Mozambique', 'Syria', 'Timor-Leste', 'Belize', 'Laos', 'Libya', 'Guinea-Bissau', 'Mali', 'Saint Kitts and Nevis', 'Burma', 'Botswana', 'Burundi', 'Sierra Leone', 'Malawi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtramos los países que no tengan el número de días necesarios para predecir a México, los países a usar al menos deben de tener dia_a_predecir días de infección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = open('database.csv','w')\n",
    "\n",
    "for value in range(dias_a_usar):\n",
    "    print(value+1,file=db,end = ',')\n",
    "print(dia_a_predecir,file=db)\n",
    "\n",
    "for country in country_namelist:\n",
    "    \n",
    "    f = open('database_confirmed.csv')\n",
    "    infected = []\n",
    "    dia_inicio = 0\n",
    "    for line in f:\n",
    "        if(country == line.split(',')[1]):\n",
    "            country_data = []\n",
    "            data = line.replace('\\n','').split(',')[4:]\n",
    "            for number in data:\n",
    "                if(number != '0'):\n",
    "                    infected.append(int(number))\n",
    "                else:\n",
    "                    dia_inicio += 1                    \n",
    "    f.close()\n",
    "\n",
    "    if(len(infected)<=dia_a_predecir):\n",
    "        #print(country,len(infected))\n",
    "        continue\n",
    "    print(country,end=',',file=db,sep=',')\n",
    "    for value in infected[:dias_a_usar]:\n",
    "        print(value,file=db,end = ',')\n",
    "    print(infected[dia_a_predecir-1],file=db)\n",
    "db.close()          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leamos la base de datos que acabamos de crear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"database.csv\",sep=',') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimimos la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Afghanistan</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>299</td>\n",
       "      <td>349</td>\n",
       "      <td>367</td>\n",
       "      <td>423</td>\n",
       "      <td>444</td>\n",
       "      <td>484</td>\n",
       "      <td>521</td>\n",
       "      <td>555</td>\n",
       "      <td>607</td>\n",
       "      <td>665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algeria</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>1320</td>\n",
       "      <td>1423</td>\n",
       "      <td>1468</td>\n",
       "      <td>1572</td>\n",
       "      <td>1666</td>\n",
       "      <td>1761</td>\n",
       "      <td>1825</td>\n",
       "      <td>1914</td>\n",
       "      <td>1983</td>\n",
       "      <td>2070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Australia</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>63</td>\n",
       "      <td>76</td>\n",
       "      <td>91</td>\n",
       "      <td>107</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>200</td>\n",
       "      <td>250</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Austria</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>12051</td>\n",
       "      <td>12297</td>\n",
       "      <td>12639</td>\n",
       "      <td>12942</td>\n",
       "      <td>13244</td>\n",
       "      <td>13555</td>\n",
       "      <td>13806</td>\n",
       "      <td>13945</td>\n",
       "      <td>14041</td>\n",
       "      <td>14226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bahrain</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>36</td>\n",
       "      <td>41</td>\n",
       "      <td>47</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>52</td>\n",
       "      <td>...</td>\n",
       "      <td>688</td>\n",
       "      <td>700</td>\n",
       "      <td>756</td>\n",
       "      <td>811</td>\n",
       "      <td>823</td>\n",
       "      <td>887</td>\n",
       "      <td>925</td>\n",
       "      <td>1040</td>\n",
       "      <td>1136</td>\n",
       "      <td>1361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Belgium</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>886</td>\n",
       "      <td>1058</td>\n",
       "      <td>1243</td>\n",
       "      <td>1486</td>\n",
       "      <td>1795</td>\n",
       "      <td>2257</td>\n",
       "      <td>2815</td>\n",
       "      <td>3401</td>\n",
       "      <td>3743</td>\n",
       "      <td>4269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cambodia</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Canada</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>49</td>\n",
       "      <td>54</td>\n",
       "      <td>64</td>\n",
       "      <td>77</td>\n",
       "      <td>79</td>\n",
       "      <td>108</td>\n",
       "      <td>117</td>\n",
       "      <td>193</td>\n",
       "      <td>198</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>China</th>\n",
       "      <td>548</td>\n",
       "      <td>643</td>\n",
       "      <td>920</td>\n",
       "      <td>1406</td>\n",
       "      <td>2075</td>\n",
       "      <td>2877</td>\n",
       "      <td>5509</td>\n",
       "      <td>6087</td>\n",
       "      <td>8141</td>\n",
       "      <td>9802</td>\n",
       "      <td>...</td>\n",
       "      <td>80136</td>\n",
       "      <td>80261</td>\n",
       "      <td>80386</td>\n",
       "      <td>80537</td>\n",
       "      <td>80690</td>\n",
       "      <td>80770</td>\n",
       "      <td>80823</td>\n",
       "      <td>80860</td>\n",
       "      <td>80887</td>\n",
       "      <td>80921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Croatia</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>1182</td>\n",
       "      <td>1222</td>\n",
       "      <td>1282</td>\n",
       "      <td>1343</td>\n",
       "      <td>1407</td>\n",
       "      <td>1495</td>\n",
       "      <td>1534</td>\n",
       "      <td>1600</td>\n",
       "      <td>1650</td>\n",
       "      <td>1704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Egypt</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>456</td>\n",
       "      <td>495</td>\n",
       "      <td>536</td>\n",
       "      <td>576</td>\n",
       "      <td>609</td>\n",
       "      <td>656</td>\n",
       "      <td>710</td>\n",
       "      <td>779</td>\n",
       "      <td>865</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Finland</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>155</td>\n",
       "      <td>225</td>\n",
       "      <td>244</td>\n",
       "      <td>277</td>\n",
       "      <td>321</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>France</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>288</td>\n",
       "      <td>380</td>\n",
       "      <td>656</td>\n",
       "      <td>959</td>\n",
       "      <td>1136</td>\n",
       "      <td>1219</td>\n",
       "      <td>1794</td>\n",
       "      <td>2293</td>\n",
       "      <td>2293</td>\n",
       "      <td>3681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Germany</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>799</td>\n",
       "      <td>1040</td>\n",
       "      <td>1176</td>\n",
       "      <td>1457</td>\n",
       "      <td>1908</td>\n",
       "      <td>2078</td>\n",
       "      <td>3675</td>\n",
       "      <td>4585</td>\n",
       "      <td>5795</td>\n",
       "      <td>7272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>India</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>56</td>\n",
       "      <td>62</td>\n",
       "      <td>73</td>\n",
       "      <td>82</td>\n",
       "      <td>102</td>\n",
       "      <td>113</td>\n",
       "      <td>119</td>\n",
       "      <td>142</td>\n",
       "      <td>156</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iran</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>28</td>\n",
       "      <td>43</td>\n",
       "      <td>61</td>\n",
       "      <td>95</td>\n",
       "      <td>139</td>\n",
       "      <td>245</td>\n",
       "      <td>388</td>\n",
       "      <td>...</td>\n",
       "      <td>41495</td>\n",
       "      <td>44605</td>\n",
       "      <td>47593</td>\n",
       "      <td>50468</td>\n",
       "      <td>53183</td>\n",
       "      <td>55743</td>\n",
       "      <td>58226</td>\n",
       "      <td>60500</td>\n",
       "      <td>62589</td>\n",
       "      <td>64586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iraq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>878</td>\n",
       "      <td>961</td>\n",
       "      <td>1031</td>\n",
       "      <td>1122</td>\n",
       "      <td>1202</td>\n",
       "      <td>1232</td>\n",
       "      <td>1279</td>\n",
       "      <td>1318</td>\n",
       "      <td>1352</td>\n",
       "      <td>1378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Israel</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>6092</td>\n",
       "      <td>6857</td>\n",
       "      <td>7428</td>\n",
       "      <td>7851</td>\n",
       "      <td>8430</td>\n",
       "      <td>8904</td>\n",
       "      <td>9248</td>\n",
       "      <td>9404</td>\n",
       "      <td>9968</td>\n",
       "      <td>10408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italy</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>12462</td>\n",
       "      <td>12462</td>\n",
       "      <td>17660</td>\n",
       "      <td>21157</td>\n",
       "      <td>24747</td>\n",
       "      <td>27980</td>\n",
       "      <td>31506</td>\n",
       "      <td>35713</td>\n",
       "      <td>41035</td>\n",
       "      <td>47021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Japan</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>274</td>\n",
       "      <td>293</td>\n",
       "      <td>331</td>\n",
       "      <td>360</td>\n",
       "      <td>420</td>\n",
       "      <td>461</td>\n",
       "      <td>502</td>\n",
       "      <td>511</td>\n",
       "      <td>581</td>\n",
       "      <td>639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korea South</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>4335</td>\n",
       "      <td>5186</td>\n",
       "      <td>5621</td>\n",
       "      <td>6088</td>\n",
       "      <td>6593</td>\n",
       "      <td>7041</td>\n",
       "      <td>7314</td>\n",
       "      <td>7478</td>\n",
       "      <td>7513</td>\n",
       "      <td>7755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kuwait</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>43</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>...</td>\n",
       "      <td>479</td>\n",
       "      <td>556</td>\n",
       "      <td>665</td>\n",
       "      <td>743</td>\n",
       "      <td>855</td>\n",
       "      <td>910</td>\n",
       "      <td>993</td>\n",
       "      <td>1154</td>\n",
       "      <td>1234</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lebanon</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>479</td>\n",
       "      <td>494</td>\n",
       "      <td>508</td>\n",
       "      <td>520</td>\n",
       "      <td>527</td>\n",
       "      <td>541</td>\n",
       "      <td>548</td>\n",
       "      <td>576</td>\n",
       "      <td>582</td>\n",
       "      <td>609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Malaysia</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>50</td>\n",
       "      <td>83</td>\n",
       "      <td>93</td>\n",
       "      <td>99</td>\n",
       "      <td>117</td>\n",
       "      <td>129</td>\n",
       "      <td>149</td>\n",
       "      <td>149</td>\n",
       "      <td>197</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nepal</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oman</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>277</td>\n",
       "      <td>298</td>\n",
       "      <td>331</td>\n",
       "      <td>371</td>\n",
       "      <td>419</td>\n",
       "      <td>457</td>\n",
       "      <td>484</td>\n",
       "      <td>546</td>\n",
       "      <td>599</td>\n",
       "      <td>727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Philippines</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>49</td>\n",
       "      <td>52</td>\n",
       "      <td>64</td>\n",
       "      <td>111</td>\n",
       "      <td>140</td>\n",
       "      <td>142</td>\n",
       "      <td>187</td>\n",
       "      <td>202</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Russia</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>28</td>\n",
       "      <td>45</td>\n",
       "      <td>59</td>\n",
       "      <td>63</td>\n",
       "      <td>90</td>\n",
       "      <td>114</td>\n",
       "      <td>147</td>\n",
       "      <td>199</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Singapore</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>117</td>\n",
       "      <td>130</td>\n",
       "      <td>138</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>160</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spain</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2277</td>\n",
       "      <td>5232</td>\n",
       "      <td>6391</td>\n",
       "      <td>7798</td>\n",
       "      <td>9942</td>\n",
       "      <td>11748</td>\n",
       "      <td>13910</td>\n",
       "      <td>17963</td>\n",
       "      <td>20410</td>\n",
       "      <td>25374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sri Lanka</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sweden</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>500</td>\n",
       "      <td>599</td>\n",
       "      <td>814</td>\n",
       "      <td>961</td>\n",
       "      <td>1022</td>\n",
       "      <td>1103</td>\n",
       "      <td>1190</td>\n",
       "      <td>1279</td>\n",
       "      <td>1439</td>\n",
       "      <td>1639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Switzerland</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>27</td>\n",
       "      <td>42</td>\n",
       "      <td>56</td>\n",
       "      <td>90</td>\n",
       "      <td>114</td>\n",
       "      <td>...</td>\n",
       "      <td>21100</td>\n",
       "      <td>21657</td>\n",
       "      <td>22253</td>\n",
       "      <td>23280</td>\n",
       "      <td>24051</td>\n",
       "      <td>24551</td>\n",
       "      <td>25107</td>\n",
       "      <td>25415</td>\n",
       "      <td>25688</td>\n",
       "      <td>25936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Thailand</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>47</td>\n",
       "      <td>48</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>53</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United Arab Emirates</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>74</td>\n",
       "      <td>74</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United Kingdom</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>459</td>\n",
       "      <td>459</td>\n",
       "      <td>802</td>\n",
       "      <td>1144</td>\n",
       "      <td>1145</td>\n",
       "      <td>1551</td>\n",
       "      <td>1960</td>\n",
       "      <td>2642</td>\n",
       "      <td>2716</td>\n",
       "      <td>4014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>98</td>\n",
       "      <td>118</td>\n",
       "      <td>149</td>\n",
       "      <td>217</td>\n",
       "      <td>262</td>\n",
       "      <td>402</td>\n",
       "      <td>518</td>\n",
       "      <td>583</td>\n",
       "      <td>959</td>\n",
       "      <td>1281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vietnam</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        1    2    3     4     5     6     7     8     9    10  \\\n",
       "Afghanistan             1    1    1     1     1     1     1     1     1     1   \n",
       "Algeria                 1    1    1     1     1     1     3     5    12    12   \n",
       "Australia               4    5    5     6     9     9    12    12    12    13   \n",
       "Austria                 2    2    3     3     9    14    18    21    29    41   \n",
       "Bahrain                 1   23   33    33    36    41    47    49    49    52   \n",
       "Belgium                 1    1    1     1     1     1     1     1     1     1   \n",
       "Cambodia                1    1    1     1     1     1     1     1     1     1   \n",
       "Canada                  1    1    2     2     2     4     4     4     4     4   \n",
       "China                 548  643  920  1406  2075  2877  5509  6087  8141  9802   \n",
       "Croatia                 1    3    3     5     6     7     7     9    10    10   \n",
       "Egypt                   1    1    1     1     1     1     1     1     1     1   \n",
       "Finland                 1    1    1     1     1     1     1     1     1     1   \n",
       "France                  2    3    3     3     4     5     5     5     6     6   \n",
       "Germany                 1    4    4     4     5     8    10    12    12    12   \n",
       "India                   1    1    1     2     3     3     3     3     3     3   \n",
       "Iran                    2    5   18    28    43    61    95   139   245   388   \n",
       "Iraq                    1    1    5     7     7    13    19    26    32    35   \n",
       "Israel                  1    1    1     1     1     2     3     4     7    10   \n",
       "Italy                   2    2    2     2     2     2     2     3     3     3   \n",
       "Japan                   2    2    2     2     4     4     7     7    11    15   \n",
       "Korea South             1    1    2     2     3     4     4     4     4    11   \n",
       "Kuwait                  1   11   26    43    45    45    45    56    56    56   \n",
       "Lebanon                 1    1    1     1     1     2     2     2     4    10   \n",
       "Malaysia                3    4    4     4     7     8     8     8     8     8   \n",
       "Nepal                   1    1    1     1     1     1     1     1     1     1   \n",
       "Oman                    2    2    4     4     4     6     6     6    12    15   \n",
       "Philippines             1    1    1     2     2     2     2     2     3     3   \n",
       "Russia                  2    2    2     2     2     2     2     2     2     2   \n",
       "Singapore               1    3    3     4     5     7     7    10    13    16   \n",
       "Spain                   1    1    1     1     1     1     1     1     2     2   \n",
       "Sri Lanka               1    1    1     1     1     1     1     1     1     1   \n",
       "Sweden                  1    1    1     1     1     1     1     1     1     1   \n",
       "Switzerland             1    1    8     8    18    27    42    56    90   114   \n",
       "Thailand                2    3    5     7     8     8    14    14    14    19   \n",
       "United Arab Emirates    4    4    4     4     5     5     5     5     5     5   \n",
       "United Kingdom          2    2    2     2     2     2     2     3     3     3   \n",
       "US                      1    1    2     2     5     5     5     5     5     7   \n",
       "Vietnam                 2    2    2     2     2     2     2     2     2     6   \n",
       "\n",
       "                      ...     41     42     43     44     45     46     47  \\\n",
       "Afghanistan           ...    299    349    367    423    444    484    521   \n",
       "Algeria               ...   1320   1423   1468   1572   1666   1761   1825   \n",
       "Australia             ...     60     63     76     91    107    128    128   \n",
       "Austria               ...  12051  12297  12639  12942  13244  13555  13806   \n",
       "Bahrain               ...    688    700    756    811    823    887    925   \n",
       "Belgium               ...    886   1058   1243   1486   1795   2257   2815   \n",
       "Cambodia              ...      1      2      2      2      3      3      5   \n",
       "Canada                ...     49     54     64     77     79    108    117   \n",
       "China                 ...  80136  80261  80386  80537  80690  80770  80823   \n",
       "Croatia               ...   1182   1222   1282   1343   1407   1495   1534   \n",
       "Egypt                 ...    456    495    536    576    609    656    710   \n",
       "Finland               ...     30     40     59     59    155    225    244   \n",
       "France                ...    288    380    656    959   1136   1219   1794   \n",
       "Germany               ...    799   1040   1176   1457   1908   2078   3675   \n",
       "India                 ...     56     62     73     82    102    113    119   \n",
       "Iran                  ...  41495  44605  47593  50468  53183  55743  58226   \n",
       "Iraq                  ...    878    961   1031   1122   1202   1232   1279   \n",
       "Israel                ...   6092   6857   7428   7851   8430   8904   9248   \n",
       "Italy                 ...  12462  12462  17660  21157  24747  27980  31506   \n",
       "Japan                 ...    274    293    331    360    420    461    502   \n",
       "Korea South           ...   4335   5186   5621   6088   6593   7041   7314   \n",
       "Kuwait                ...    479    556    665    743    855    910    993   \n",
       "Lebanon               ...    479    494    508    520    527    541    548   \n",
       "Malaysia              ...     50     83     93     99    117    129    149   \n",
       "Nepal                 ...      1      1      1      1      1      1      1   \n",
       "Oman                  ...    277    298    331    371    419    457    484   \n",
       "Philippines           ...     33     49     52     64    111    140    142   \n",
       "Russia                ...     20     28     45     59     63     90    114   \n",
       "Singapore             ...    110    110    117    130    138    150    150   \n",
       "Spain                 ...   2277   5232   6391   7798   9942  11748  13910   \n",
       "Sri Lanka             ...      1      1      1      1      2      2      6   \n",
       "Sweden                ...    500    599    814    961   1022   1103   1190   \n",
       "Switzerland           ...  21100  21657  22253  23280  24051  24551  25107   \n",
       "Thailand              ...     43     43     43     47     48     50     50   \n",
       "United Arab Emirates  ...     45     74     74     85     85     85     98   \n",
       "United Kingdom        ...    459    459    802   1144   1145   1551   1960   \n",
       "US                    ...     98    118    149    217    262    402    518   \n",
       "Vietnam               ...     16     16     16     16     18     30     30   \n",
       "\n",
       "                         48     49     50  \n",
       "Afghanistan             555    607    665  \n",
       "Algeria                1914   1983   2070  \n",
       "Australia               200    250    297  \n",
       "Austria               13945  14041  14226  \n",
       "Bahrain                1040   1136   1361  \n",
       "Belgium                3401   3743   4269  \n",
       "Cambodia                  7      7      7  \n",
       "Canada                  193    198    252  \n",
       "China                 80860  80887  80921  \n",
       "Croatia                1600   1650   1704  \n",
       "Egypt                   779    865    985  \n",
       "Finland                 277    321    336  \n",
       "France                 2293   2293   3681  \n",
       "Germany                4585   5795   7272  \n",
       "India                   142    156    194  \n",
       "Iran                  60500  62589  64586  \n",
       "Iraq                   1318   1352   1378  \n",
       "Israel                 9404   9968  10408  \n",
       "Italy                 35713  41035  47021  \n",
       "Japan                   511    581    639  \n",
       "Korea South            7478   7513   7755  \n",
       "Kuwait                 1154   1234   1300  \n",
       "Lebanon                 576    582    609  \n",
       "Malaysia                149    197    238  \n",
       "Nepal                     1      1      1  \n",
       "Oman                    546    599    727  \n",
       "Philippines             187    202    217  \n",
       "Russia                  147    199    253  \n",
       "Singapore               160    178    178  \n",
       "Spain                 17963  20410  25374  \n",
       "Sri Lanka                10     18     28  \n",
       "Sweden                 1279   1439   1639  \n",
       "Switzerland           25415  25688  25936  \n",
       "Thailand                 50     53     59  \n",
       "United Arab Emirates     98     98    113  \n",
       "United Kingdom         2642   2716   4014  \n",
       "US                      583    959   1281  \n",
       "Vietnam                  31     38     39  \n",
       "\n",
       "[38 rows x 50 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un conjunto X y un conjunto Y y dividimos train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 49) (36, 1)\n",
      "(2, 49) (2, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = pd.DataFrame()\n",
    "for i in range(1,dias_a_usar+1):\n",
    "    X[str(i)] = data[str(i)]\n",
    "Y = pd.DataFrame()\n",
    "Y[str(dia_a_predecir)] = data[str(dia_a_predecir)]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.05)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=dias_a_usar+0, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilamos la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='MAPE', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36 samples, validate on 2 samples\n",
      "Epoch 1/1500\n",
      "36/36 [==============================] - 1s 36ms/step - loss: 104.6179 - val_loss: 87.3494\n",
      "Epoch 2/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 95.2476 - val_loss: 69.9702\n",
      "Epoch 3/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 85.8478 - val_loss: 50.0053\n",
      "Epoch 4/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 74.7330 - val_loss: 25.8791\n",
      "Epoch 5/1500\n",
      "36/36 [==============================] - 0s 575us/step - loss: 63.8340 - val_loss: 36.0799\n",
      "Epoch 6/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 50.2528 - val_loss: 46.0342\n",
      "Epoch 7/1500\n",
      "36/36 [==============================] - 0s 763us/step - loss: 39.4277 - val_loss: 77.8150\n",
      "Epoch 8/1500\n",
      "36/36 [==============================] - 0s 448us/step - loss: 33.8712 - val_loss: 97.8283\n",
      "Epoch 9/1500\n",
      "36/36 [==============================] - 0s 399us/step - loss: 34.4208 - val_loss: 104.7762\n",
      "Epoch 10/1500\n",
      "36/36 [==============================] - 0s 446us/step - loss: 34.4584 - val_loss: 93.9008\n",
      "Epoch 11/1500\n",
      "36/36 [==============================] - 0s 482us/step - loss: 30.8628 - val_loss: 69.4659\n",
      "Epoch 12/1500\n",
      "36/36 [==============================] - 0s 513us/step - loss: 24.9267 - val_loss: 39.6526\n",
      "Epoch 13/1500\n",
      "36/36 [==============================] - 0s 537us/step - loss: 20.6602 - val_loss: 23.6260\n",
      "Epoch 14/1500\n",
      "36/36 [==============================] - 0s 536us/step - loss: 18.3430 - val_loss: 23.3903\n",
      "Epoch 15/1500\n",
      "36/36 [==============================] - 0s 450us/step - loss: 15.1906 - val_loss: 23.9016\n",
      "Epoch 16/1500\n",
      "36/36 [==============================] - 0s 485us/step - loss: 12.8529 - val_loss: 17.2365\n",
      "Epoch 17/1500\n",
      "36/36 [==============================] - 0s 480us/step - loss: 12.2896 - val_loss: 7.2953\n",
      "Epoch 18/1500\n",
      "36/36 [==============================] - 0s 554us/step - loss: 13.1598 - val_loss: 1.2933\n",
      "Epoch 19/1500\n",
      "36/36 [==============================] - 0s 480us/step - loss: 13.6098 - val_loss: 1.1595\n",
      "Epoch 20/1500\n",
      "36/36 [==============================] - 0s 574us/step - loss: 13.3538 - val_loss: 3.3426\n",
      "Epoch 21/1500\n",
      "36/36 [==============================] - 0s 539us/step - loss: 12.4016 - val_loss: 15.7378\n",
      "Epoch 22/1500\n",
      "36/36 [==============================] - 0s 690us/step - loss: 13.2651 - val_loss: 18.5220\n",
      "Epoch 23/1500\n",
      "36/36 [==============================] - 0s 498us/step - loss: 12.8536 - val_loss: 12.3262\n",
      "Epoch 24/1500\n",
      "36/36 [==============================] - 0s 491us/step - loss: 11.6489 - val_loss: 9.6459\n",
      "Epoch 25/1500\n",
      "36/36 [==============================] - 0s 525us/step - loss: 12.3645 - val_loss: 12.7873\n",
      "Epoch 26/1500\n",
      "36/36 [==============================] - 0s 740us/step - loss: 11.0513 - val_loss: 19.2498\n",
      "Epoch 27/1500\n",
      "36/36 [==============================] - 0s 674us/step - loss: 11.1187 - val_loss: 14.4737\n",
      "Epoch 28/1500\n",
      "36/36 [==============================] - 0s 544us/step - loss: 10.0520 - val_loss: 7.1710\n",
      "Epoch 29/1500\n",
      "36/36 [==============================] - 0s 614us/step - loss: 10.2778 - val_loss: 10.2881\n",
      "Epoch 30/1500\n",
      "36/36 [==============================] - 0s 550us/step - loss: 9.6781 - val_loss: 15.5360\n",
      "Epoch 31/1500\n",
      "36/36 [==============================] - 0s 750us/step - loss: 10.1242 - val_loss: 13.2805\n",
      "Epoch 32/1500\n",
      "36/36 [==============================] - 0s 682us/step - loss: 9.4815 - val_loss: 12.7784\n",
      "Epoch 33/1500\n",
      "36/36 [==============================] - 0s 889us/step - loss: 9.2444 - val_loss: 14.0235\n",
      "Epoch 34/1500\n",
      "36/36 [==============================] - 0s 803us/step - loss: 9.1879 - val_loss: 8.1418\n",
      "Epoch 35/1500\n",
      "36/36 [==============================] - 0s 568us/step - loss: 9.2095 - val_loss: 6.7017\n",
      "Epoch 36/1500\n",
      "36/36 [==============================] - 0s 700us/step - loss: 10.5379 - val_loss: 7.5177\n",
      "Epoch 37/1500\n",
      "36/36 [==============================] - ETA: 0s - loss: 10.00 - 0s 528us/step - loss: 9.6771 - val_loss: 13.8410\n",
      "Epoch 38/1500\n",
      "36/36 [==============================] - 0s 682us/step - loss: 8.6259 - val_loss: 12.5601\n",
      "Epoch 39/1500\n",
      "36/36 [==============================] - 0s 565us/step - loss: 8.6800 - val_loss: 10.5651\n",
      "Epoch 40/1500\n",
      "36/36 [==============================] - 0s 892us/step - loss: 8.7565 - val_loss: 11.2719\n",
      "Epoch 41/1500\n",
      "36/36 [==============================] - 0s 862us/step - loss: 8.7148 - val_loss: 13.9797\n",
      "Epoch 42/1500\n",
      "36/36 [==============================] - 0s 636us/step - loss: 9.2087 - val_loss: 11.2155\n",
      "Epoch 43/1500\n",
      "36/36 [==============================] - 0s 471us/step - loss: 8.6431 - val_loss: 6.2444\n",
      "Epoch 44/1500\n",
      "36/36 [==============================] - 0s 537us/step - loss: 9.6498 - val_loss: 12.4761\n",
      "Epoch 45/1500\n",
      "36/36 [==============================] - 0s 595us/step - loss: 8.2784 - val_loss: 18.9851\n",
      "Epoch 46/1500\n",
      "36/36 [==============================] - 0s 521us/step - loss: 9.2509 - val_loss: 12.2341\n",
      "Epoch 47/1500\n",
      "36/36 [==============================] - 0s 695us/step - loss: 8.2196 - val_loss: 10.2036\n",
      "Epoch 48/1500\n",
      "36/36 [==============================] - 0s 838us/step - loss: 8.2144 - val_loss: 17.8483\n",
      "Epoch 49/1500\n",
      "36/36 [==============================] - ETA: 0s - loss: 8.846 - 0s 374us/step - loss: 8.1043 - val_loss: 21.1203\n",
      "Epoch 50/1500\n",
      "36/36 [==============================] - 0s 587us/step - loss: 8.8965 - val_loss: 19.0458\n",
      "Epoch 51/1500\n",
      "36/36 [==============================] - 0s 501us/step - loss: 8.5871 - val_loss: 14.5203\n",
      "Epoch 52/1500\n",
      "36/36 [==============================] - 0s 673us/step - loss: 8.1341 - val_loss: 15.5458\n",
      "Epoch 53/1500\n",
      "36/36 [==============================] - 0s 415us/step - loss: 8.0886 - val_loss: 15.5613\n",
      "Epoch 54/1500\n",
      "36/36 [==============================] - 0s 604us/step - loss: 7.9373 - val_loss: 11.4282\n",
      "Epoch 55/1500\n",
      "36/36 [==============================] - 0s 518us/step - loss: 7.6070 - val_loss: 14.1581\n",
      "Epoch 56/1500\n",
      "36/36 [==============================] - 0s 450us/step - loss: 7.3992 - val_loss: 15.2539\n",
      "Epoch 57/1500\n",
      "36/36 [==============================] - 0s 371us/step - loss: 7.5441 - val_loss: 15.2510\n",
      "Epoch 58/1500\n",
      "36/36 [==============================] - 0s 463us/step - loss: 7.4845 - val_loss: 11.4598\n",
      "Epoch 59/1500\n",
      "36/36 [==============================] - 0s 472us/step - loss: 6.9076 - val_loss: 6.7315\n",
      "Epoch 60/1500\n",
      "36/36 [==============================] - 0s 397us/step - loss: 11.2671 - val_loss: 8.2473\n",
      "Epoch 61/1500\n",
      "36/36 [==============================] - 0s 356us/step - loss: 9.1889 - val_loss: 27.5650\n",
      "Epoch 62/1500\n",
      "36/36 [==============================] - 0s 370us/step - loss: 11.0088 - val_loss: 37.2560\n",
      "Epoch 63/1500\n",
      "36/36 [==============================] - 0s 543us/step - loss: 14.4876 - val_loss: 24.0720\n",
      "Epoch 64/1500\n",
      "36/36 [==============================] - 0s 390us/step - loss: 8.6795 - val_loss: 12.9619\n",
      "Epoch 65/1500\n",
      "36/36 [==============================] - 0s 500us/step - loss: 10.2151 - val_loss: 12.6015\n",
      "Epoch 66/1500\n",
      "36/36 [==============================] - 0s 473us/step - loss: 8.5295 - val_loss: 20.6928\n",
      "Epoch 67/1500\n",
      "36/36 [==============================] - 0s 394us/step - loss: 7.9672 - val_loss: 24.8195\n",
      "Epoch 68/1500\n",
      "36/36 [==============================] - 0s 490us/step - loss: 10.1745 - val_loss: 18.9222\n",
      "Epoch 69/1500\n",
      "36/36 [==============================] - 0s 527us/step - loss: 7.5510 - val_loss: 6.3647\n",
      "Epoch 70/1500\n",
      "36/36 [==============================] - 0s 501us/step - loss: 9.5009 - val_loss: 4.7480\n",
      "Epoch 71/1500\n",
      "36/36 [==============================] - 0s 428us/step - loss: 11.6252 - val_loss: 9.6493\n",
      "Epoch 72/1500\n",
      "36/36 [==============================] - 0s 437us/step - loss: 7.0937 - val_loss: 21.4946\n",
      "Epoch 73/1500\n",
      "36/36 [==============================] - 0s 609us/step - loss: 10.4496 - val_loss: 17.8221\n",
      "Epoch 74/1500\n",
      "36/36 [==============================] - 0s 660us/step - loss: 8.6355 - val_loss: 8.2413\n",
      "Epoch 75/1500\n",
      "36/36 [==============================] - 0s 421us/step - loss: 7.5430 - val_loss: 9.5538\n",
      "Epoch 76/1500\n",
      "36/36 [==============================] - 0s 502us/step - loss: 7.0000 - val_loss: 17.9384\n",
      "Epoch 77/1500\n",
      "36/36 [==============================] - 0s 416us/step - loss: 7.4662 - val_loss: 14.6543\n",
      "Epoch 78/1500\n",
      "36/36 [==============================] - 0s 491us/step - loss: 6.4466 - val_loss: 8.4315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/1500\n",
      "36/36 [==============================] - 0s 450us/step - loss: 7.8315 - val_loss: 13.5689\n",
      "Epoch 80/1500\n",
      "36/36 [==============================] - 0s 524us/step - loss: 5.9978 - val_loss: 20.4179\n",
      "Epoch 81/1500\n",
      "36/36 [==============================] - 0s 591us/step - loss: 8.4126 - val_loss: 15.2778\n",
      "Epoch 82/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 6.9575 - val_loss: 7.0902\n",
      "Epoch 83/1500\n",
      "36/36 [==============================] - 0s 573us/step - loss: 7.8957 - val_loss: 13.7412\n",
      "Epoch 84/1500\n",
      "36/36 [==============================] - 0s 614us/step - loss: 5.8018 - val_loss: 19.9559\n",
      "Epoch 85/1500\n",
      "36/36 [==============================] - 0s 946us/step - loss: 7.9785 - val_loss: 14.6726\n",
      "Epoch 86/1500\n",
      "36/36 [==============================] - 0s 361us/step - loss: 6.4135 - val_loss: 7.5494\n",
      "Epoch 87/1500\n",
      "36/36 [==============================] - 0s 435us/step - loss: 8.0296 - val_loss: 12.7052\n",
      "Epoch 88/1500\n",
      "36/36 [==============================] - 0s 438us/step - loss: 6.4156 - val_loss: 20.9392\n",
      "Epoch 89/1500\n",
      "36/36 [==============================] - 0s 416us/step - loss: 8.2370 - val_loss: 17.9901\n",
      "Epoch 90/1500\n",
      "36/36 [==============================] - 0s 544us/step - loss: 6.5784 - val_loss: 12.7281\n",
      "Epoch 91/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 7.1333 - val_loss: 18.5561\n",
      "Epoch 92/1500\n",
      "36/36 [==============================] - 0s 873us/step - loss: 7.1324 - val_loss: 20.3434\n",
      "Epoch 93/1500\n",
      "36/36 [==============================] - 0s 325us/step - loss: 7.1320 - val_loss: 15.8816\n",
      "Epoch 94/1500\n",
      "36/36 [==============================] - 0s 705us/step - loss: 6.2590 - val_loss: 10.0128\n",
      "Epoch 95/1500\n",
      "36/36 [==============================] - 0s 489us/step - loss: 6.4826 - val_loss: 8.5583\n",
      "Epoch 96/1500\n",
      "36/36 [==============================] - 0s 526us/step - loss: 6.2753 - val_loss: 15.0340\n",
      "Epoch 97/1500\n",
      "36/36 [==============================] - 0s 382us/step - loss: 6.9330 - val_loss: 16.6750\n",
      "Epoch 98/1500\n",
      "36/36 [==============================] - 0s 491us/step - loss: 7.0325 - val_loss: 11.5517\n",
      "Epoch 99/1500\n",
      "36/36 [==============================] - 0s 411us/step - loss: 5.6285 - val_loss: 9.3349\n",
      "Epoch 100/1500\n",
      "36/36 [==============================] - 0s 483us/step - loss: 6.2967 - val_loss: 13.2427\n",
      "Epoch 101/1500\n",
      "36/36 [==============================] - 0s 424us/step - loss: 5.3655 - val_loss: 17.7224\n",
      "Epoch 102/1500\n",
      "36/36 [==============================] - 0s 458us/step - loss: 6.4473 - val_loss: 16.0391\n",
      "Epoch 103/1500\n",
      "36/36 [==============================] - 0s 402us/step - loss: 5.7523 - val_loss: 10.1241\n",
      "Epoch 104/1500\n",
      "36/36 [==============================] - 0s 446us/step - loss: 5.6794 - val_loss: 7.5387\n",
      "Epoch 105/1500\n",
      "36/36 [==============================] - 0s 460us/step - loss: 6.3429 - val_loss: 9.0744\n",
      "Epoch 106/1500\n",
      "36/36 [==============================] - 0s 496us/step - loss: 5.6122 - val_loss: 12.1808\n",
      "Epoch 107/1500\n",
      "36/36 [==============================] - 0s 446us/step - loss: 5.5473 - val_loss: 11.7728\n",
      "Epoch 108/1500\n",
      "36/36 [==============================] - 0s 421us/step - loss: 5.2189 - val_loss: 12.0767\n",
      "Epoch 109/1500\n",
      "36/36 [==============================] - 0s 530us/step - loss: 5.2008 - val_loss: 10.0047\n",
      "Epoch 110/1500\n",
      "36/36 [==============================] - 0s 424us/step - loss: 5.4024 - val_loss: 11.6555\n",
      "Epoch 111/1500\n",
      "36/36 [==============================] - 0s 416us/step - loss: 4.9048 - val_loss: 12.7418\n",
      "Epoch 112/1500\n",
      "36/36 [==============================] - 0s 585us/step - loss: 5.8308 - val_loss: 10.5546\n",
      "Epoch 113/1500\n",
      "36/36 [==============================] - 0s 335us/step - loss: 6.2362 - val_loss: 8.4428\n",
      "Epoch 114/1500\n",
      "36/36 [==============================] - 0s 349us/step - loss: 5.6782 - val_loss: 8.1702\n",
      "Epoch 115/1500\n",
      "36/36 [==============================] - 0s 465us/step - loss: 5.1120 - val_loss: 9.0577\n",
      "Epoch 116/1500\n",
      "36/36 [==============================] - 0s 436us/step - loss: 4.7874 - val_loss: 19.7041\n",
      "Epoch 117/1500\n",
      "36/36 [==============================] - 0s 470us/step - loss: 6.6534 - val_loss: 16.6856\n",
      "Epoch 118/1500\n",
      "36/36 [==============================] - 0s 428us/step - loss: 5.9914 - val_loss: 9.3411\n",
      "Epoch 119/1500\n",
      "36/36 [==============================] - 0s 437us/step - loss: 7.3449 - val_loss: 10.8090\n",
      "Epoch 120/1500\n",
      "36/36 [==============================] - 0s 423us/step - loss: 5.8017 - val_loss: 17.5460\n",
      "Epoch 121/1500\n",
      "36/36 [==============================] - 0s 404us/step - loss: 7.5042 - val_loss: 16.9568\n",
      "Epoch 122/1500\n",
      "36/36 [==============================] - 0s 519us/step - loss: 7.4038 - val_loss: 3.7757\n",
      "Epoch 123/1500\n",
      "36/36 [==============================] - 0s 468us/step - loss: 7.0155 - val_loss: 2.2422\n",
      "Epoch 124/1500\n",
      "36/36 [==============================] - 0s 556us/step - loss: 8.2061 - val_loss: 10.6634\n",
      "Epoch 125/1500\n",
      "36/36 [==============================] - 0s 425us/step - loss: 5.5593 - val_loss: 17.3293\n",
      "Epoch 126/1500\n",
      "36/36 [==============================] - 0s 412us/step - loss: 7.0029 - val_loss: 12.7652\n",
      "Epoch 127/1500\n",
      "36/36 [==============================] - 0s 360us/step - loss: 5.4914 - val_loss: 9.9270\n",
      "Epoch 128/1500\n",
      "36/36 [==============================] - 0s 354us/step - loss: 5.3399 - val_loss: 14.3395\n",
      "Epoch 129/1500\n",
      "36/36 [==============================] - 0s 364us/step - loss: 5.1031 - val_loss: 16.4510\n",
      "Epoch 130/1500\n",
      "36/36 [==============================] - 0s 422us/step - loss: 4.6364 - val_loss: 10.9198\n",
      "Epoch 131/1500\n",
      "36/36 [==============================] - 0s 446us/step - loss: 7.1526 - val_loss: 13.2877\n",
      "Epoch 132/1500\n",
      "36/36 [==============================] - 0s 502us/step - loss: 7.5588 - val_loss: 22.9381\n",
      "Epoch 133/1500\n",
      "36/36 [==============================] - 0s 504us/step - loss: 6.1640 - val_loss: 29.6108\n",
      "Epoch 134/1500\n",
      "36/36 [==============================] - 0s 515us/step - loss: 7.8361 - val_loss: 26.5955\n",
      "Epoch 135/1500\n",
      "36/36 [==============================] - 0s 427us/step - loss: 6.5996 - val_loss: 14.5119\n",
      "Epoch 136/1500\n",
      "36/36 [==============================] - 0s 406us/step - loss: 7.6997 - val_loss: 11.4808\n",
      "Epoch 137/1500\n",
      "36/36 [==============================] - 0s 434us/step - loss: 9.0872 - val_loss: 15.1460\n",
      "Epoch 138/1500\n",
      "36/36 [==============================] - 0s 487us/step - loss: 4.6595 - val_loss: 21.4883\n",
      "Epoch 139/1500\n",
      "36/36 [==============================] - 0s 441us/step - loss: 7.3297 - val_loss: 18.2189\n",
      "Epoch 140/1500\n",
      "36/36 [==============================] - 0s 396us/step - loss: 5.7557 - val_loss: 12.7765\n",
      "Epoch 141/1500\n",
      "36/36 [==============================] - 0s 414us/step - loss: 5.0532 - val_loss: 11.0661\n",
      "Epoch 142/1500\n",
      "36/36 [==============================] - 0s 387us/step - loss: 5.5903 - val_loss: 10.9187\n",
      "Epoch 143/1500\n",
      "36/36 [==============================] - 0s 448us/step - loss: 5.0567 - val_loss: 13.3896\n",
      "Epoch 144/1500\n",
      "36/36 [==============================] - 0s 393us/step - loss: 5.0235 - val_loss: 15.4329\n",
      "Epoch 145/1500\n",
      "36/36 [==============================] - 0s 466us/step - loss: 6.0426 - val_loss: 8.3549\n",
      "Epoch 146/1500\n",
      "36/36 [==============================] - 0s 544us/step - loss: 4.7336 - val_loss: 4.8158\n",
      "Epoch 147/1500\n",
      "36/36 [==============================] - 0s 439us/step - loss: 6.1075 - val_loss: 12.7440\n",
      "Epoch 148/1500\n",
      "36/36 [==============================] - 0s 537us/step - loss: 4.6187 - val_loss: 20.3910\n",
      "Epoch 149/1500\n",
      "36/36 [==============================] - 0s 646us/step - loss: 6.3917 - val_loss: 18.8015\n",
      "Epoch 150/1500\n",
      "36/36 [==============================] - 0s 658us/step - loss: 5.4797 - val_loss: 11.7223\n",
      "Epoch 151/1500\n",
      "36/36 [==============================] - 0s 685us/step - loss: 6.4950 - val_loss: 7.8763\n",
      "Epoch 152/1500\n",
      "36/36 [==============================] - 0s 751us/step - loss: 9.1930 - val_loss: 9.8024\n",
      "Epoch 153/1500\n",
      "36/36 [==============================] - 0s 558us/step - loss: 6.4579 - val_loss: 19.8058\n",
      "Epoch 154/1500\n",
      "36/36 [==============================] - 0s 560us/step - loss: 6.3268 - val_loss: 16.9233\n",
      "Epoch 155/1500\n",
      "36/36 [==============================] - ETA: 0s - loss: 5.680 - 0s 661us/step - loss: 5.2589 - val_loss: 6.4178\n",
      "Epoch 156/1500\n",
      "36/36 [==============================] - 0s 504us/step - loss: 5.5447 - val_loss: 9.3243\n",
      "Epoch 157/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - ETA: 0s - loss: 4.272 - 0s 563us/step - loss: 4.1183 - val_loss: 19.1785\n",
      "Epoch 158/1500\n",
      "36/36 [==============================] - 0s 654us/step - loss: 6.0306 - val_loss: 18.5580\n",
      "Epoch 159/1500\n",
      "36/36 [==============================] - 0s 356us/step - loss: 5.5226 - val_loss: 11.4756\n",
      "Epoch 160/1500\n",
      "36/36 [==============================] - 0s 544us/step - loss: 5.8030 - val_loss: 8.4270\n",
      "Epoch 161/1500\n",
      "36/36 [==============================] - 0s 451us/step - loss: 6.1944 - val_loss: 11.6805\n",
      "Epoch 162/1500\n",
      "36/36 [==============================] - 0s 507us/step - loss: 4.3582 - val_loss: 13.6533\n",
      "Epoch 163/1500\n",
      "36/36 [==============================] - 0s 462us/step - loss: 5.7148 - val_loss: 9.0912\n",
      "Epoch 164/1500\n",
      "36/36 [==============================] - 0s 367us/step - loss: 4.8870 - val_loss: 5.0775\n",
      "Epoch 165/1500\n",
      "36/36 [==============================] - 0s 516us/step - loss: 5.7866 - val_loss: 6.6127\n",
      "Epoch 166/1500\n",
      "36/36 [==============================] - 0s 507us/step - loss: 5.1670 - val_loss: 13.2481\n",
      "Epoch 167/1500\n",
      "36/36 [==============================] - 0s 428us/step - loss: 5.0676 - val_loss: 10.6358\n",
      "Epoch 168/1500\n",
      "36/36 [==============================] - 0s 640us/step - loss: 4.6213 - val_loss: 11.2011\n",
      "Epoch 169/1500\n",
      "36/36 [==============================] - 0s 532us/step - loss: 4.9927 - val_loss: 15.8513\n",
      "Epoch 170/1500\n",
      "36/36 [==============================] - ETA: 0s - loss: 5.449 - 0s 533us/step - loss: 5.5756 - val_loss: 15.0273\n",
      "Epoch 171/1500\n",
      "36/36 [==============================] - 0s 436us/step - loss: 5.2301 - val_loss: 7.5939\n",
      "Epoch 172/1500\n",
      "36/36 [==============================] - 0s 402us/step - loss: 4.7411 - val_loss: 6.0310\n",
      "Epoch 173/1500\n",
      "36/36 [==============================] - 0s 423us/step - loss: 4.4961 - val_loss: 13.5470\n",
      "Epoch 174/1500\n",
      "36/36 [==============================] - 0s 589us/step - loss: 5.6614 - val_loss: 12.1943\n",
      "Epoch 175/1500\n",
      "36/36 [==============================] - 0s 632us/step - loss: 4.5498 - val_loss: 8.3816\n",
      "Epoch 176/1500\n",
      "36/36 [==============================] - 0s 472us/step - loss: 4.3499 - val_loss: 17.4926\n",
      "Epoch 177/1500\n",
      "36/36 [==============================] - 0s 610us/step - loss: 5.1721 - val_loss: 17.3601\n",
      "Epoch 178/1500\n",
      "36/36 [==============================] - ETA: 0s - loss: 4.478 - 0s 583us/step - loss: 4.5778 - val_loss: 13.6768\n",
      "Epoch 179/1500\n",
      "36/36 [==============================] - 0s 519us/step - loss: 4.7387 - val_loss: 14.2377\n",
      "Epoch 180/1500\n",
      "36/36 [==============================] - 0s 344us/step - loss: 4.1682 - val_loss: 14.7286\n",
      "Epoch 181/1500\n",
      "36/36 [==============================] - 0s 517us/step - loss: 4.7342 - val_loss: 14.9619\n",
      "Epoch 182/1500\n",
      "36/36 [==============================] - 0s 589us/step - loss: 5.7449 - val_loss: 6.9561\n",
      "Epoch 183/1500\n",
      "36/36 [==============================] - 0s 784us/step - loss: 5.1644 - val_loss: 4.5689\n",
      "Epoch 184/1500\n",
      "36/36 [==============================] - 0s 474us/step - loss: 7.2828 - val_loss: 9.2813\n",
      "Epoch 185/1500\n",
      "36/36 [==============================] - ETA: 0s - loss: 4.534 - 0s 363us/step - loss: 4.6548 - val_loss: 15.3482\n",
      "Epoch 186/1500\n",
      "36/36 [==============================] - 0s 493us/step - loss: 5.9418 - val_loss: 7.0350\n",
      "Epoch 187/1500\n",
      "36/36 [==============================] - 0s 481us/step - loss: 4.5818 - val_loss: 6.4836\n",
      "Epoch 188/1500\n",
      "36/36 [==============================] - 0s 326us/step - loss: 4.5956 - val_loss: 14.6351\n",
      "Epoch 189/1500\n",
      "36/36 [==============================] - 0s 487us/step - loss: 5.8374 - val_loss: 17.0750\n",
      "Epoch 190/1500\n",
      "36/36 [==============================] - 0s 356us/step - loss: 6.6869 - val_loss: 11.4559\n",
      "Epoch 191/1500\n",
      "36/36 [==============================] - 0s 466us/step - loss: 4.7513 - val_loss: 9.4525\n",
      "Epoch 192/1500\n",
      "36/36 [==============================] - 0s 362us/step - loss: 4.1592 - val_loss: 9.2973\n",
      "Epoch 193/1500\n",
      "36/36 [==============================] - 0s 445us/step - loss: 3.8337 - val_loss: 9.8535\n",
      "Epoch 194/1500\n",
      "36/36 [==============================] - 0s 675us/step - loss: 3.6633 - val_loss: 9.3484\n",
      "Epoch 195/1500\n",
      "36/36 [==============================] - 0s 535us/step - loss: 3.6060 - val_loss: 12.9612\n",
      "Epoch 196/1500\n",
      "36/36 [==============================] - 0s 499us/step - loss: 4.2216 - val_loss: 10.7812\n",
      "Epoch 197/1500\n",
      "36/36 [==============================] - 0s 507us/step - loss: 4.4524 - val_loss: 9.2303\n",
      "Epoch 198/1500\n",
      "36/36 [==============================] - 0s 641us/step - loss: 4.0604 - val_loss: 13.9124\n",
      "Epoch 199/1500\n",
      "36/36 [==============================] - 0s 595us/step - loss: 5.3183 - val_loss: 8.0729\n",
      "Epoch 200/1500\n",
      "36/36 [==============================] - 0s 457us/step - loss: 3.9162 - val_loss: 4.9668\n",
      "Epoch 201/1500\n",
      "36/36 [==============================] - 0s 745us/step - loss: 4.2314 - val_loss: 13.4996\n",
      "Epoch 202/1500\n",
      "36/36 [==============================] - ETA: 0s - loss: 5.457 - 0s 426us/step - loss: 5.7692 - val_loss: 12.1555\n",
      "Epoch 203/1500\n",
      "36/36 [==============================] - 0s 345us/step - loss: 5.4865 - val_loss: 4.1124\n",
      "Epoch 204/1500\n",
      "36/36 [==============================] - 0s 497us/step - loss: 4.6645 - val_loss: 5.9108\n",
      "Epoch 205/1500\n",
      "36/36 [==============================] - 0s 516us/step - loss: 4.3491 - val_loss: 10.4353\n",
      "Epoch 206/1500\n",
      "36/36 [==============================] - 0s 435us/step - loss: 4.3922 - val_loss: 7.8218\n",
      "Epoch 207/1500\n",
      "36/36 [==============================] - 0s 545us/step - loss: 4.1619 - val_loss: 9.0841\n",
      "Epoch 208/1500\n",
      "36/36 [==============================] - 0s 534us/step - loss: 3.4749 - val_loss: 13.6771\n",
      "Epoch 209/1500\n",
      "36/36 [==============================] - 0s 360us/step - loss: 3.7695 - val_loss: 12.0541\n",
      "Epoch 210/1500\n",
      "36/36 [==============================] - 0s 498us/step - loss: 4.3511 - val_loss: 11.4420\n",
      "Epoch 211/1500\n",
      "36/36 [==============================] - 0s 574us/step - loss: 4.3228 - val_loss: 19.2142\n",
      "Epoch 212/1500\n",
      "36/36 [==============================] - 0s 769us/step - loss: 6.2472 - val_loss: 17.5647\n",
      "Epoch 213/1500\n",
      "36/36 [==============================] - 0s 737us/step - loss: 5.7326 - val_loss: 5.8063\n",
      "Epoch 214/1500\n",
      "36/36 [==============================] - 0s 559us/step - loss: 4.7018 - val_loss: 4.4910\n",
      "Epoch 215/1500\n",
      "36/36 [==============================] - 0s 705us/step - loss: 4.8105 - val_loss: 14.7889\n",
      "Epoch 216/1500\n",
      "36/36 [==============================] - 0s 703us/step - loss: 5.5714 - val_loss: 21.4731\n",
      "Epoch 217/1500\n",
      "36/36 [==============================] - 0s 599us/step - loss: 9.2032 - val_loss: 15.9398\n",
      "Epoch 218/1500\n",
      "36/36 [==============================] - 0s 587us/step - loss: 5.4119 - val_loss: 6.9429\n",
      "Epoch 219/1500\n",
      "36/36 [==============================] - 0s 518us/step - loss: 4.1568 - val_loss: 13.7679\n",
      "Epoch 220/1500\n",
      "36/36 [==============================] - 0s 535us/step - loss: 4.2756 - val_loss: 15.4176\n",
      "Epoch 221/1500\n",
      "36/36 [==============================] - 0s 527us/step - loss: 4.9848 - val_loss: 11.5338\n",
      "Epoch 222/1500\n",
      "36/36 [==============================] - 0s 531us/step - loss: 3.0949 - val_loss: 13.5878\n",
      "Epoch 223/1500\n",
      "36/36 [==============================] - 0s 502us/step - loss: 3.9383 - val_loss: 10.1775\n",
      "Epoch 224/1500\n",
      "36/36 [==============================] - 0s 503us/step - loss: 3.3469 - val_loss: 7.7811\n",
      "Epoch 225/1500\n",
      "36/36 [==============================] - 0s 535us/step - loss: 3.4608 - val_loss: 15.7948\n",
      "Epoch 226/1500\n",
      "36/36 [==============================] - 0s 424us/step - loss: 5.9509 - val_loss: 11.6335\n",
      "Epoch 227/1500\n",
      "36/36 [==============================] - 0s 641us/step - loss: 3.8050 - val_loss: 5.4300\n",
      "Epoch 228/1500\n",
      "36/36 [==============================] - 0s 475us/step - loss: 5.8879 - val_loss: 8.1320\n",
      "Epoch 229/1500\n",
      "36/36 [==============================] - 0s 347us/step - loss: 3.7912 - val_loss: 14.1451\n",
      "Epoch 230/1500\n",
      "36/36 [==============================] - 0s 599us/step - loss: 4.7226 - val_loss: 10.8409\n",
      "Epoch 231/1500\n",
      "36/36 [==============================] - 0s 810us/step - loss: 3.7315 - val_loss: 7.6129\n",
      "Epoch 232/1500\n",
      "36/36 [==============================] - 0s 418us/step - loss: 4.5816 - val_loss: 10.9008\n",
      "Epoch 233/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 3.4227 - val_loss: 8.1654\n",
      "Epoch 234/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 719us/step - loss: 3.2259 - val_loss: 11.1814\n",
      "Epoch 235/1500\n",
      "36/36 [==============================] - 0s 428us/step - loss: 3.4672 - val_loss: 11.2778\n",
      "Epoch 236/1500\n",
      "36/36 [==============================] - 0s 364us/step - loss: 4.0202 - val_loss: 8.2540\n",
      "Epoch 237/1500\n",
      "36/36 [==============================] - 0s 384us/step - loss: 3.2212 - val_loss: 11.2948\n",
      "Epoch 238/1500\n",
      "36/36 [==============================] - 0s 347us/step - loss: 3.1033 - val_loss: 14.0995\n",
      "Epoch 239/1500\n",
      "36/36 [==============================] - 0s 377us/step - loss: 3.6886 - val_loss: 13.9210\n",
      "Epoch 240/1500\n",
      "36/36 [==============================] - 0s 364us/step - loss: 3.4931 - val_loss: 11.7598\n",
      "Epoch 241/1500\n",
      "36/36 [==============================] - 0s 453us/step - loss: 2.9791 - val_loss: 10.5039\n",
      "Epoch 242/1500\n",
      "36/36 [==============================] - 0s 323us/step - loss: 3.3584 - val_loss: 10.2724\n",
      "Epoch 243/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 3.5664 - val_loss: 8.9001\n",
      "Epoch 244/1500\n",
      "36/36 [==============================] - 0s 778us/step - loss: 3.4796 - val_loss: 6.0795\n",
      "Epoch 245/1500\n",
      "36/36 [==============================] - 0s 387us/step - loss: 4.0616 - val_loss: 13.3646\n",
      "Epoch 246/1500\n",
      "36/36 [==============================] - 0s 437us/step - loss: 4.4434 - val_loss: 15.2599\n",
      "Epoch 247/1500\n",
      "36/36 [==============================] - 0s 333us/step - loss: 4.6463 - val_loss: 8.1441\n",
      "Epoch 248/1500\n",
      "36/36 [==============================] - 0s 392us/step - loss: 3.8856 - val_loss: 14.5875\n",
      "Epoch 249/1500\n",
      "36/36 [==============================] - 0s 383us/step - loss: 4.1106 - val_loss: 17.5318\n",
      "Epoch 250/1500\n",
      "36/36 [==============================] - 0s 604us/step - loss: 5.4492 - val_loss: 11.3898\n",
      "Epoch 251/1500\n",
      "36/36 [==============================] - 0s 588us/step - loss: 3.0051 - val_loss: 5.4563\n",
      "Epoch 252/1500\n",
      "36/36 [==============================] - 0s 498us/step - loss: 4.8420 - val_loss: 8.7564\n",
      "Epoch 253/1500\n",
      "36/36 [==============================] - 0s 484us/step - loss: 3.5595 - val_loss: 12.6444\n",
      "Epoch 254/1500\n",
      "36/36 [==============================] - 0s 525us/step - loss: 4.6462 - val_loss: 6.8729\n",
      "Epoch 255/1500\n",
      "36/36 [==============================] - 0s 557us/step - loss: 4.0555 - val_loss: 10.8118\n",
      "Epoch 256/1500\n",
      "36/36 [==============================] - 0s 529us/step - loss: 3.2294 - val_loss: 16.9038\n",
      "Epoch 257/1500\n",
      "36/36 [==============================] - 0s 573us/step - loss: 5.0594 - val_loss: 14.9292\n",
      "Epoch 258/1500\n",
      "36/36 [==============================] - 0s 621us/step - loss: 4.0966 - val_loss: 8.7527\n",
      "Epoch 259/1500\n",
      "36/36 [==============================] - 0s 619us/step - loss: 3.9913 - val_loss: 8.9597\n",
      "Epoch 260/1500\n",
      "36/36 [==============================] - 0s 626us/step - loss: 2.8183 - val_loss: 11.1198\n",
      "Epoch 261/1500\n",
      "36/36 [==============================] - 0s 524us/step - loss: 3.5500 - val_loss: 9.0071\n",
      "Epoch 262/1500\n",
      "36/36 [==============================] - 0s 826us/step - loss: 3.5636 - val_loss: 7.6855\n",
      "Epoch 263/1500\n",
      "36/36 [==============================] - 0s 577us/step - loss: 3.4996 - val_loss: 8.2817\n",
      "Epoch 264/1500\n",
      "36/36 [==============================] - 0s 695us/step - loss: 3.2427 - val_loss: 12.8175\n",
      "Epoch 265/1500\n",
      "36/36 [==============================] - ETA: 0s - loss: 3.873 - 0s 657us/step - loss: 3.9316 - val_loss: 11.4796\n",
      "Epoch 266/1500\n",
      "36/36 [==============================] - 0s 698us/step - loss: 3.1336 - val_loss: 3.1811\n",
      "Epoch 267/1500\n",
      "36/36 [==============================] - 0s 722us/step - loss: 7.1106 - val_loss: 6.7257\n",
      "Epoch 268/1500\n",
      "36/36 [==============================] - 0s 783us/step - loss: 4.7102 - val_loss: 16.3330\n",
      "Epoch 269/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 5.4589 - val_loss: 13.6603\n",
      "Epoch 270/1500\n",
      "36/36 [==============================] - 0s 952us/step - loss: 3.8833 - val_loss: 5.0894\n",
      "Epoch 271/1500\n",
      "36/36 [==============================] - 0s 749us/step - loss: 5.3665 - val_loss: 6.5740\n",
      "Epoch 272/1500\n",
      "36/36 [==============================] - 0s 853us/step - loss: 4.8189 - val_loss: 11.8485\n",
      "Epoch 273/1500\n",
      "36/36 [==============================] - 0s 522us/step - loss: 4.7496 - val_loss: 9.4340\n",
      "Epoch 274/1500\n",
      "36/36 [==============================] - 0s 619us/step - loss: 3.9931 - val_loss: 9.0196\n",
      "Epoch 275/1500\n",
      "36/36 [==============================] - 0s 460us/step - loss: 3.9288 - val_loss: 14.4011\n",
      "Epoch 276/1500\n",
      "36/36 [==============================] - 0s 524us/step - loss: 3.7635 - val_loss: 15.7350\n",
      "Epoch 277/1500\n",
      "36/36 [==============================] - 0s 428us/step - loss: 4.0487 - val_loss: 8.9085\n",
      "Epoch 278/1500\n",
      "36/36 [==============================] - ETA: 0s - loss: 4.987 - 0s 556us/step - loss: 5.6476 - val_loss: 8.7763\n",
      "Epoch 279/1500\n",
      "36/36 [==============================] - 0s 762us/step - loss: 4.0148 - val_loss: 14.5941\n",
      "Epoch 280/1500\n",
      "36/36 [==============================] - 0s 524us/step - loss: 4.5838 - val_loss: 10.4626\n",
      "Epoch 281/1500\n",
      "36/36 [==============================] - 0s 473us/step - loss: 3.6538 - val_loss: 5.5482\n",
      "Epoch 282/1500\n",
      "36/36 [==============================] - 0s 615us/step - loss: 3.6271 - val_loss: 9.1504\n",
      "Epoch 283/1500\n",
      "36/36 [==============================] - 0s 592us/step - loss: 3.6423 - val_loss: 9.5127\n",
      "Epoch 284/1500\n",
      "36/36 [==============================] - 0s 559us/step - loss: 3.4219 - val_loss: 10.6772\n",
      "Epoch 285/1500\n",
      "36/36 [==============================] - 0s 546us/step - loss: 4.2969 - val_loss: 16.6197\n",
      "Epoch 286/1500\n",
      "36/36 [==============================] - 0s 521us/step - loss: 4.6082 - val_loss: 13.0165\n",
      "Epoch 287/1500\n",
      "36/36 [==============================] - 0s 672us/step - loss: 3.2316 - val_loss: 7.3839\n",
      "Epoch 288/1500\n",
      "36/36 [==============================] - 0s 560us/step - loss: 3.1795 - val_loss: 10.6694\n",
      "Epoch 289/1500\n",
      "36/36 [==============================] - 0s 522us/step - loss: 4.4680 - val_loss: 10.2805\n",
      "Epoch 290/1500\n",
      "36/36 [==============================] - 0s 592us/step - loss: 4.9204 - val_loss: 9.1489\n",
      "Epoch 291/1500\n",
      "36/36 [==============================] - 0s 607us/step - loss: 3.2468 - val_loss: 9.8138\n",
      "Epoch 292/1500\n",
      "36/36 [==============================] - 0s 456us/step - loss: 3.8491 - val_loss: 16.1382\n",
      "Epoch 293/1500\n",
      "36/36 [==============================] - 0s 578us/step - loss: 3.7636 - val_loss: 26.2746\n",
      "Epoch 294/1500\n",
      "36/36 [==============================] - 0s 551us/step - loss: 7.6579 - val_loss: 23.9103\n",
      "Epoch 295/1500\n",
      "36/36 [==============================] - 0s 453us/step - loss: 5.6101 - val_loss: 8.9504\n",
      "Epoch 296/1500\n",
      "36/36 [==============================] - 0s 397us/step - loss: 7.6485 - val_loss: 8.5703\n",
      "Epoch 297/1500\n",
      "36/36 [==============================] - 0s 500us/step - loss: 9.0655 - val_loss: 17.0134\n",
      "Epoch 298/1500\n",
      "36/36 [==============================] - 0s 581us/step - loss: 3.7718 - val_loss: 25.4473\n",
      "Epoch 299/1500\n",
      "36/36 [==============================] - 0s 423us/step - loss: 7.5003 - val_loss: 17.4928\n",
      "Epoch 300/1500\n",
      "36/36 [==============================] - 0s 526us/step - loss: 3.9687 - val_loss: 9.5181\n",
      "Epoch 301/1500\n",
      "36/36 [==============================] - 0s 480us/step - loss: 5.7553 - val_loss: 11.9095\n",
      "Epoch 302/1500\n",
      "36/36 [==============================] - 0s 380us/step - loss: 3.3394 - val_loss: 17.0889\n",
      "Epoch 303/1500\n",
      "36/36 [==============================] - 0s 499us/step - loss: 4.9704 - val_loss: 12.0383\n",
      "Epoch 304/1500\n",
      "36/36 [==============================] - 0s 491us/step - loss: 3.2734 - val_loss: 8.6342\n",
      "Epoch 305/1500\n",
      "36/36 [==============================] - 0s 771us/step - loss: 3.8145 - val_loss: 13.7477\n",
      "Epoch 306/1500\n",
      "36/36 [==============================] - 0s 408us/step - loss: 3.2003 - val_loss: 16.4556\n",
      "Epoch 307/1500\n",
      "36/36 [==============================] - 0s 423us/step - loss: 4.1804 - val_loss: 8.7041\n",
      "Epoch 308/1500\n",
      "36/36 [==============================] - 0s 374us/step - loss: 3.7580 - val_loss: 6.3315\n",
      "Epoch 309/1500\n",
      "36/36 [==============================] - 0s 528us/step - loss: 3.8465 - val_loss: 10.0018\n",
      "Epoch 310/1500\n",
      "36/36 [==============================] - 0s 373us/step - loss: 3.3590 - val_loss: 11.6210\n",
      "Epoch 311/1500\n",
      "36/36 [==============================] - 0s 418us/step - loss: 3.7261 - val_loss: 11.4077\n",
      "Epoch 312/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 438us/step - loss: 3.2021 - val_loss: 13.8170\n",
      "Epoch 313/1500\n",
      "36/36 [==============================] - 0s 370us/step - loss: 3.1009 - val_loss: 16.4459\n",
      "Epoch 314/1500\n",
      "36/36 [==============================] - 0s 360us/step - loss: 3.4795 - val_loss: 14.1098\n",
      "Epoch 315/1500\n",
      "36/36 [==============================] - 0s 380us/step - loss: 3.8295 - val_loss: 12.4925\n",
      "Epoch 316/1500\n",
      "36/36 [==============================] - 0s 501us/step - loss: 4.6238 - val_loss: 11.6172\n",
      "Epoch 317/1500\n",
      "36/36 [==============================] - 0s 578us/step - loss: 3.5380 - val_loss: 13.8017\n",
      "Epoch 318/1500\n",
      "36/36 [==============================] - 0s 902us/step - loss: 3.9611 - val_loss: 7.6895\n",
      "Epoch 319/1500\n",
      "36/36 [==============================] - 0s 521us/step - loss: 3.3647 - val_loss: 2.6227\n",
      "Epoch 320/1500\n",
      "36/36 [==============================] - 0s 721us/step - loss: 6.3236 - val_loss: 6.6772\n",
      "Epoch 321/1500\n",
      "36/36 [==============================] - 0s 639us/step - loss: 3.1543 - val_loss: 15.8994\n",
      "Epoch 322/1500\n",
      "36/36 [==============================] - 0s 558us/step - loss: 6.3594 - val_loss: 13.4898\n",
      "Epoch 323/1500\n",
      "36/36 [==============================] - 0s 719us/step - loss: 3.6323 - val_loss: 5.2479\n",
      "Epoch 324/1500\n",
      "36/36 [==============================] - 0s 470us/step - loss: 6.5682 - val_loss: 8.1084\n",
      "Epoch 325/1500\n",
      "36/36 [==============================] - 0s 613us/step - loss: 4.7590 - val_loss: 14.9270\n",
      "Epoch 326/1500\n",
      "36/36 [==============================] - 0s 513us/step - loss: 3.9619 - val_loss: 7.5297\n",
      "Epoch 327/1500\n",
      "36/36 [==============================] - 0s 551us/step - loss: 3.6040 - val_loss: 3.5045\n",
      "Epoch 328/1500\n",
      "36/36 [==============================] - 0s 492us/step - loss: 5.3831 - val_loss: 5.8141\n",
      "Epoch 329/1500\n",
      "36/36 [==============================] - 0s 407us/step - loss: 3.7555 - val_loss: 8.0794\n",
      "Epoch 330/1500\n",
      "36/36 [==============================] - 0s 883us/step - loss: 3.8674 - val_loss: 2.1317\n",
      "Epoch 331/1500\n",
      "36/36 [==============================] - 0s 789us/step - loss: 6.1770 - val_loss: 4.5838\n",
      "Epoch 332/1500\n",
      "36/36 [==============================] - 0s 481us/step - loss: 5.4748 - val_loss: 13.9250\n",
      "Epoch 333/1500\n",
      "36/36 [==============================] - 0s 733us/step - loss: 4.1914 - val_loss: 13.9130\n",
      "Epoch 334/1500\n",
      "36/36 [==============================] - 0s 676us/step - loss: 3.5141 - val_loss: 11.7428\n",
      "Epoch 335/1500\n",
      "36/36 [==============================] - 0s 641us/step - loss: 3.5802 - val_loss: 13.6386\n",
      "Epoch 336/1500\n",
      "36/36 [==============================] - 0s 572us/step - loss: 2.9639 - val_loss: 12.0656\n",
      "Epoch 337/1500\n",
      "36/36 [==============================] - 0s 572us/step - loss: 3.0224 - val_loss: 8.8002\n",
      "Epoch 338/1500\n",
      "36/36 [==============================] - 0s 374us/step - loss: 2.9050 - val_loss: 6.7733\n",
      "Epoch 339/1500\n",
      "36/36 [==============================] - 0s 388us/step - loss: 3.6509 - val_loss: 11.2620\n",
      "Epoch 340/1500\n",
      "36/36 [==============================] - 0s 414us/step - loss: 3.1177 - val_loss: 13.6093\n",
      "Epoch 341/1500\n",
      "36/36 [==============================] - 0s 335us/step - loss: 3.2393 - val_loss: 11.3755\n",
      "Epoch 342/1500\n",
      "36/36 [==============================] - 0s 496us/step - loss: 2.9022 - val_loss: 15.0289\n",
      "Epoch 343/1500\n",
      "36/36 [==============================] - 0s 497us/step - loss: 3.5146 - val_loss: 12.8319\n",
      "Epoch 344/1500\n",
      "36/36 [==============================] - 0s 589us/step - loss: 2.8285 - val_loss: 9.1867\n",
      "Epoch 345/1500\n",
      "36/36 [==============================] - 0s 394us/step - loss: 3.2896 - val_loss: 11.2434\n",
      "Epoch 346/1500\n",
      "36/36 [==============================] - 0s 535us/step - loss: 2.8495 - val_loss: 11.9848\n",
      "Epoch 347/1500\n",
      "36/36 [==============================] - 0s 528us/step - loss: 2.6981 - val_loss: 14.6059\n",
      "Epoch 348/1500\n",
      "36/36 [==============================] - 0s 487us/step - loss: 3.3138 - val_loss: 12.1878\n",
      "Epoch 349/1500\n",
      "36/36 [==============================] - 0s 422us/step - loss: 2.7889 - val_loss: 9.8058\n",
      "Epoch 350/1500\n",
      "36/36 [==============================] - 0s 642us/step - loss: 3.0056 - val_loss: 12.4241\n",
      "Epoch 351/1500\n",
      "36/36 [==============================] - 0s 455us/step - loss: 2.8792 - val_loss: 3.7738\n",
      "Epoch 352/1500\n",
      "36/36 [==============================] - 0s 460us/step - loss: 5.0457 - val_loss: 2.7670\n",
      "Epoch 353/1500\n",
      "36/36 [==============================] - 0s 532us/step - loss: 5.0573 - val_loss: 7.3367\n",
      "Epoch 354/1500\n",
      "36/36 [==============================] - 0s 374us/step - loss: 3.3840 - val_loss: 8.3050\n",
      "Epoch 355/1500\n",
      "36/36 [==============================] - 0s 387us/step - loss: 3.0139 - val_loss: 12.2674\n",
      "Epoch 356/1500\n",
      "36/36 [==============================] - 0s 623us/step - loss: 3.0303 - val_loss: 12.2732\n",
      "Epoch 357/1500\n",
      "36/36 [==============================] - 0s 361us/step - loss: 2.8454 - val_loss: 9.6783\n",
      "Epoch 358/1500\n",
      "36/36 [==============================] - 0s 308us/step - loss: 2.6916 - val_loss: 13.1542\n",
      "Epoch 359/1500\n",
      "36/36 [==============================] - 0s 388us/step - loss: 2.9585 - val_loss: 11.5371\n",
      "Epoch 360/1500\n",
      "36/36 [==============================] - 0s 435us/step - loss: 2.3277 - val_loss: 10.6636\n",
      "Epoch 361/1500\n",
      "36/36 [==============================] - 0s 399us/step - loss: 2.7802 - val_loss: 16.8493\n",
      "Epoch 362/1500\n",
      "36/36 [==============================] - 0s 602us/step - loss: 4.3579 - val_loss: 14.9573\n",
      "Epoch 363/1500\n",
      "36/36 [==============================] - 0s 487us/step - loss: 3.4727 - val_loss: 9.2638\n",
      "Epoch 364/1500\n",
      "36/36 [==============================] - 0s 598us/step - loss: 3.3636 - val_loss: 18.0690\n",
      "Epoch 365/1500\n",
      "36/36 [==============================] - 0s 620us/step - loss: 5.4944 - val_loss: 16.5252\n",
      "Epoch 366/1500\n",
      "36/36 [==============================] - 0s 617us/step - loss: 4.8937 - val_loss: 6.6864\n",
      "Epoch 367/1500\n",
      "36/36 [==============================] - 0s 512us/step - loss: 4.3577 - val_loss: 9.8617\n",
      "Epoch 368/1500\n",
      "36/36 [==============================] - 0s 409us/step - loss: 3.0397 - val_loss: 15.9605\n",
      "Epoch 369/1500\n",
      "36/36 [==============================] - 0s 698us/step - loss: 5.7109 - val_loss: 11.0542\n",
      "Epoch 370/1500\n",
      "36/36 [==============================] - 0s 367us/step - loss: 3.0349 - val_loss: 4.0366\n",
      "Epoch 371/1500\n",
      "36/36 [==============================] - 0s 360us/step - loss: 4.8949 - val_loss: 8.3741\n",
      "Epoch 372/1500\n",
      "36/36 [==============================] - 0s 369us/step - loss: 3.2573 - val_loss: 14.8690\n",
      "Epoch 373/1500\n",
      "36/36 [==============================] - 0s 423us/step - loss: 5.8121 - val_loss: 12.0705\n",
      "Epoch 374/1500\n",
      "36/36 [==============================] - 0s 418us/step - loss: 4.1871 - val_loss: 5.2367\n",
      "Epoch 375/1500\n",
      "36/36 [==============================] - 0s 391us/step - loss: 5.8149 - val_loss: 6.1727\n",
      "Epoch 376/1500\n",
      "36/36 [==============================] - 0s 378us/step - loss: 6.1748 - val_loss: 12.4745\n",
      "Epoch 377/1500\n",
      "36/36 [==============================] - 0s 337us/step - loss: 3.9574 - val_loss: 15.3893\n",
      "Epoch 378/1500\n",
      "36/36 [==============================] - 0s 389us/step - loss: 3.6638 - val_loss: 14.1405\n",
      "Epoch 379/1500\n",
      "36/36 [==============================] - 0s 384us/step - loss: 3.5728 - val_loss: 9.0070\n",
      "Epoch 380/1500\n",
      "36/36 [==============================] - 0s 396us/step - loss: 4.5262 - val_loss: 11.2610\n",
      "Epoch 381/1500\n",
      "36/36 [==============================] - 0s 378us/step - loss: 3.9259 - val_loss: 17.1813\n",
      "Epoch 382/1500\n",
      "36/36 [==============================] - 0s 320us/step - loss: 5.8027 - val_loss: 12.7685\n",
      "Epoch 383/1500\n",
      "36/36 [==============================] - 0s 378us/step - loss: 3.1088 - val_loss: 8.1094\n",
      "Epoch 384/1500\n",
      "36/36 [==============================] - 0s 380us/step - loss: 3.3578 - val_loss: 10.9407\n",
      "Epoch 385/1500\n",
      "36/36 [==============================] - 0s 406us/step - loss: 2.5139 - val_loss: 9.1966\n",
      "Epoch 386/1500\n",
      "36/36 [==============================] - 0s 379us/step - loss: 3.1726 - val_loss: 13.7991\n",
      "Epoch 387/1500\n",
      "36/36 [==============================] - 0s 374us/step - loss: 3.6479 - val_loss: 13.1424\n",
      "Epoch 388/1500\n",
      "36/36 [==============================] - 0s 416us/step - loss: 3.2174 - val_loss: 8.8912\n",
      "Epoch 389/1500\n",
      "36/36 [==============================] - 0s 577us/step - loss: 3.2735 - val_loss: 9.2429\n",
      "Epoch 390/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 423us/step - loss: 2.9269 - val_loss: 10.6269\n",
      "Epoch 391/1500\n",
      "36/36 [==============================] - 0s 496us/step - loss: 2.5743 - val_loss: 16.2630\n",
      "Epoch 392/1500\n",
      "36/36 [==============================] - 0s 411us/step - loss: 3.4747 - val_loss: 11.2353\n",
      "Epoch 393/1500\n",
      "36/36 [==============================] - 0s 504us/step - loss: 3.0778 - val_loss: 12.0085\n",
      "Epoch 394/1500\n",
      "36/36 [==============================] - 0s 412us/step - loss: 2.4991 - val_loss: 19.8625\n",
      "Epoch 395/1500\n",
      "36/36 [==============================] - 0s 423us/step - loss: 5.5967 - val_loss: 14.3331\n",
      "Epoch 396/1500\n",
      "36/36 [==============================] - 0s 691us/step - loss: 3.1078 - val_loss: 5.2250\n",
      "Epoch 397/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 6.0497 - val_loss: 5.0827\n",
      "Epoch 398/1500\n",
      "36/36 [==============================] - 0s 721us/step - loss: 3.8281 - val_loss: 9.9970\n",
      "Epoch 399/1500\n",
      "36/36 [==============================] - 0s 355us/step - loss: 4.0775 - val_loss: 6.2699\n",
      "Epoch 400/1500\n",
      "36/36 [==============================] - 0s 288us/step - loss: 3.7477 - val_loss: 8.8021\n",
      "Epoch 401/1500\n",
      "36/36 [==============================] - 0s 384us/step - loss: 4.1911 - val_loss: 21.2800\n",
      "Epoch 402/1500\n",
      "36/36 [==============================] - 0s 442us/step - loss: 5.9448 - val_loss: 24.8636\n",
      "Epoch 403/1500\n",
      "36/36 [==============================] - 0s 455us/step - loss: 6.9608 - val_loss: 18.2248\n",
      "Epoch 404/1500\n",
      "36/36 [==============================] - 0s 422us/step - loss: 5.3873 - val_loss: 10.7904\n",
      "Epoch 405/1500\n",
      "36/36 [==============================] - 0s 421us/step - loss: 6.4633 - val_loss: 11.3581\n",
      "Epoch 406/1500\n",
      "36/36 [==============================] - 0s 483us/step - loss: 3.8729 - val_loss: 16.9152\n",
      "Epoch 407/1500\n",
      "36/36 [==============================] - 0s 446us/step - loss: 4.9978 - val_loss: 9.4600\n",
      "Epoch 408/1500\n",
      "36/36 [==============================] - 0s 441us/step - loss: 4.0921 - val_loss: 7.2446\n",
      "Epoch 409/1500\n",
      "36/36 [==============================] - 0s 462us/step - loss: 5.5970 - val_loss: 9.7717\n",
      "Epoch 410/1500\n",
      "36/36 [==============================] - 0s 484us/step - loss: 3.3724 - val_loss: 18.0715\n",
      "Epoch 411/1500\n",
      "36/36 [==============================] - 0s 539us/step - loss: 5.1150 - val_loss: 14.4507\n",
      "Epoch 412/1500\n",
      "36/36 [==============================] - 0s 727us/step - loss: 3.5979 - val_loss: 9.0107\n",
      "Epoch 413/1500\n",
      "36/36 [==============================] - 0s 733us/step - loss: 7.6636 - val_loss: 9.0031\n",
      "Epoch 414/1500\n",
      "36/36 [==============================] - 0s 330us/step - loss: 6.4268 - val_loss: 18.4687\n",
      "Epoch 415/1500\n",
      "36/36 [==============================] - 0s 357us/step - loss: 5.0183 - val_loss: 21.4923\n",
      "Epoch 416/1500\n",
      "36/36 [==============================] - 0s 365us/step - loss: 6.0351 - val_loss: 14.1459\n",
      "Epoch 417/1500\n",
      "36/36 [==============================] - ETA: 0s - loss: 3.510 - 0s 319us/step - loss: 3.5765 - val_loss: 7.6598\n",
      "Epoch 418/1500\n",
      "36/36 [==============================] - 0s 314us/step - loss: 4.0491 - val_loss: 9.4010\n",
      "Epoch 419/1500\n",
      "36/36 [==============================] - 0s 486us/step - loss: 3.2902 - val_loss: 8.3115\n",
      "Epoch 420/1500\n",
      "36/36 [==============================] - 0s 511us/step - loss: 3.7435 - val_loss: 5.2850\n",
      "Epoch 421/1500\n",
      "36/36 [==============================] - 0s 545us/step - loss: 4.4318 - val_loss: 6.9473\n",
      "Epoch 422/1500\n",
      "36/36 [==============================] - 0s 416us/step - loss: 3.6815 - val_loss: 13.2534\n",
      "Epoch 423/1500\n",
      "36/36 [==============================] - 0s 513us/step - loss: 3.1795 - val_loss: 14.6016\n",
      "Epoch 424/1500\n",
      "36/36 [==============================] - 0s 428us/step - loss: 3.0380 - val_loss: 17.5517\n",
      "Epoch 425/1500\n",
      "36/36 [==============================] - 0s 591us/step - loss: 3.7430 - val_loss: 16.2335\n",
      "Epoch 426/1500\n",
      "36/36 [==============================] - 0s 418us/step - loss: 3.4643 - val_loss: 11.6241\n",
      "Epoch 427/1500\n",
      "36/36 [==============================] - 0s 444us/step - loss: 2.8741 - val_loss: 10.3015\n",
      "Epoch 428/1500\n",
      "36/36 [==============================] - 0s 437us/step - loss: 2.1973 - val_loss: 7.1215\n",
      "Epoch 429/1500\n",
      "36/36 [==============================] - 0s 485us/step - loss: 2.8330 - val_loss: 7.6408\n",
      "Epoch 430/1500\n",
      "36/36 [==============================] - 0s 421us/step - loss: 2.7365 - val_loss: 8.2012\n",
      "Epoch 431/1500\n",
      "36/36 [==============================] - 0s 424us/step - loss: 2.6401 - val_loss: 8.5215\n",
      "Epoch 432/1500\n",
      "36/36 [==============================] - 0s 414us/step - loss: 2.7608 - val_loss: 12.7863\n",
      "Epoch 433/1500\n",
      "36/36 [==============================] - 0s 824us/step - loss: 2.9350 - val_loss: 11.4049\n",
      "Epoch 434/1500\n",
      "36/36 [==============================] - 0s 604us/step - loss: 2.4469 - val_loss: 10.7165\n",
      "Epoch 435/1500\n",
      "36/36 [==============================] - 0s 584us/step - loss: 2.2344 - val_loss: 15.1084\n",
      "Epoch 436/1500\n",
      "36/36 [==============================] - 0s 574us/step - loss: 3.9958 - val_loss: 7.9100\n",
      "Epoch 437/1500\n",
      "36/36 [==============================] - 0s 522us/step - loss: 3.3408 - val_loss: 7.1728\n",
      "Epoch 438/1500\n",
      "36/36 [==============================] - 0s 515us/step - loss: 3.9330 - val_loss: 13.2634\n",
      "Epoch 439/1500\n",
      "36/36 [==============================] - 0s 681us/step - loss: 3.5735 - val_loss: 11.8757\n",
      "Epoch 440/1500\n",
      "36/36 [==============================] - 0s 616us/step - loss: 2.4512 - val_loss: 7.6536\n",
      "Epoch 441/1500\n",
      "36/36 [==============================] - 0s 675us/step - loss: 3.4075 - val_loss: 13.0563\n",
      "Epoch 442/1500\n",
      "36/36 [==============================] - 0s 542us/step - loss: 3.8655 - val_loss: 12.7477\n",
      "Epoch 443/1500\n",
      "36/36 [==============================] - 0s 720us/step - loss: 4.6292 - val_loss: 5.8920\n",
      "Epoch 444/1500\n",
      "36/36 [==============================] - 0s 647us/step - loss: 3.8062 - val_loss: 4.3674\n",
      "Epoch 445/1500\n",
      "36/36 [==============================] - 0s 568us/step - loss: 4.0845 - val_loss: 9.6131\n",
      "Epoch 446/1500\n",
      "36/36 [==============================] - 0s 481us/step - loss: 3.6655 - val_loss: 12.9850\n",
      "Epoch 447/1500\n",
      "36/36 [==============================] - 0s 442us/step - loss: 3.7534 - val_loss: 10.9010\n",
      "Epoch 448/1500\n",
      "36/36 [==============================] - 0s 978us/step - loss: 2.4969 - val_loss: 12.7929\n",
      "Epoch 449/1500\n",
      "36/36 [==============================] - 0s 235us/step - loss: 2.4313 - val_loss: 14.9285\n",
      "Epoch 450/1500\n",
      "36/36 [==============================] - 0s 311us/step - loss: 2.8880 - val_loss: 12.0741\n",
      "Epoch 451/1500\n",
      "36/36 [==============================] - 0s 398us/step - loss: 3.0723 - val_loss: 14.4141\n",
      "Epoch 452/1500\n",
      "36/36 [==============================] - 0s 476us/step - loss: 3.0051 - val_loss: 12.8187\n",
      "Epoch 453/1500\n",
      "36/36 [==============================] - 0s 429us/step - loss: 2.7562 - val_loss: 7.5918\n",
      "Epoch 454/1500\n",
      "36/36 [==============================] - 0s 487us/step - loss: 3.2807 - val_loss: 12.7245\n",
      "Epoch 455/1500\n",
      "36/36 [==============================] - 0s 432us/step - loss: 3.7141 - val_loss: 12.2482\n",
      "Epoch 456/1500\n",
      "36/36 [==============================] - 0s 823us/step - loss: 3.6162 - val_loss: 8.1770\n",
      "Epoch 457/1500\n",
      "36/36 [==============================] - 0s 610us/step - loss: 3.2098 - val_loss: 11.8870\n",
      "Epoch 458/1500\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 2.3498 - val_loss: 12.7754\n",
      "Epoch 459/1500\n",
      "36/36 [==============================] - 0s 473us/step - loss: 2.3642 - val_loss: 8.2250\n",
      "Epoch 460/1500\n",
      "36/36 [==============================] - 0s 563us/step - loss: 4.7780 - val_loss: 9.1026\n",
      "Epoch 461/1500\n",
      "36/36 [==============================] - 0s 596us/step - loss: 3.4107 - val_loss: 14.0676\n",
      "Epoch 462/1500\n",
      "36/36 [==============================] - 0s 510us/step - loss: 2.5980 - val_loss: 10.1545\n",
      "Epoch 463/1500\n",
      "36/36 [==============================] - 0s 585us/step - loss: 3.1354 - val_loss: 12.1494\n",
      "Epoch 464/1500\n",
      "36/36 [==============================] - 0s 590us/step - loss: 2.3957 - val_loss: 14.4600\n",
      "Epoch 465/1500\n",
      "36/36 [==============================] - 0s 575us/step - loss: 2.5012 - val_loss: 11.8211\n",
      "Epoch 466/1500\n",
      "36/36 [==============================] - 0s 556us/step - loss: 2.5589 - val_loss: 13.1923\n",
      "Epoch 467/1500\n",
      "36/36 [==============================] - 0s 428us/step - loss: 2.3278 - val_loss: 8.0099\n",
      "Epoch 468/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 370us/step - loss: 2.9896 - val_loss: 8.8292\n",
      "Epoch 469/1500\n",
      "36/36 [==============================] - 0s 384us/step - loss: 2.7831 - val_loss: 12.8447\n",
      "Epoch 470/1500\n",
      "36/36 [==============================] - 0s 446us/step - loss: 3.3439 - val_loss: 7.3733\n",
      "Epoch 471/1500\n",
      "36/36 [==============================] - 0s 426us/step - loss: 3.1342 - val_loss: 6.9013\n",
      "Epoch 472/1500\n",
      "36/36 [==============================] - 0s 414us/step - loss: 3.3353 - val_loss: 14.1609\n",
      "Epoch 473/1500\n",
      "36/36 [==============================] - 0s 536us/step - loss: 3.8004 - val_loss: 13.9914\n",
      "Epoch 474/1500\n",
      "36/36 [==============================] - 0s 470us/step - loss: 4.1883 - val_loss: 7.9272\n",
      "Epoch 475/1500\n",
      "36/36 [==============================] - 0s 456us/step - loss: 3.2246 - val_loss: 5.8526\n",
      "Epoch 476/1500\n",
      "36/36 [==============================] - 0s 528us/step - loss: 3.5751 - val_loss: 10.7078\n",
      "Epoch 477/1500\n",
      "36/36 [==============================] - 0s 431us/step - loss: 2.8494 - val_loss: 11.2362\n",
      "Epoch 478/1500\n",
      "36/36 [==============================] - 0s 594us/step - loss: 2.9434 - val_loss: 9.0620\n",
      "Epoch 479/1500\n",
      "36/36 [==============================] - ETA: 0s - loss: 2.763 - 0s 444us/step - loss: 2.6489 - val_loss: 9.4475\n",
      "Epoch 480/1500\n",
      "36/36 [==============================] - 0s 516us/step - loss: 2.3676 - val_loss: 12.1297\n",
      "Epoch 481/1500\n",
      "36/36 [==============================] - 0s 458us/step - loss: 2.0050 - val_loss: 9.4340\n",
      "Epoch 482/1500\n",
      "36/36 [==============================] - 0s 428us/step - loss: 2.7771 - val_loss: 11.4931\n",
      "Epoch 483/1500\n",
      "36/36 [==============================] - 0s 427us/step - loss: 2.4912 - val_loss: 10.0461\n",
      "Epoch 484/1500\n",
      "36/36 [==============================] - 0s 516us/step - loss: 2.1933 - val_loss: 6.4935\n",
      "Epoch 485/1500\n",
      "36/36 [==============================] - 0s 471us/step - loss: 3.2754 - val_loss: 9.2925\n",
      "Epoch 486/1500\n",
      "36/36 [==============================] - 0s 526us/step - loss: 2.2321 - val_loss: 9.2616\n",
      "Epoch 487/1500\n",
      "36/36 [==============================] - 0s 455us/step - loss: 2.6604 - val_loss: 9.9638\n",
      "Epoch 488/1500\n",
      "36/36 [==============================] - 0s 511us/step - loss: 2.4930 - val_loss: 13.1749\n",
      "Epoch 489/1500\n",
      "36/36 [==============================] - 0s 472us/step - loss: 2.7665 - val_loss: 8.2164\n",
      "Epoch 490/1500\n",
      "36/36 [==============================] - 0s 646us/step - loss: 4.2976 - val_loss: 10.4092\n",
      "Epoch 491/1500\n",
      "36/36 [==============================] - 0s 399us/step - loss: 4.3045 - val_loss: 17.0430\n",
      "Epoch 492/1500\n",
      "36/36 [==============================] - 0s 631us/step - loss: 3.0064 - val_loss: 12.4687\n",
      "Epoch 493/1500\n",
      "36/36 [==============================] - 0s 414us/step - loss: 2.5697 - val_loss: 9.3571\n",
      "Epoch 494/1500\n",
      "36/36 [==============================] - 0s 382us/step - loss: 2.5300 - val_loss: 17.2124\n",
      "Epoch 495/1500\n",
      "36/36 [==============================] - 0s 399us/step - loss: 5.9472 - val_loss: 17.9214\n",
      "Epoch 496/1500\n",
      "36/36 [==============================] - 0s 375us/step - loss: 5.6411 - val_loss: 8.3142\n",
      "Epoch 497/1500\n",
      "36/36 [==============================] - 0s 431us/step - loss: 4.0650 - val_loss: 7.7393\n",
      "Epoch 498/1500\n",
      "36/36 [==============================] - 0s 415us/step - loss: 4.8279 - val_loss: 18.0974\n",
      "Epoch 499/1500\n",
      "36/36 [==============================] - 0s 393us/step - loss: 5.5099 - val_loss: 21.0493\n",
      "Epoch 500/1500\n",
      "36/36 [==============================] - 0s 484us/step - loss: 6.7140 - val_loss: 10.0237\n",
      "Epoch 501/1500\n",
      "36/36 [==============================] - 0s 716us/step - loss: 3.2450 - val_loss: 6.1508\n",
      "Epoch 502/1500\n",
      "36/36 [==============================] - 0s 415us/step - loss: 6.4859 - val_loss: 8.6744\n",
      "Epoch 503/1500\n",
      "36/36 [==============================] - 0s 440us/step - loss: 2.3517 - val_loss: 16.2618\n",
      "Epoch 504/1500\n",
      "36/36 [==============================] - 0s 507us/step - loss: 6.0702 - val_loss: 8.8346\n",
      "Epoch 505/1500\n",
      "36/36 [==============================] - 0s 464us/step - loss: 3.3213 - val_loss: 7.8808\n",
      "Epoch 506/1500\n",
      "36/36 [==============================] - 0s 882us/step - loss: 5.6168 - val_loss: 10.3745\n",
      "Epoch 507/1500\n",
      "36/36 [==============================] - 0s 487us/step - loss: 2.8580 - val_loss: 16.6650\n",
      "Epoch 508/1500\n",
      "36/36 [==============================] - 0s 653us/step - loss: 4.0701 - val_loss: 12.3450\n",
      "Epoch 509/1500\n",
      "36/36 [==============================] - 0s 538us/step - loss: 2.2712 - val_loss: 6.6316\n",
      "Epoch 510/1500\n",
      "36/36 [==============================] - 0s 442us/step - loss: 4.3742 - val_loss: 12.1316\n",
      "Epoch 511/1500\n",
      "36/36 [==============================] - 0s 443us/step - loss: 2.1138 - val_loss: 14.9223\n",
      "Epoch 512/1500\n",
      "36/36 [==============================] - 0s 522us/step - loss: 2.6793 - val_loss: 7.8941\n",
      "Epoch 513/1500\n",
      "36/36 [==============================] - 0s 449us/step - loss: 4.4921 - val_loss: 10.1517\n",
      "Epoch 514/1500\n",
      "36/36 [==============================] - 0s 633us/step - loss: 3.3796 - val_loss: 18.9534\n",
      "Epoch 515/1500\n",
      "36/36 [==============================] - 0s 394us/step - loss: 4.4004 - val_loss: 17.7518\n",
      "Epoch 516/1500\n",
      "36/36 [==============================] - 0s 477us/step - loss: 3.5275 - val_loss: 9.6620\n",
      "Epoch 517/1500\n",
      "36/36 [==============================] - 0s 389us/step - loss: 3.3368 - val_loss: 12.0398\n",
      "Epoch 518/1500\n",
      "36/36 [==============================] - ETA: 0s - loss: 2.394 - 0s 390us/step - loss: 2.2969 - val_loss: 19.4285\n",
      "Epoch 519/1500\n",
      "36/36 [==============================] - 0s 411us/step - loss: 4.3734 - val_loss: 16.3912\n",
      "Epoch 520/1500\n",
      "36/36 [==============================] - 0s 384us/step - loss: 2.9360 - val_loss: 12.8935\n",
      "Epoch 521/1500\n",
      "36/36 [==============================] - 0s 438us/step - loss: 5.4383 - val_loss: 15.3158\n",
      "Epoch 522/1500\n",
      "36/36 [==============================] - 0s 424us/step - loss: 4.0654 - val_loss: 21.2101\n",
      "Epoch 523/1500\n",
      "36/36 [==============================] - 0s 399us/step - loss: 5.7068 - val_loss: 18.3957\n",
      "Epoch 524/1500\n",
      "36/36 [==============================] - 0s 419us/step - loss: 4.9722 - val_loss: 5.0439\n",
      "Epoch 525/1500\n",
      "36/36 [==============================] - 0s 420us/step - loss: 4.2139 - val_loss: 3.2493\n",
      "Epoch 526/1500\n",
      "36/36 [==============================] - 0s 431us/step - loss: 5.3661 - val_loss: 4.5464\n",
      "Epoch 527/1500\n",
      "36/36 [==============================] - 0s 405us/step - loss: 4.9193 - val_loss: 7.1603\n",
      "Epoch 528/1500\n",
      "36/36 [==============================] - 0s 419us/step - loss: 5.0748 - val_loss: 2.9047\n",
      "Epoch 529/1500\n",
      "36/36 [==============================] - 0s 525us/step - loss: 4.3887 - val_loss: 4.1622\n",
      "Epoch 530/1500\n",
      "36/36 [==============================] - 0s 440us/step - loss: 4.7415 - val_loss: 12.5736\n",
      "Epoch 531/1500\n",
      "36/36 [==============================] - 0s 465us/step - loss: 3.9594 - val_loss: 15.0594\n",
      "Epoch 532/1500\n",
      "36/36 [==============================] - 0s 501us/step - loss: 4.5697 - val_loss: 6.1363\n",
      "Epoch 533/1500\n",
      "36/36 [==============================] - 0s 605us/step - loss: 5.6133 - val_loss: 5.4859\n",
      "Epoch 534/1500\n",
      "36/36 [==============================] - 0s 583us/step - loss: 8.5017 - val_loss: 7.1031\n",
      "Epoch 535/1500\n",
      "36/36 [==============================] - 0s 411us/step - loss: 4.3269 - val_loss: 17.0556\n",
      "Epoch 536/1500\n",
      "36/36 [==============================] - 0s 409us/step - loss: 5.7371 - val_loss: 17.0801\n",
      "Epoch 537/1500\n",
      "36/36 [==============================] - 0s 489us/step - loss: 6.0050 - val_loss: 9.7123\n",
      "Epoch 538/1500\n",
      "36/36 [==============================] - 0s 502us/step - loss: 3.2524 - val_loss: 8.6455\n",
      "Epoch 539/1500\n",
      "36/36 [==============================] - 0s 498us/step - loss: 3.2926 - val_loss: 12.5021\n",
      "Epoch 540/1500\n",
      "36/36 [==============================] - 0s 460us/step - loss: 2.9808 - val_loss: 15.7948\n",
      "Epoch 541/1500\n",
      "36/36 [==============================] - ETA: 0s - loss: 3.519 - 0s 492us/step - loss: 3.4155 - val_loss: 14.4197\n",
      "Epoch 542/1500\n",
      "36/36 [==============================] - 0s 520us/step - loss: 2.6970 - val_loss: 10.0761\n",
      "Epoch 543/1500\n",
      "36/36 [==============================] - 0s 514us/step - loss: 2.4984 - val_loss: 7.0269\n",
      "Epoch 544/1500\n",
      "36/36 [==============================] - ETA: 0s - loss: 2.656 - 0s 473us/step - loss: 2.6322 - val_loss: 9.1993\n",
      "Epoch 545/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - ETA: 0s - loss: 2.320 - 0s 444us/step - loss: 2.4053 - val_loss: 11.1931\n",
      "Epoch 546/1500\n",
      "36/36 [==============================] - 0s 527us/step - loss: 2.7408 - val_loss: 6.2815\n",
      "Epoch 547/1500\n",
      "36/36 [==============================] - 0s 602us/step - loss: 3.5832 - val_loss: 9.4893\n",
      "Epoch 548/1500\n",
      "36/36 [==============================] - 0s 363us/step - loss: 2.5905 - val_loss: 14.3828\n",
      "Epoch 549/1500\n",
      "36/36 [==============================] - 0s 414us/step - loss: 2.7705 - val_loss: 9.2267\n",
      "Epoch 550/1500\n",
      "36/36 [==============================] - 0s 383us/step - loss: 2.8907 - val_loss: 11.9538\n",
      "Epoch 551/1500\n",
      "36/36 [==============================] - 0s 413us/step - loss: 1.9630 - val_loss: 14.9151\n",
      "Epoch 552/1500\n",
      "36/36 [==============================] - 0s 498us/step - loss: 3.3833 - val_loss: 6.1412\n",
      "Epoch 553/1500\n",
      "36/36 [==============================] - 0s 445us/step - loss: 4.2097 - val_loss: 5.1802\n",
      "Epoch 554/1500\n",
      "36/36 [==============================] - 0s 431us/step - loss: 4.8838 - val_loss: 13.2910\n",
      "Epoch 555/1500\n",
      "36/36 [==============================] - 0s 382us/step - loss: 2.8137 - val_loss: 15.7511\n",
      "Epoch 556/1500\n",
      "36/36 [==============================] - 0s 408us/step - loss: 3.3529 - val_loss: 10.5852\n",
      "Epoch 557/1500\n",
      "36/36 [==============================] - 0s 415us/step - loss: 2.4461 - val_loss: 13.5033\n",
      "Epoch 558/1500\n",
      "36/36 [==============================] - 0s 529us/step - loss: 2.2414 - val_loss: 15.3714\n",
      "Epoch 559/1500\n",
      "36/36 [==============================] - 0s 392us/step - loss: 2.7938 - val_loss: 11.8137\n",
      "Epoch 560/1500\n",
      "36/36 [==============================] - 0s 447us/step - loss: 1.8321 - val_loss: 9.5371\n",
      "Epoch 561/1500\n",
      "36/36 [==============================] - 0s 564us/step - loss: 2.4716 - val_loss: 8.6533\n",
      "Epoch 562/1500\n",
      "36/36 [==============================] - 0s 481us/step - loss: 2.9068 - val_loss: 11.5047\n",
      "Epoch 563/1500\n",
      "36/36 [==============================] - 0s 533us/step - loss: 1.9102 - val_loss: 14.0854\n",
      "Epoch 564/1500\n",
      "36/36 [==============================] - 0s 534us/step - loss: 2.7591 - val_loss: 10.4484\n",
      "Epoch 565/1500\n",
      "36/36 [==============================] - 0s 555us/step - loss: 2.1788 - val_loss: 11.2985\n",
      "Epoch 566/1500\n",
      "36/36 [==============================] - 0s 460us/step - loss: 1.9831 - val_loss: 12.6575\n",
      "Epoch 567/1500\n",
      "36/36 [==============================] - 0s 404us/step - loss: 2.2349 - val_loss: 11.6433\n",
      "Epoch 568/1500\n",
      "36/36 [==============================] - 0s 441us/step - loss: 1.7237 - val_loss: 10.0475\n",
      "Epoch 569/1500\n",
      "36/36 [==============================] - 0s 452us/step - loss: 2.9479 - val_loss: 12.1873\n",
      "Epoch 570/1500\n",
      "36/36 [==============================] - 0s 392us/step - loss: 2.6225 - val_loss: 16.4170\n",
      "Epoch 571/1500\n",
      "36/36 [==============================] - 0s 376us/step - loss: 2.9258 - val_loss: 9.9475\n",
      "Epoch 572/1500\n",
      "36/36 [==============================] - 0s 369us/step - loss: 3.2284 - val_loss: 9.7039\n",
      "Epoch 573/1500\n",
      "36/36 [==============================] - 0s 439us/step - loss: 2.8074 - val_loss: 12.1784\n",
      "Epoch 574/1500\n",
      "36/36 [==============================] - 0s 487us/step - loss: 3.1318 - val_loss: 9.0506\n",
      "Epoch 575/1500\n",
      "36/36 [==============================] - 0s 422us/step - loss: 2.5484 - val_loss: 13.2367\n",
      "Epoch 576/1500\n",
      "36/36 [==============================] - 0s 603us/step - loss: 2.7370 - val_loss: 13.6413\n",
      "Epoch 577/1500\n",
      "36/36 [==============================] - 0s 648us/step - loss: 2.5652 - val_loss: 8.7529\n",
      "Epoch 578/1500\n",
      "36/36 [==============================] - 0s 490us/step - loss: 3.4669 - val_loss: 12.3854\n",
      "Epoch 579/1500\n",
      "36/36 [==============================] - 0s 502us/step - loss: 1.7435 - val_loss: 16.8202\n",
      "Epoch 580/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 3.7063 - val_loss: 12.3458\n",
      "Epoch 581/1500\n",
      "36/36 [==============================] - 0s 703us/step - loss: 2.2465 - val_loss: 8.9280\n",
      "Epoch 582/1500\n",
      "36/36 [==============================] - 0s 568us/step - loss: 2.9019 - val_loss: 12.1387\n",
      "Epoch 583/1500\n",
      "36/36 [==============================] - 0s 492us/step - loss: 2.5265 - val_loss: 13.2544\n",
      "Epoch 584/1500\n",
      "36/36 [==============================] - 0s 603us/step - loss: 2.3656 - val_loss: 9.7247\n",
      "Epoch 585/1500\n",
      "36/36 [==============================] - 0s 768us/step - loss: 2.7758 - val_loss: 13.2292\n",
      "Epoch 586/1500\n",
      "36/36 [==============================] - 0s 423us/step - loss: 2.3917 - val_loss: 15.5308\n",
      "Epoch 587/1500\n",
      "36/36 [==============================] - 0s 441us/step - loss: 3.0227 - val_loss: 9.6869\n",
      "Epoch 588/1500\n",
      "36/36 [==============================] - 0s 357us/step - loss: 3.4757 - val_loss: 10.3016\n",
      "Epoch 589/1500\n",
      "36/36 [==============================] - 0s 415us/step - loss: 2.4365 - val_loss: 13.5696\n",
      "Epoch 590/1500\n",
      "36/36 [==============================] - 0s 444us/step - loss: 2.7764 - val_loss: 8.7778\n",
      "Epoch 591/1500\n",
      "36/36 [==============================] - 0s 405us/step - loss: 2.1144 - val_loss: 7.0205\n",
      "Epoch 592/1500\n",
      "36/36 [==============================] - 0s 387us/step - loss: 2.3911 - val_loss: 7.5974\n",
      "Epoch 593/1500\n",
      "36/36 [==============================] - 0s 392us/step - loss: 2.2744 - val_loss: 7.4054\n",
      "Epoch 594/1500\n",
      "36/36 [==============================] - 0s 366us/step - loss: 2.2571 - val_loss: 12.6379\n",
      "Epoch 595/1500\n",
      "36/36 [==============================] - 0s 388us/step - loss: 2.7049 - val_loss: 13.4880\n",
      "Epoch 596/1500\n",
      "36/36 [==============================] - 0s 589us/step - loss: 2.2938 - val_loss: 10.9413\n",
      "Epoch 597/1500\n",
      "36/36 [==============================] - 0s 512us/step - loss: 2.7889 - val_loss: 12.7645\n",
      "Epoch 598/1500\n",
      "36/36 [==============================] - 0s 749us/step - loss: 2.0779 - val_loss: 10.3576\n",
      "Epoch 599/1500\n",
      "36/36 [==============================] - 0s 536us/step - loss: 2.1168 - val_loss: 9.5741\n",
      "Epoch 600/1500\n",
      "36/36 [==============================] - 0s 833us/step - loss: 2.1831 - val_loss: 11.8415\n",
      "Epoch 601/1500\n",
      "36/36 [==============================] - 0s 649us/step - loss: 2.0698 - val_loss: 8.7183\n",
      "Epoch 602/1500\n",
      "36/36 [==============================] - 0s 700us/step - loss: 2.7217 - val_loss: 12.1885\n",
      "Epoch 603/1500\n",
      "36/36 [==============================] - 0s 605us/step - loss: 2.0773 - val_loss: 16.1706\n",
      "Epoch 604/1500\n",
      "36/36 [==============================] - 0s 562us/step - loss: 3.3763 - val_loss: 12.2008\n",
      "Epoch 605/1500\n",
      "36/36 [==============================] - 0s 414us/step - loss: 1.9891 - val_loss: 8.9767\n",
      "Epoch 606/1500\n",
      "36/36 [==============================] - 0s 512us/step - loss: 2.0500 - val_loss: 12.1768\n",
      "Epoch 607/1500\n",
      "36/36 [==============================] - 0s 466us/step - loss: 2.5770 - val_loss: 9.9068\n",
      "Epoch 608/1500\n",
      "36/36 [==============================] - 0s 449us/step - loss: 2.0655 - val_loss: 10.1302\n",
      "Epoch 609/1500\n",
      "36/36 [==============================] - 0s 503us/step - loss: 2.1959 - val_loss: 12.9986\n",
      "Epoch 610/1500\n",
      "36/36 [==============================] - 0s 563us/step - loss: 2.0832 - val_loss: 9.7524\n",
      "Epoch 611/1500\n",
      "36/36 [==============================] - 0s 676us/step - loss: 3.4660 - val_loss: 9.4033\n",
      "Epoch 612/1500\n",
      "36/36 [==============================] - 0s 566us/step - loss: 2.7832 - val_loss: 15.9468\n",
      "Epoch 613/1500\n",
      "36/36 [==============================] - 0s 542us/step - loss: 4.1781 - val_loss: 13.9156\n",
      "Epoch 614/1500\n",
      "36/36 [==============================] - 0s 617us/step - loss: 2.9852 - val_loss: 8.5277\n",
      "Epoch 615/1500\n",
      "36/36 [==============================] - 0s 526us/step - loss: 2.6542 - val_loss: 13.6577\n",
      "Epoch 616/1500\n",
      "36/36 [==============================] - 0s 498us/step - loss: 2.9235 - val_loss: 13.7150\n",
      "Epoch 617/1500\n",
      "36/36 [==============================] - 0s 459us/step - loss: 2.6929 - val_loss: 7.1382\n",
      "Epoch 618/1500\n",
      "36/36 [==============================] - 0s 516us/step - loss: 4.4722 - val_loss: 8.3083\n",
      "Epoch 619/1500\n",
      "36/36 [==============================] - 0s 823us/step - loss: 2.9957 - val_loss: 13.0327\n",
      "Epoch 620/1500\n",
      "36/36 [==============================] - 0s 526us/step - loss: 2.8873 - val_loss: 10.2302\n",
      "Epoch 621/1500\n",
      "36/36 [==============================] - 0s 553us/step - loss: 2.2609 - val_loss: 9.9604\n",
      "Epoch 622/1500\n",
      "36/36 [==============================] - 0s 555us/step - loss: 2.7808 - val_loss: 13.7158\n",
      "Epoch 623/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 570us/step - loss: 2.3271 - val_loss: 17.4582\n",
      "Epoch 624/1500\n",
      "36/36 [==============================] - 0s 377us/step - loss: 2.7437 - val_loss: 15.9452\n",
      "Epoch 625/1500\n",
      "36/36 [==============================] - 0s 479us/step - loss: 2.8418 - val_loss: 15.8665\n",
      "Epoch 626/1500\n",
      "36/36 [==============================] - 0s 477us/step - loss: 2.9442 - val_loss: 13.9352\n",
      "Epoch 627/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 2.2444 - val_loss: 8.9503\n",
      "Epoch 628/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.7445 - val_loss: 11.0165\n",
      "Epoch 629/1500\n",
      "36/36 [==============================] - 0s 554us/step - loss: 3.7727 - val_loss: 10.3654\n",
      "Epoch 630/1500\n",
      "36/36 [==============================] - 0s 503us/step - loss: 4.8809 - val_loss: 7.9025\n",
      "Epoch 631/1500\n",
      "36/36 [==============================] - 0s 727us/step - loss: 2.9302 - val_loss: 9.1463\n",
      "Epoch 632/1500\n",
      "36/36 [==============================] - 0s 975us/step - loss: 1.6271 - val_loss: 11.9619\n",
      "Epoch 633/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.7593 - val_loss: 10.3828\n",
      "Epoch 634/1500\n",
      "36/36 [==============================] - 0s 441us/step - loss: 2.2483 - val_loss: 11.0595\n",
      "Epoch 635/1500\n",
      "36/36 [==============================] - 0s 556us/step - loss: 1.8394 - val_loss: 10.5410\n",
      "Epoch 636/1500\n",
      "36/36 [==============================] - 0s 633us/step - loss: 1.5216 - val_loss: 8.9132\n",
      "Epoch 637/1500\n",
      "36/36 [==============================] - 0s 728us/step - loss: 1.6739 - val_loss: 10.7175\n",
      "Epoch 638/1500\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 1.3984 - val_loss: 10.4055\n",
      "Epoch 639/1500\n",
      "36/36 [==============================] - 0s 660us/step - loss: 1.9565 - val_loss: 12.6124\n",
      "Epoch 640/1500\n",
      "36/36 [==============================] - 0s 883us/step - loss: 2.2435 - val_loss: 13.0859\n",
      "Epoch 641/1500\n",
      "36/36 [==============================] - 0s 539us/step - loss: 1.6956 - val_loss: 12.0148\n",
      "Epoch 642/1500\n",
      "36/36 [==============================] - 0s 565us/step - loss: 2.2281 - val_loss: 10.5374\n",
      "Epoch 643/1500\n",
      "36/36 [==============================] - 0s 617us/step - loss: 3.0704 - val_loss: 10.6336\n",
      "Epoch 644/1500\n",
      "36/36 [==============================] - 0s 691us/step - loss: 1.6883 - val_loss: 12.1963\n",
      "Epoch 645/1500\n",
      "36/36 [==============================] - 0s 604us/step - loss: 1.5700 - val_loss: 16.6923\n",
      "Epoch 646/1500\n",
      "36/36 [==============================] - 0s 764us/step - loss: 2.8074 - val_loss: 13.4311\n",
      "Epoch 647/1500\n",
      "36/36 [==============================] - 0s 507us/step - loss: 1.8879 - val_loss: 8.8135\n",
      "Epoch 648/1500\n",
      "36/36 [==============================] - 0s 538us/step - loss: 3.3476 - val_loss: 16.1237\n",
      "Epoch 649/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 3.3757 - val_loss: 15.0328\n",
      "Epoch 650/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 2.9741 - val_loss: 9.5021\n",
      "Epoch 651/1500\n",
      "36/36 [==============================] - 0s 597us/step - loss: 2.7215 - val_loss: 14.4013\n",
      "Epoch 652/1500\n",
      "36/36 [==============================] - 0s 620us/step - loss: 1.7804 - val_loss: 16.3483\n",
      "Epoch 653/1500\n",
      "36/36 [==============================] - 0s 680us/step - loss: 2.8038 - val_loss: 14.0510\n",
      "Epoch 654/1500\n",
      "36/36 [==============================] - 0s 648us/step - loss: 2.4344 - val_loss: 11.1000\n",
      "Epoch 655/1500\n",
      "36/36 [==============================] - 0s 483us/step - loss: 2.5886 - val_loss: 11.1803\n",
      "Epoch 656/1500\n",
      "36/36 [==============================] - 0s 437us/step - loss: 2.1893 - val_loss: 14.3739\n",
      "Epoch 657/1500\n",
      "36/36 [==============================] - 0s 503us/step - loss: 2.9206 - val_loss: 9.7558\n",
      "Epoch 658/1500\n",
      "36/36 [==============================] - 0s 461us/step - loss: 2.5189 - val_loss: 11.7530\n",
      "Epoch 659/1500\n",
      "36/36 [==============================] - 0s 558us/step - loss: 2.2802 - val_loss: 14.6095\n",
      "Epoch 660/1500\n",
      "36/36 [==============================] - 0s 386us/step - loss: 2.5238 - val_loss: 8.6190\n",
      "Epoch 661/1500\n",
      "36/36 [==============================] - 0s 467us/step - loss: 3.3804 - val_loss: 10.5797\n",
      "Epoch 662/1500\n",
      "36/36 [==============================] - 0s 567us/step - loss: 2.1298 - val_loss: 16.0228\n",
      "Epoch 663/1500\n",
      "36/36 [==============================] - 0s 464us/step - loss: 3.5726 - val_loss: 9.8669\n",
      "Epoch 664/1500\n",
      "36/36 [==============================] - 0s 394us/step - loss: 2.2238 - val_loss: 10.0395\n",
      "Epoch 665/1500\n",
      "36/36 [==============================] - 0s 383us/step - loss: 2.8555 - val_loss: 14.9605\n",
      "Epoch 666/1500\n",
      "36/36 [==============================] - 0s 386us/step - loss: 3.1848 - val_loss: 12.6409\n",
      "Epoch 667/1500\n",
      "36/36 [==============================] - 0s 510us/step - loss: 2.2262 - val_loss: 9.0314\n",
      "Epoch 668/1500\n",
      "36/36 [==============================] - 0s 366us/step - loss: 3.0736 - val_loss: 11.1700\n",
      "Epoch 669/1500\n",
      "36/36 [==============================] - 0s 849us/step - loss: 1.7978 - val_loss: 14.9254\n",
      "Epoch 670/1500\n",
      "36/36 [==============================] - 0s 760us/step - loss: 2.3263 - val_loss: 12.6695\n",
      "Epoch 671/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 2.4797 - val_loss: 16.6672\n",
      "Epoch 672/1500\n",
      "36/36 [==============================] - 0s 657us/step - loss: 2.2984 - val_loss: 14.0983\n",
      "Epoch 673/1500\n",
      "36/36 [==============================] - 0s 358us/step - loss: 1.8474 - val_loss: 9.6846\n",
      "Epoch 674/1500\n",
      "36/36 [==============================] - 0s 882us/step - loss: 2.3852 - val_loss: 13.3460\n",
      "Epoch 675/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 3.1518 - val_loss: 8.8910\n",
      "Epoch 676/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 3.2077 - val_loss: 10.7018\n",
      "Epoch 677/1500\n",
      "36/36 [==============================] - 0s 343us/step - loss: 3.0289 - val_loss: 19.9700\n",
      "Epoch 678/1500\n",
      "36/36 [==============================] - 0s 386us/step - loss: 5.7398 - val_loss: 19.3771\n",
      "Epoch 679/1500\n",
      "36/36 [==============================] - 0s 440us/step - loss: 4.5053 - val_loss: 6.9918\n",
      "Epoch 680/1500\n",
      "36/36 [==============================] - 0s 475us/step - loss: 6.0020 - val_loss: 6.8143\n",
      "Epoch 681/1500\n",
      "36/36 [==============================] - 0s 571us/step - loss: 8.4817 - val_loss: 12.4516\n",
      "Epoch 682/1500\n",
      "36/36 [==============================] - 0s 478us/step - loss: 2.9753 - val_loss: 23.2419\n",
      "Epoch 683/1500\n",
      "36/36 [==============================] - 0s 493us/step - loss: 7.1115 - val_loss: 18.0564\n",
      "Epoch 684/1500\n",
      "36/36 [==============================] - 0s 521us/step - loss: 3.3275 - val_loss: 8.2046\n",
      "Epoch 685/1500\n",
      "36/36 [==============================] - 0s 609us/step - loss: 5.4777 - val_loss: 9.9959\n",
      "Epoch 686/1500\n",
      "36/36 [==============================] - 0s 324us/step - loss: 4.0099 - val_loss: 19.7220\n",
      "Epoch 687/1500\n",
      "36/36 [==============================] - 0s 615us/step - loss: 4.4813 - val_loss: 18.8369\n",
      "Epoch 688/1500\n",
      "36/36 [==============================] - 0s 438us/step - loss: 3.9391 - val_loss: 11.3717\n",
      "Epoch 689/1500\n",
      "36/36 [==============================] - 0s 531us/step - loss: 2.9259 - val_loss: 9.7891\n",
      "Epoch 690/1500\n",
      "36/36 [==============================] - 0s 400us/step - loss: 3.4968 - val_loss: 13.1131\n",
      "Epoch 691/1500\n",
      "36/36 [==============================] - 0s 436us/step - loss: 2.5260 - val_loss: 16.7509\n",
      "Epoch 692/1500\n",
      "36/36 [==============================] - 0s 460us/step - loss: 2.6203 - val_loss: 13.7488\n",
      "Epoch 693/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 2.5091 - val_loss: 13.1154\n",
      "Epoch 694/1500\n",
      "36/36 [==============================] - 0s 566us/step - loss: 2.4629 - val_loss: 14.6942\n",
      "Epoch 695/1500\n",
      "36/36 [==============================] - 0s 393us/step - loss: 3.0168 - val_loss: 9.4569\n",
      "Epoch 696/1500\n",
      "36/36 [==============================] - 0s 534us/step - loss: 2.5183 - val_loss: 6.6717\n",
      "Epoch 697/1500\n",
      "36/36 [==============================] - 0s 506us/step - loss: 3.4551 - val_loss: 8.2675\n",
      "Epoch 698/1500\n",
      "36/36 [==============================] - 0s 493us/step - loss: 3.5090 - val_loss: 9.8644\n",
      "Epoch 699/1500\n",
      "36/36 [==============================] - 0s 555us/step - loss: 2.8393 - val_loss: 11.9382\n",
      "Epoch 700/1500\n",
      "36/36 [==============================] - 0s 724us/step - loss: 1.6647 - val_loss: 14.4139\n",
      "Epoch 701/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 504us/step - loss: 2.4248 - val_loss: 15.7094\n",
      "Epoch 702/1500\n",
      "36/36 [==============================] - 0s 531us/step - loss: 2.9835 - val_loss: 17.9826\n",
      "Epoch 703/1500\n",
      "36/36 [==============================] - 0s 576us/step - loss: 2.9524 - val_loss: 13.3930\n",
      "Epoch 704/1500\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4441 - val_loss: 7.7470\n",
      "Epoch 705/1500\n",
      "36/36 [==============================] - 0s 339us/step - loss: 2.5783 - val_loss: 7.4776\n",
      "Epoch 706/1500\n",
      "36/36 [==============================] - 0s 694us/step - loss: 2.4903 - val_loss: 9.7161\n",
      "Epoch 707/1500\n",
      "36/36 [==============================] - 0s 324us/step - loss: 1.6967 - val_loss: 14.8131\n",
      "Epoch 708/1500\n",
      "36/36 [==============================] - 0s 508us/step - loss: 3.0463 - val_loss: 15.3782\n",
      "Epoch 709/1500\n",
      "36/36 [==============================] - 0s 543us/step - loss: 3.1472 - val_loss: 8.6579\n",
      "Epoch 710/1500\n",
      "36/36 [==============================] - 0s 558us/step - loss: 3.4385 - val_loss: 7.0138\n",
      "Epoch 711/1500\n",
      "36/36 [==============================] - 0s 714us/step - loss: 2.8481 - val_loss: 11.4956\n",
      "Epoch 712/1500\n",
      "36/36 [==============================] - 0s 488us/step - loss: 3.2404 - val_loss: 9.3529\n",
      "Epoch 713/1500\n",
      "36/36 [==============================] - 0s 412us/step - loss: 2.3784 - val_loss: 8.4413\n",
      "Epoch 714/1500\n",
      "36/36 [==============================] - 0s 399us/step - loss: 2.8810 - val_loss: 13.1358\n",
      "Epoch 715/1500\n",
      "36/36 [==============================] - 0s 408us/step - loss: 2.5538 - val_loss: 13.9709\n",
      "Epoch 716/1500\n",
      "36/36 [==============================] - 0s 500us/step - loss: 2.5489 - val_loss: 7.7516\n",
      "Epoch 717/1500\n",
      "36/36 [==============================] - 0s 411us/step - loss: 3.9110 - val_loss: 7.0850\n",
      "Epoch 718/1500\n",
      "36/36 [==============================] - 0s 499us/step - loss: 3.7119 - val_loss: 16.3691\n",
      "Epoch 719/1500\n",
      "36/36 [==============================] - 0s 466us/step - loss: 4.8399 - val_loss: 16.7539\n",
      "Epoch 720/1500\n",
      "36/36 [==============================] - 0s 519us/step - loss: 5.5028 - val_loss: 4.8683\n",
      "Epoch 721/1500\n",
      "36/36 [==============================] - 0s 479us/step - loss: 3.8859 - val_loss: 4.6506\n",
      "Epoch 722/1500\n",
      "36/36 [==============================] - 0s 850us/step - loss: 6.4037 - val_loss: 6.6143\n",
      "Epoch 723/1500\n",
      "36/36 [==============================] - 0s 644us/step - loss: 3.3367 - val_loss: 19.2134\n",
      "Epoch 724/1500\n",
      "36/36 [==============================] - 0s 727us/step - loss: 9.1959 - val_loss: 22.6504\n",
      "Epoch 725/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 8.5969 - val_loss: 12.6454\n",
      "Epoch 726/1500\n",
      "36/36 [==============================] - 0s 593us/step - loss: 2.7776 - val_loss: 9.9774\n",
      "Epoch 727/1500\n",
      "36/36 [==============================] - 0s 570us/step - loss: 7.5693 - val_loss: 11.9472\n",
      "Epoch 728/1500\n",
      "36/36 [==============================] - 0s 549us/step - loss: 4.5040 - val_loss: 19.1019\n",
      "Epoch 729/1500\n",
      "36/36 [==============================] - 0s 690us/step - loss: 2.8289 - val_loss: 16.6570\n",
      "Epoch 730/1500\n",
      "36/36 [==============================] - 0s 468us/step - loss: 2.4183 - val_loss: 12.4769\n",
      "Epoch 731/1500\n",
      "36/36 [==============================] - 0s 658us/step - loss: 3.0054 - val_loss: 15.8562\n",
      "Epoch 732/1500\n",
      "36/36 [==============================] - 0s 772us/step - loss: 2.5061 - val_loss: 14.7060\n",
      "Epoch 733/1500\n",
      "36/36 [==============================] - 0s 720us/step - loss: 2.3446 - val_loss: 9.8982\n",
      "Epoch 734/1500\n",
      "36/36 [==============================] - 0s 844us/step - loss: 2.3371 - val_loss: 12.7757\n",
      "Epoch 735/1500\n",
      "36/36 [==============================] - 0s 640us/step - loss: 2.0985 - val_loss: 16.6539\n",
      "Epoch 736/1500\n",
      "36/36 [==============================] - 0s 592us/step - loss: 2.9873 - val_loss: 13.3682\n",
      "Epoch 737/1500\n",
      "36/36 [==============================] - 0s 780us/step - loss: 1.7800 - val_loss: 13.0163\n",
      "Epoch 738/1500\n",
      "36/36 [==============================] - 0s 845us/step - loss: 2.3116 - val_loss: 19.9051\n",
      "Epoch 739/1500\n",
      "36/36 [==============================] - 0s 913us/step - loss: 3.3794 - val_loss: 17.8719\n",
      "Epoch 740/1500\n",
      "36/36 [==============================] - 0s 647us/step - loss: 2.6304 - val_loss: 13.3344\n",
      "Epoch 741/1500\n",
      "36/36 [==============================] - 0s 750us/step - loss: 2.3146 - val_loss: 14.3203\n",
      "Epoch 742/1500\n",
      "36/36 [==============================] - 0s 488us/step - loss: 1.9938 - val_loss: 12.4370\n",
      "Epoch 743/1500\n",
      "36/36 [==============================] - 0s 498us/step - loss: 2.0684 - val_loss: 13.2309\n",
      "Epoch 744/1500\n",
      "36/36 [==============================] - 0s 352us/step - loss: 2.0911 - val_loss: 13.7943\n",
      "Epoch 745/1500\n",
      "36/36 [==============================] - 0s 341us/step - loss: 2.6474 - val_loss: 10.0256\n",
      "Epoch 746/1500\n",
      "36/36 [==============================] - 0s 434us/step - loss: 2.1941 - val_loss: 11.0306\n",
      "Epoch 747/1500\n",
      "36/36 [==============================] - 0s 651us/step - loss: 2.0000 - val_loss: 16.8105\n",
      "Epoch 748/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 2.4554 - val_loss: 14.9726\n",
      "Epoch 749/1500\n",
      "36/36 [==============================] - 0s 601us/step - loss: 1.6518 - val_loss: 10.5737\n",
      "Epoch 750/1500\n",
      "36/36 [==============================] - 0s 577us/step - loss: 2.0078 - val_loss: 11.9765\n",
      "Epoch 751/1500\n",
      "36/36 [==============================] - 0s 623us/step - loss: 2.1663 - val_loss: 13.0224\n",
      "Epoch 752/1500\n",
      "36/36 [==============================] - 0s 640us/step - loss: 2.0656 - val_loss: 10.1123\n",
      "Epoch 753/1500\n",
      "36/36 [==============================] - 0s 592us/step - loss: 2.8618 - val_loss: 13.1644\n",
      "Epoch 754/1500\n",
      "36/36 [==============================] - 0s 613us/step - loss: 1.8176 - val_loss: 16.4676\n",
      "Epoch 755/1500\n",
      "36/36 [==============================] - 0s 502us/step - loss: 2.2961 - val_loss: 10.2722\n",
      "Epoch 756/1500\n",
      "36/36 [==============================] - 0s 435us/step - loss: 2.6329 - val_loss: 10.9932\n",
      "Epoch 757/1500\n",
      "36/36 [==============================] - 0s 506us/step - loss: 1.7027 - val_loss: 13.2850\n",
      "Epoch 758/1500\n",
      "36/36 [==============================] - 0s 505us/step - loss: 1.6791 - val_loss: 8.4903\n",
      "Epoch 759/1500\n",
      "36/36 [==============================] - 0s 419us/step - loss: 2.4779 - val_loss: 13.2795\n",
      "Epoch 760/1500\n",
      "36/36 [==============================] - 0s 537us/step - loss: 2.0448 - val_loss: 12.5355\n",
      "Epoch 761/1500\n",
      "36/36 [==============================] - 0s 338us/step - loss: 2.0942 - val_loss: 5.8537\n",
      "Epoch 762/1500\n",
      "36/36 [==============================] - 0s 552us/step - loss: 3.2977 - val_loss: 5.5110\n",
      "Epoch 763/1500\n",
      "36/36 [==============================] - 0s 486us/step - loss: 3.0791 - val_loss: 12.8132\n",
      "Epoch 764/1500\n",
      "36/36 [==============================] - 0s 421us/step - loss: 3.1231 - val_loss: 14.8769\n",
      "Epoch 765/1500\n",
      "36/36 [==============================] - 0s 362us/step - loss: 3.0036 - val_loss: 10.4852\n",
      "Epoch 766/1500\n",
      "36/36 [==============================] - 0s 357us/step - loss: 3.5589 - val_loss: 12.5320\n",
      "Epoch 767/1500\n",
      "36/36 [==============================] - 0s 396us/step - loss: 3.3344 - val_loss: 18.6188\n",
      "Epoch 768/1500\n",
      "36/36 [==============================] - 0s 365us/step - loss: 3.8967 - val_loss: 16.0252\n",
      "Epoch 769/1500\n",
      "36/36 [==============================] - 0s 372us/step - loss: 2.6463 - val_loss: 10.9082\n",
      "Epoch 770/1500\n",
      "36/36 [==============================] - 0s 435us/step - loss: 3.5103 - val_loss: 10.7719\n",
      "Epoch 771/1500\n",
      "36/36 [==============================] - 0s 424us/step - loss: 2.7968 - val_loss: 12.0866\n",
      "Epoch 772/1500\n",
      "36/36 [==============================] - 0s 371us/step - loss: 2.0559 - val_loss: 12.8281\n",
      "Epoch 773/1500\n",
      "36/36 [==============================] - 0s 437us/step - loss: 1.6309 - val_loss: 14.0776\n",
      "Epoch 774/1500\n",
      "36/36 [==============================] - 0s 445us/step - loss: 2.1237 - val_loss: 14.1949\n",
      "Epoch 775/1500\n",
      "36/36 [==============================] - 0s 404us/step - loss: 2.1505 - val_loss: 13.0221\n",
      "Epoch 776/1500\n",
      "36/36 [==============================] - 0s 406us/step - loss: 1.6719 - val_loss: 12.6421\n",
      "Epoch 777/1500\n",
      "36/36 [==============================] - 0s 475us/step - loss: 2.0750 - val_loss: 18.6273\n",
      "Epoch 778/1500\n",
      "36/36 [==============================] - 0s 696us/step - loss: 3.9243 - val_loss: 16.7621\n",
      "Epoch 779/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 424us/step - loss: 3.0143 - val_loss: 12.6401\n",
      "Epoch 780/1500\n",
      "36/36 [==============================] - 0s 479us/step - loss: 5.1178 - val_loss: 12.2017\n",
      "Epoch 781/1500\n",
      "36/36 [==============================] - 0s 539us/step - loss: 5.0079 - val_loss: 18.2286\n",
      "Epoch 782/1500\n",
      "36/36 [==============================] - 0s 507us/step - loss: 4.6553 - val_loss: 20.3557\n",
      "Epoch 783/1500\n",
      "36/36 [==============================] - 0s 520us/step - loss: 5.3510 - val_loss: 9.7624\n",
      "Epoch 784/1500\n",
      "36/36 [==============================] - 0s 548us/step - loss: 4.3422 - val_loss: 7.0944\n",
      "Epoch 785/1500\n",
      "36/36 [==============================] - 0s 529us/step - loss: 7.6228 - val_loss: 6.5366\n",
      "Epoch 786/1500\n",
      "36/36 [==============================] - 0s 622us/step - loss: 3.7844 - val_loss: 14.2271\n",
      "Epoch 787/1500\n",
      "36/36 [==============================] - 0s 488us/step - loss: 3.7768 - val_loss: 13.8228\n",
      "Epoch 788/1500\n",
      "36/36 [==============================] - 0s 739us/step - loss: 3.2774 - val_loss: 11.1569\n",
      "Epoch 789/1500\n",
      "36/36 [==============================] - 0s 626us/step - loss: 1.9515 - val_loss: 16.0089\n",
      "Epoch 790/1500\n",
      "36/36 [==============================] - 0s 493us/step - loss: 2.0034 - val_loss: 17.3597\n",
      "Epoch 791/1500\n",
      "36/36 [==============================] - 0s 509us/step - loss: 2.0172 - val_loss: 14.1508\n",
      "Epoch 792/1500\n",
      "36/36 [==============================] - 0s 571us/step - loss: 2.5382 - val_loss: 15.6032\n",
      "Epoch 793/1500\n",
      "36/36 [==============================] - 0s 481us/step - loss: 2.1463 - val_loss: 14.7376\n",
      "Epoch 794/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 2.0013 - val_loss: 12.4466\n",
      "Epoch 795/1500\n",
      "36/36 [==============================] - 0s 735us/step - loss: 1.9176 - val_loss: 13.1693\n",
      "Epoch 796/1500\n",
      "36/36 [==============================] - 0s 852us/step - loss: 1.6932 - val_loss: 16.6372\n",
      "Epoch 797/1500\n",
      "36/36 [==============================] - 0s 535us/step - loss: 1.9440 - val_loss: 18.5723\n",
      "Epoch 798/1500\n",
      "36/36 [==============================] - 0s 911us/step - loss: 2.1317 - val_loss: 14.9590\n",
      "Epoch 799/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.3141 - val_loss: 15.8042\n",
      "Epoch 800/1500\n",
      "36/36 [==============================] - 0s 652us/step - loss: 2.7117 - val_loss: 11.4034\n",
      "Epoch 801/1500\n",
      "36/36 [==============================] - 0s 485us/step - loss: 2.4187 - val_loss: 9.2666\n",
      "Epoch 802/1500\n",
      "36/36 [==============================] - 0s 692us/step - loss: 3.0050 - val_loss: 13.2855\n",
      "Epoch 803/1500\n",
      "36/36 [==============================] - 0s 912us/step - loss: 1.5601 - val_loss: 14.5321\n",
      "Epoch 804/1500\n",
      "36/36 [==============================] - 0s 584us/step - loss: 1.8766 - val_loss: 12.2136\n",
      "Epoch 805/1500\n",
      "36/36 [==============================] - 0s 581us/step - loss: 2.6552 - val_loss: 12.6761\n",
      "Epoch 806/1500\n",
      "36/36 [==============================] - 0s 340us/step - loss: 1.8743 - val_loss: 12.3189\n",
      "Epoch 807/1500\n",
      "36/36 [==============================] - 0s 852us/step - loss: 2.5558 - val_loss: 8.1164\n",
      "Epoch 808/1500\n",
      "36/36 [==============================] - 0s 745us/step - loss: 2.6175 - val_loss: 10.2469\n",
      "Epoch 809/1500\n",
      "36/36 [==============================] - 0s 491us/step - loss: 2.2939 - val_loss: 13.7660\n",
      "Epoch 810/1500\n",
      "36/36 [==============================] - 0s 666us/step - loss: 1.8042 - val_loss: 10.4170\n",
      "Epoch 811/1500\n",
      "36/36 [==============================] - 0s 562us/step - loss: 3.5652 - val_loss: 12.3589\n",
      "Epoch 812/1500\n",
      "36/36 [==============================] - 0s 594us/step - loss: 2.2982 - val_loss: 19.1842\n",
      "Epoch 813/1500\n",
      "36/36 [==============================] - 0s 638us/step - loss: 3.4556 - val_loss: 15.6091\n",
      "Epoch 814/1500\n",
      "36/36 [==============================] - 0s 427us/step - loss: 1.9847 - val_loss: 12.3122\n",
      "Epoch 815/1500\n",
      "36/36 [==============================] - 0s 410us/step - loss: 3.3453 - val_loss: 17.5481\n",
      "Epoch 816/1500\n",
      "36/36 [==============================] - 0s 442us/step - loss: 2.3992 - val_loss: 18.3143\n",
      "Epoch 817/1500\n",
      "36/36 [==============================] - 0s 528us/step - loss: 3.3216 - val_loss: 8.5496\n",
      "Epoch 818/1500\n",
      "36/36 [==============================] - 0s 451us/step - loss: 3.5912 - val_loss: 8.2758\n",
      "Epoch 819/1500\n",
      "36/36 [==============================] - 0s 458us/step - loss: 3.2475 - val_loss: 14.9047\n",
      "Epoch 820/1500\n",
      "36/36 [==============================] - 0s 600us/step - loss: 3.7116 - val_loss: 16.3452\n",
      "Epoch 821/1500\n",
      "36/36 [==============================] - 0s 684us/step - loss: 4.3123 - val_loss: 8.1993\n",
      "Epoch 822/1500\n",
      "36/36 [==============================] - 0s 643us/step - loss: 3.2387 - val_loss: 8.4953\n",
      "Epoch 823/1500\n",
      "36/36 [==============================] - 0s 662us/step - loss: 4.6950 - val_loss: 12.0788\n",
      "Epoch 824/1500\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 2.9812 - val_loss: 14.1774\n",
      "Epoch 825/1500\n",
      "36/36 [==============================] - 0s 995us/step - loss: 4.4175 - val_loss: 8.8433\n",
      "Epoch 826/1500\n",
      "36/36 [==============================] - 0s 891us/step - loss: 3.2531 - val_loss: 9.1258\n",
      "Epoch 827/1500\n",
      "36/36 [==============================] - 0s 621us/step - loss: 4.4511 - val_loss: 10.1786\n",
      "Epoch 828/1500\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 3.0035 - val_loss: 14.7642\n",
      "Epoch 829/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 3.8125 - val_loss: 10.8534\n",
      "Epoch 830/1500\n",
      "36/36 [==============================] - 0s 898us/step - loss: 3.7843 - val_loss: 10.1940\n",
      "Epoch 831/1500\n",
      "36/36 [==============================] - 0s 458us/step - loss: 5.9976 - val_loss: 11.5971\n",
      "Epoch 832/1500\n",
      "36/36 [==============================] - 0s 615us/step - loss: 1.8252 - val_loss: 18.6441\n",
      "Epoch 833/1500\n",
      "36/36 [==============================] - 0s 678us/step - loss: 5.8894 - val_loss: 14.8504\n",
      "Epoch 834/1500\n",
      "36/36 [==============================] - 0s 375us/step - loss: 4.2542 - val_loss: 7.3296\n",
      "Epoch 835/1500\n",
      "36/36 [==============================] - 0s 779us/step - loss: 3.1869 - val_loss: 9.1384\n",
      "Epoch 836/1500\n",
      "36/36 [==============================] - 0s 425us/step - loss: 2.8653 - val_loss: 12.8596\n",
      "Epoch 837/1500\n",
      "36/36 [==============================] - 0s 700us/step - loss: 2.7810 - val_loss: 13.6326\n",
      "Epoch 838/1500\n",
      "36/36 [==============================] - 0s 996us/step - loss: 2.4607 - val_loss: 13.7909\n",
      "Epoch 839/1500\n",
      "36/36 [==============================] - 0s 765us/step - loss: 2.3707 - val_loss: 16.4607\n",
      "Epoch 840/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 2.4805 - val_loss: 15.0990\n",
      "Epoch 841/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 2.3917 - val_loss: 11.9692\n",
      "Epoch 842/1500\n",
      "36/36 [==============================] - 0s 808us/step - loss: 3.3409 - val_loss: 12.9857\n",
      "Epoch 843/1500\n",
      "36/36 [==============================] - 0s 388us/step - loss: 2.5010 - val_loss: 16.1156\n",
      "Epoch 844/1500\n",
      "36/36 [==============================] - 0s 685us/step - loss: 2.4215 - val_loss: 12.6125\n",
      "Epoch 845/1500\n",
      "36/36 [==============================] - 0s 731us/step - loss: 1.7689 - val_loss: 9.9174\n",
      "Epoch 846/1500\n",
      "36/36 [==============================] - 0s 679us/step - loss: 2.9779 - val_loss: 13.8287\n",
      "Epoch 847/1500\n",
      "36/36 [==============================] - 0s 540us/step - loss: 1.4510 - val_loss: 15.5188\n",
      "Epoch 848/1500\n",
      "36/36 [==============================] - 0s 440us/step - loss: 1.4378 - val_loss: 11.3048\n",
      "Epoch 849/1500\n",
      "36/36 [==============================] - 0s 437us/step - loss: 3.6104 - val_loss: 16.0653\n",
      "Epoch 850/1500\n",
      "36/36 [==============================] - 0s 472us/step - loss: 1.7038 - val_loss: 21.9185\n",
      "Epoch 851/1500\n",
      "36/36 [==============================] - 0s 607us/step - loss: 4.3618 - val_loss: 16.4206\n",
      "Epoch 852/1500\n",
      "36/36 [==============================] - 0s 540us/step - loss: 1.3615 - val_loss: 12.0548\n",
      "Epoch 853/1500\n",
      "36/36 [==============================] - 0s 900us/step - loss: 3.4295 - val_loss: 17.0594\n",
      "Epoch 854/1500\n",
      "36/36 [==============================] - 0s 617us/step - loss: 2.0793 - val_loss: 17.9422\n",
      "Epoch 855/1500\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 2.2875 - val_loss: 10.1847\n",
      "Epoch 856/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 3.2254 - val_loss: 11.6151\n",
      "Epoch 857/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 650us/step - loss: 1.9791 - val_loss: 17.4595\n",
      "Epoch 858/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 4.4117 - val_loss: 11.6575\n",
      "Epoch 859/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.8660 - val_loss: 7.0222\n",
      "Epoch 860/1500\n",
      "36/36 [==============================] - 0s 635us/step - loss: 3.4986 - val_loss: 10.7851\n",
      "Epoch 861/1500\n",
      "36/36 [==============================] - 0s 429us/step - loss: 2.6414 - val_loss: 12.6056\n",
      "Epoch 862/1500\n",
      "36/36 [==============================] - 0s 731us/step - loss: 3.1514 - val_loss: 8.2523\n",
      "Epoch 863/1500\n",
      "36/36 [==============================] - 0s 350us/step - loss: 3.5011 - val_loss: 9.2204\n",
      "Epoch 864/1500\n",
      "36/36 [==============================] - 0s 380us/step - loss: 3.7064 - val_loss: 16.1118\n",
      "Epoch 865/1500\n",
      "36/36 [==============================] - 0s 377us/step - loss: 3.5954 - val_loss: 17.9279\n",
      "Epoch 866/1500\n",
      "36/36 [==============================] - 0s 452us/step - loss: 3.6867 - val_loss: 15.1652\n",
      "Epoch 867/1500\n",
      "36/36 [==============================] - 0s 353us/step - loss: 2.6290 - val_loss: 17.2953\n",
      "Epoch 868/1500\n",
      "36/36 [==============================] - 0s 372us/step - loss: 2.4667 - val_loss: 16.9731\n",
      "Epoch 869/1500\n",
      "36/36 [==============================] - 0s 375us/step - loss: 2.5486 - val_loss: 14.6654\n",
      "Epoch 870/1500\n",
      "36/36 [==============================] - 0s 361us/step - loss: 1.8046 - val_loss: 11.0966\n",
      "Epoch 871/1500\n",
      "36/36 [==============================] - 0s 398us/step - loss: 1.2087 - val_loss: 7.6895\n",
      "Epoch 872/1500\n",
      "36/36 [==============================] - 0s 347us/step - loss: 2.7207 - val_loss: 7.7370\n",
      "Epoch 873/1500\n",
      "36/36 [==============================] - 0s 430us/step - loss: 1.6900 - val_loss: 12.4704\n",
      "Epoch 874/1500\n",
      "36/36 [==============================] - 0s 343us/step - loss: 3.3177 - val_loss: 10.2883\n",
      "Epoch 875/1500\n",
      "36/36 [==============================] - 0s 382us/step - loss: 2.1847 - val_loss: 8.0583\n",
      "Epoch 876/1500\n",
      "36/36 [==============================] - 0s 344us/step - loss: 2.7368 - val_loss: 16.6901\n",
      "Epoch 877/1500\n",
      "36/36 [==============================] - 0s 332us/step - loss: 3.1429 - val_loss: 17.9740\n",
      "Epoch 878/1500\n",
      "36/36 [==============================] - 0s 369us/step - loss: 3.2708 - val_loss: 10.0363\n",
      "Epoch 879/1500\n",
      "36/36 [==============================] - 0s 363us/step - loss: 2.8021 - val_loss: 8.0556\n",
      "Epoch 880/1500\n",
      "36/36 [==============================] - 0s 374us/step - loss: 2.3039 - val_loss: 12.3709\n",
      "Epoch 881/1500\n",
      "36/36 [==============================] - 0s 355us/step - loss: 2.6777 - val_loss: 10.3034\n",
      "Epoch 882/1500\n",
      "36/36 [==============================] - 0s 374us/step - loss: 1.6741 - val_loss: 9.9012\n",
      "Epoch 883/1500\n",
      "36/36 [==============================] - 0s 396us/step - loss: 2.2953 - val_loss: 15.2730\n",
      "Epoch 884/1500\n",
      "36/36 [==============================] - 0s 417us/step - loss: 2.1423 - val_loss: 15.5617\n",
      "Epoch 885/1500\n",
      "36/36 [==============================] - 0s 442us/step - loss: 2.0222 - val_loss: 10.2771\n",
      "Epoch 886/1500\n",
      "36/36 [==============================] - 0s 534us/step - loss: 2.8792 - val_loss: 10.7680\n",
      "Epoch 887/1500\n",
      "36/36 [==============================] - 0s 548us/step - loss: 1.9964 - val_loss: 13.6297\n",
      "Epoch 888/1500\n",
      "36/36 [==============================] - 0s 434us/step - loss: 3.3137 - val_loss: 10.7345\n",
      "Epoch 889/1500\n",
      "36/36 [==============================] - 0s 434us/step - loss: 2.1019 - val_loss: 8.6448\n",
      "Epoch 890/1500\n",
      "36/36 [==============================] - 0s 598us/step - loss: 2.7717 - val_loss: 12.5991\n",
      "Epoch 891/1500\n",
      "36/36 [==============================] - 0s 645us/step - loss: 1.5326 - val_loss: 15.9065\n",
      "Epoch 892/1500\n",
      "36/36 [==============================] - 0s 806us/step - loss: 2.1145 - val_loss: 13.1565\n",
      "Epoch 893/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.2316 - val_loss: 12.2091\n",
      "Epoch 894/1500\n",
      "36/36 [==============================] - 0s 415us/step - loss: 1.3889 - val_loss: 12.7016\n",
      "Epoch 895/1500\n",
      "36/36 [==============================] - 0s 684us/step - loss: 1.3016 - val_loss: 14.1462\n",
      "Epoch 896/1500\n",
      "36/36 [==============================] - 0s 988us/step - loss: 1.1630 - val_loss: 17.7182\n",
      "Epoch 897/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 2.8556 - val_loss: 15.0265\n",
      "Epoch 898/1500\n",
      "36/36 [==============================] - 0s 998us/step - loss: 2.0224 - val_loss: 9.8053\n",
      "Epoch 899/1500\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 3.1644 - val_loss: 14.8091\n",
      "Epoch 900/1500\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 2.6718 - val_loss: 17.9839\n",
      "Epoch 901/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 4.8851 - val_loss: 11.3193\n",
      "Epoch 902/1500\n",
      "36/36 [==============================] - 0s 382us/step - loss: 2.1100 - val_loss: 5.0145\n",
      "Epoch 903/1500\n",
      "36/36 [==============================] - 0s 737us/step - loss: 4.7276 - val_loss: 5.0178\n",
      "Epoch 904/1500\n",
      "36/36 [==============================] - 0s 863us/step - loss: 2.9107 - val_loss: 9.5850\n",
      "Epoch 905/1500\n",
      "36/36 [==============================] - 0s 690us/step - loss: 2.4690 - val_loss: 11.0708\n",
      "Epoch 906/1500\n",
      "36/36 [==============================] - 0s 748us/step - loss: 2.2301 - val_loss: 10.1813\n",
      "Epoch 907/1500\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 1.8258 - val_loss: 10.7757\n",
      "Epoch 908/1500\n",
      "36/36 [==============================] - 0s 945us/step - loss: 1.7837 - val_loss: 16.2757\n",
      "Epoch 909/1500\n",
      "36/36 [==============================] - 0s 791us/step - loss: 2.9168 - val_loss: 12.2422\n",
      "Epoch 910/1500\n",
      "36/36 [==============================] - 0s 834us/step - loss: 1.6130 - val_loss: 11.2880\n",
      "Epoch 911/1500\n",
      "36/36 [==============================] - 0s 414us/step - loss: 1.7399 - val_loss: 14.1974\n",
      "Epoch 912/1500\n",
      "36/36 [==============================] - 0s 359us/step - loss: 1.9094 - val_loss: 11.8664\n",
      "Epoch 913/1500\n",
      "36/36 [==============================] - 0s 473us/step - loss: 1.5503 - val_loss: 15.0919\n",
      "Epoch 914/1500\n",
      "36/36 [==============================] - 0s 471us/step - loss: 2.4918 - val_loss: 12.5661\n",
      "Epoch 915/1500\n",
      "36/36 [==============================] - 0s 444us/step - loss: 1.5424 - val_loss: 8.6423\n",
      "Epoch 916/1500\n",
      "36/36 [==============================] - 0s 458us/step - loss: 3.0040 - val_loss: 9.9587\n",
      "Epoch 917/1500\n",
      "36/36 [==============================] - 0s 329us/step - loss: 1.8038 - val_loss: 10.9672\n",
      "Epoch 918/1500\n",
      "36/36 [==============================] - 0s 384us/step - loss: 2.2296 - val_loss: 10.2661\n",
      "Epoch 919/1500\n",
      "36/36 [==============================] - 0s 541us/step - loss: 1.9445 - val_loss: 12.4020\n",
      "Epoch 920/1500\n",
      "36/36 [==============================] - 0s 550us/step - loss: 1.7359 - val_loss: 18.3529\n",
      "Epoch 921/1500\n",
      "36/36 [==============================] - 0s 639us/step - loss: 2.6879 - val_loss: 17.9929\n",
      "Epoch 922/1500\n",
      "36/36 [==============================] - 0s 426us/step - loss: 2.4011 - val_loss: 12.4287\n",
      "Epoch 923/1500\n",
      "36/36 [==============================] - 0s 515us/step - loss: 2.0507 - val_loss: 12.5600\n",
      "Epoch 924/1500\n",
      "36/36 [==============================] - 0s 336us/step - loss: 1.7685 - val_loss: 12.7262\n",
      "Epoch 925/1500\n",
      "36/36 [==============================] - 0s 338us/step - loss: 2.1098 - val_loss: 9.2451\n",
      "Epoch 926/1500\n",
      "36/36 [==============================] - 0s 382us/step - loss: 2.0308 - val_loss: 12.3131\n",
      "Epoch 927/1500\n",
      "36/36 [==============================] - 0s 418us/step - loss: 1.7602 - val_loss: 11.6081\n",
      "Epoch 928/1500\n",
      "36/36 [==============================] - 0s 432us/step - loss: 1.5384 - val_loss: 6.2747\n",
      "Epoch 929/1500\n",
      "36/36 [==============================] - 0s 471us/step - loss: 2.6890 - val_loss: 9.5953\n",
      "Epoch 930/1500\n",
      "36/36 [==============================] - 0s 600us/step - loss: 2.7606 - val_loss: 11.8029\n",
      "Epoch 931/1500\n",
      "36/36 [==============================] - 0s 459us/step - loss: 3.2745 - val_loss: 7.5496\n",
      "Epoch 932/1500\n",
      "36/36 [==============================] - 0s 388us/step - loss: 4.0648 - val_loss: 9.1718\n",
      "Epoch 933/1500\n",
      "36/36 [==============================] - 0s 504us/step - loss: 5.6382 - val_loss: 17.1597\n",
      "Epoch 934/1500\n",
      "36/36 [==============================] - 0s 449us/step - loss: 2.7681 - val_loss: 24.1767\n",
      "Epoch 935/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 462us/step - loss: 5.9216 - val_loss: 17.0068\n",
      "Epoch 936/1500\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 2.8152 - val_loss: 9.6614\n",
      "Epoch 937/1500\n",
      "36/36 [==============================] - 0s 389us/step - loss: 5.1614 - val_loss: 10.1508\n",
      "Epoch 938/1500\n",
      "36/36 [==============================] - 0s 625us/step - loss: 2.2126 - val_loss: 14.1221\n",
      "Epoch 939/1500\n",
      "36/36 [==============================] - 0s 589us/step - loss: 3.4852 - val_loss: 10.7164\n",
      "Epoch 940/1500\n",
      "36/36 [==============================] - 0s 335us/step - loss: 2.6447 - val_loss: 13.0968\n",
      "Epoch 941/1500\n",
      "36/36 [==============================] - 0s 421us/step - loss: 1.6616 - val_loss: 21.9713\n",
      "Epoch 942/1500\n",
      "36/36 [==============================] - 0s 434us/step - loss: 3.5572 - val_loss: 24.0829\n",
      "Epoch 943/1500\n",
      "36/36 [==============================] - 0s 449us/step - loss: 3.6833 - val_loss: 19.6654\n",
      "Epoch 944/1500\n",
      "36/36 [==============================] - 0s 445us/step - loss: 3.6859 - val_loss: 19.3278\n",
      "Epoch 945/1500\n",
      "36/36 [==============================] - 0s 438us/step - loss: 2.6006 - val_loss: 18.6031\n",
      "Epoch 946/1500\n",
      "36/36 [==============================] - 0s 462us/step - loss: 2.3219 - val_loss: 11.9983\n",
      "Epoch 947/1500\n",
      "36/36 [==============================] - 0s 887us/step - loss: 3.6825 - val_loss: 9.3620\n",
      "Epoch 948/1500\n",
      "36/36 [==============================] - 0s 969us/step - loss: 3.5066 - val_loss: 13.9212\n",
      "Epoch 949/1500\n",
      "36/36 [==============================] - 0s 701us/step - loss: 3.2792 - val_loss: 15.2771\n",
      "Epoch 950/1500\n",
      "36/36 [==============================] - 0s 661us/step - loss: 3.2786 - val_loss: 13.1889\n",
      "Epoch 951/1500\n",
      "36/36 [==============================] - 0s 772us/step - loss: 3.1723 - val_loss: 14.5332\n",
      "Epoch 952/1500\n",
      "36/36 [==============================] - 0s 721us/step - loss: 3.1517 - val_loss: 17.7398\n",
      "Epoch 953/1500\n",
      "36/36 [==============================] - 0s 540us/step - loss: 2.9151 - val_loss: 17.9762\n",
      "Epoch 954/1500\n",
      "36/36 [==============================] - 0s 394us/step - loss: 2.0656 - val_loss: 15.2254\n",
      "Epoch 955/1500\n",
      "36/36 [==============================] - 0s 653us/step - loss: 2.5072 - val_loss: 15.5379\n",
      "Epoch 956/1500\n",
      "36/36 [==============================] - 0s 484us/step - loss: 2.7927 - val_loss: 15.8690\n",
      "Epoch 957/1500\n",
      "36/36 [==============================] - 0s 620us/step - loss: 2.5346 - val_loss: 16.8155\n",
      "Epoch 958/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 2.2944 - val_loss: 18.9717\n",
      "Epoch 959/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 2.1098 - val_loss: 17.9133\n",
      "Epoch 960/1500\n",
      "36/36 [==============================] - 0s 886us/step - loss: 1.7204 - val_loss: 9.3537\n",
      "Epoch 961/1500\n",
      "36/36 [==============================] - 0s 929us/step - loss: 3.9272 - val_loss: 8.1596\n",
      "Epoch 962/1500\n",
      "36/36 [==============================] - 0s 981us/step - loss: 3.7896 - val_loss: 17.1193\n",
      "Epoch 963/1500\n",
      "36/36 [==============================] - 0s 945us/step - loss: 3.2828 - val_loss: 17.5875\n",
      "Epoch 964/1500\n",
      "36/36 [==============================] - 0s 304us/step - loss: 2.9811 - val_loss: 10.8276\n",
      "Epoch 965/1500\n",
      "36/36 [==============================] - 0s 547us/step - loss: 2.8950 - val_loss: 12.6310\n",
      "Epoch 966/1500\n",
      "36/36 [==============================] - 0s 589us/step - loss: 1.8315 - val_loss: 18.2949\n",
      "Epoch 967/1500\n",
      "36/36 [==============================] - 0s 629us/step - loss: 3.3885 - val_loss: 14.6125\n",
      "Epoch 968/1500\n",
      "36/36 [==============================] - 0s 606us/step - loss: 1.9101 - val_loss: 10.0647\n",
      "Epoch 969/1500\n",
      "36/36 [==============================] - 0s 436us/step - loss: 2.9296 - val_loss: 13.7852\n",
      "Epoch 970/1500\n",
      "36/36 [==============================] - 0s 626us/step - loss: 2.0910 - val_loss: 13.5106\n",
      "Epoch 971/1500\n",
      "36/36 [==============================] - 0s 402us/step - loss: 2.2840 - val_loss: 10.8616\n",
      "Epoch 972/1500\n",
      "36/36 [==============================] - 0s 562us/step - loss: 1.8538 - val_loss: 15.1653\n",
      "Epoch 973/1500\n",
      "36/36 [==============================] - 0s 591us/step - loss: 2.8842 - val_loss: 12.5647\n",
      "Epoch 974/1500\n",
      "36/36 [==============================] - 0s 363us/step - loss: 1.6578 - val_loss: 11.2303\n",
      "Epoch 975/1500\n",
      "36/36 [==============================] - 0s 443us/step - loss: 2.2490 - val_loss: 14.4903\n",
      "Epoch 976/1500\n",
      "36/36 [==============================] - 0s 538us/step - loss: 1.5676 - val_loss: 18.9834\n",
      "Epoch 977/1500\n",
      "36/36 [==============================] - 0s 469us/step - loss: 3.0460 - val_loss: 17.7353\n",
      "Epoch 978/1500\n",
      "36/36 [==============================] - 0s 455us/step - loss: 2.2866 - val_loss: 11.0738\n",
      "Epoch 979/1500\n",
      "36/36 [==============================] - 0s 414us/step - loss: 2.8738 - val_loss: 13.1565\n",
      "Epoch 980/1500\n",
      "36/36 [==============================] - 0s 494us/step - loss: 1.3779 - val_loss: 14.4297\n",
      "Epoch 981/1500\n",
      "36/36 [==============================] - 0s 416us/step - loss: 1.6807 - val_loss: 11.4502\n",
      "Epoch 982/1500\n",
      "36/36 [==============================] - 0s 371us/step - loss: 1.8181 - val_loss: 15.7433\n",
      "Epoch 983/1500\n",
      "36/36 [==============================] - 0s 385us/step - loss: 1.6775 - val_loss: 13.7454\n",
      "Epoch 984/1500\n",
      "36/36 [==============================] - 0s 466us/step - loss: 1.9565 - val_loss: 14.2283\n",
      "Epoch 985/1500\n",
      "36/36 [==============================] - 0s 473us/step - loss: 2.4158 - val_loss: 20.3141\n",
      "Epoch 986/1500\n",
      "36/36 [==============================] - 0s 471us/step - loss: 3.2948 - val_loss: 17.1999\n",
      "Epoch 987/1500\n",
      "36/36 [==============================] - 0s 382us/step - loss: 2.2297 - val_loss: 10.0338\n",
      "Epoch 988/1500\n",
      "36/36 [==============================] - 0s 392us/step - loss: 2.8495 - val_loss: 11.5264\n",
      "Epoch 989/1500\n",
      "36/36 [==============================] - 0s 507us/step - loss: 2.5640 - val_loss: 11.4986\n",
      "Epoch 990/1500\n",
      "36/36 [==============================] - 0s 374us/step - loss: 2.7263 - val_loss: 9.9774\n",
      "Epoch 991/1500\n",
      "36/36 [==============================] - 0s 462us/step - loss: 2.2698 - val_loss: 13.6261\n",
      "Epoch 992/1500\n",
      "36/36 [==============================] - 0s 463us/step - loss: 1.9064 - val_loss: 15.2254\n",
      "Epoch 993/1500\n",
      "36/36 [==============================] - 0s 379us/step - loss: 1.8885 - val_loss: 15.9104\n",
      "Epoch 994/1500\n",
      "36/36 [==============================] - 0s 376us/step - loss: 1.5970 - val_loss: 16.4867\n",
      "Epoch 995/1500\n",
      "36/36 [==============================] - 0s 467us/step - loss: 1.7537 - val_loss: 19.0003\n",
      "Epoch 996/1500\n",
      "36/36 [==============================] - 0s 494us/step - loss: 3.0764 - val_loss: 15.3980\n",
      "Epoch 997/1500\n",
      "36/36 [==============================] - 0s 479us/step - loss: 1.7776 - val_loss: 10.8078\n",
      "Epoch 998/1500\n",
      "36/36 [==============================] - 0s 492us/step - loss: 2.8639 - val_loss: 15.8891\n",
      "Epoch 999/1500\n",
      "36/36 [==============================] - 0s 382us/step - loss: 2.7306 - val_loss: 13.6329\n",
      "Epoch 1000/1500\n",
      "36/36 [==============================] - 0s 381us/step - loss: 1.4686 - val_loss: 9.8223\n",
      "Epoch 1001/1500\n",
      "36/36 [==============================] - 0s 533us/step - loss: 3.0732 - val_loss: 14.2047\n",
      "Epoch 1002/1500\n",
      "36/36 [==============================] - 0s 573us/step - loss: 1.9058 - val_loss: 15.8374\n",
      "Epoch 1003/1500\n",
      "36/36 [==============================] - 0s 382us/step - loss: 2.8578 - val_loss: 10.6556\n",
      "Epoch 1004/1500\n",
      "36/36 [==============================] - 0s 384us/step - loss: 2.3290 - val_loss: 11.2875\n",
      "Epoch 1005/1500\n",
      "36/36 [==============================] - 0s 504us/step - loss: 1.6003 - val_loss: 9.7883\n",
      "Epoch 1006/1500\n",
      "36/36 [==============================] - 0s 418us/step - loss: 2.5927 - val_loss: 9.1580\n",
      "Epoch 1007/1500\n",
      "36/36 [==============================] - 0s 945us/step - loss: 3.0620 - val_loss: 11.1849\n",
      "Epoch 1008/1500\n",
      "36/36 [==============================] - 0s 398us/step - loss: 2.2623 - val_loss: 10.9115\n",
      "Epoch 1009/1500\n",
      "36/36 [==============================] - 0s 433us/step - loss: 2.0195 - val_loss: 14.1925\n",
      "Epoch 1010/1500\n",
      "36/36 [==============================] - 0s 433us/step - loss: 1.8264 - val_loss: 15.7184\n",
      "Epoch 1011/1500\n",
      "36/36 [==============================] - 0s 509us/step - loss: 2.6758 - val_loss: 9.9644\n",
      "Epoch 1012/1500\n",
      "36/36 [==============================] - 0s 401us/step - loss: 1.8474 - val_loss: 8.5965\n",
      "Epoch 1013/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 511us/step - loss: 1.6905 - val_loss: 10.9307\n",
      "Epoch 1014/1500\n",
      "36/36 [==============================] - 0s 329us/step - loss: 2.1635 - val_loss: 9.8462\n",
      "Epoch 1015/1500\n",
      "36/36 [==============================] - 0s 423us/step - loss: 1.8433 - val_loss: 11.8293\n",
      "Epoch 1016/1500\n",
      "36/36 [==============================] - 0s 387us/step - loss: 1.6608 - val_loss: 16.5359\n",
      "Epoch 1017/1500\n",
      "36/36 [==============================] - 0s 426us/step - loss: 1.8942 - val_loss: 13.9938\n",
      "Epoch 1018/1500\n",
      "36/36 [==============================] - 0s 433us/step - loss: 2.8658 - val_loss: 13.2993\n",
      "Epoch 1019/1500\n",
      "36/36 [==============================] - 0s 410us/step - loss: 3.0508 - val_loss: 18.3875\n",
      "Epoch 1020/1500\n",
      "36/36 [==============================] - 0s 501us/step - loss: 2.9421 - val_loss: 16.5259\n",
      "Epoch 1021/1500\n",
      "36/36 [==============================] - 0s 420us/step - loss: 2.2886 - val_loss: 11.6414\n",
      "Epoch 1022/1500\n",
      "36/36 [==============================] - 0s 364us/step - loss: 2.4172 - val_loss: 13.0404\n",
      "Epoch 1023/1500\n",
      "36/36 [==============================] - 0s 435us/step - loss: 2.5197 - val_loss: 10.9945\n",
      "Epoch 1024/1500\n",
      "36/36 [==============================] - 0s 345us/step - loss: 2.5170 - val_loss: 11.2236\n",
      "Epoch 1025/1500\n",
      "36/36 [==============================] - 0s 340us/step - loss: 2.0697 - val_loss: 15.9634\n",
      "Epoch 1026/1500\n",
      "36/36 [==============================] - 0s 372us/step - loss: 1.9770 - val_loss: 13.5698\n",
      "Epoch 1027/1500\n",
      "36/36 [==============================] - 0s 366us/step - loss: 2.3733 - val_loss: 15.8662\n",
      "Epoch 1028/1500\n",
      "36/36 [==============================] - 0s 343us/step - loss: 1.7262 - val_loss: 16.9992\n",
      "Epoch 1029/1500\n",
      "36/36 [==============================] - 0s 330us/step - loss: 1.4479 - val_loss: 12.6739\n",
      "Epoch 1030/1500\n",
      "36/36 [==============================] - 0s 362us/step - loss: 2.5099 - val_loss: 14.2781\n",
      "Epoch 1031/1500\n",
      "36/36 [==============================] - 0s 354us/step - loss: 1.5703 - val_loss: 19.8709\n",
      "Epoch 1032/1500\n",
      "36/36 [==============================] - 0s 421us/step - loss: 3.6751 - val_loss: 15.7784\n",
      "Epoch 1033/1500\n",
      "36/36 [==============================] - 0s 440us/step - loss: 1.5869 - val_loss: 9.6232\n",
      "Epoch 1034/1500\n",
      "36/36 [==============================] - 0s 352us/step - loss: 3.5229 - val_loss: 13.2787\n",
      "Epoch 1035/1500\n",
      "36/36 [==============================] - 0s 339us/step - loss: 1.7639 - val_loss: 15.3650\n",
      "Epoch 1036/1500\n",
      "36/36 [==============================] - 0s 390us/step - loss: 3.1868 - val_loss: 9.4968\n",
      "Epoch 1037/1500\n",
      "36/36 [==============================] - 0s 544us/step - loss: 2.2944 - val_loss: 10.8051\n",
      "Epoch 1038/1500\n",
      "36/36 [==============================] - 0s 400us/step - loss: 1.7028 - val_loss: 18.1370\n",
      "Epoch 1039/1500\n",
      "36/36 [==============================] - 0s 352us/step - loss: 3.9151 - val_loss: 18.0551\n",
      "Epoch 1040/1500\n",
      "36/36 [==============================] - 0s 367us/step - loss: 2.6389 - val_loss: 12.1117\n",
      "Epoch 1041/1500\n",
      "36/36 [==============================] - 0s 394us/step - loss: 3.7177 - val_loss: 10.9873\n",
      "Epoch 1042/1500\n",
      "36/36 [==============================] - ETA: 0s - loss: 4.194 - 0s 508us/step - loss: 4.0039 - val_loss: 17.0918\n",
      "Epoch 1043/1500\n",
      "36/36 [==============================] - 0s 389us/step - loss: 2.5546 - val_loss: 19.0247\n",
      "Epoch 1044/1500\n",
      "36/36 [==============================] - 0s 362us/step - loss: 3.9329 - val_loss: 12.1544\n",
      "Epoch 1045/1500\n",
      "36/36 [==============================] - 0s 336us/step - loss: 1.9894 - val_loss: 7.8281\n",
      "Epoch 1046/1500\n",
      "36/36 [==============================] - 0s 353us/step - loss: 2.9896 - val_loss: 12.3200\n",
      "Epoch 1047/1500\n",
      "36/36 [==============================] - 0s 332us/step - loss: 2.6922 - val_loss: 13.5052\n",
      "Epoch 1048/1500\n",
      "36/36 [==============================] - 0s 389us/step - loss: 2.0835 - val_loss: 11.1614\n",
      "Epoch 1049/1500\n",
      "36/36 [==============================] - 0s 355us/step - loss: 2.4722 - val_loss: 14.6279\n",
      "Epoch 1050/1500\n",
      "36/36 [==============================] - 0s 387us/step - loss: 1.5305 - val_loss: 17.2860\n",
      "Epoch 1051/1500\n",
      "36/36 [==============================] - 0s 382us/step - loss: 1.9443 - val_loss: 13.5647\n",
      "Epoch 1052/1500\n",
      "36/36 [==============================] - 0s 386us/step - loss: 2.3246 - val_loss: 12.7455\n",
      "Epoch 1053/1500\n",
      "36/36 [==============================] - 0s 502us/step - loss: 2.6577 - val_loss: 16.2312\n",
      "Epoch 1054/1500\n",
      "36/36 [==============================] - ETA: 0s - loss: 1.376 - 0s 459us/step - loss: 1.5316 - val_loss: 13.1468\n",
      "Epoch 1055/1500\n",
      "36/36 [==============================] - 0s 475us/step - loss: 0.9832 - val_loss: 7.8309\n",
      "Epoch 1056/1500\n",
      "36/36 [==============================] - 0s 520us/step - loss: 2.7978 - val_loss: 13.2078\n",
      "Epoch 1057/1500\n",
      "36/36 [==============================] - 0s 409us/step - loss: 2.4388 - val_loss: 14.6825\n",
      "Epoch 1058/1500\n",
      "36/36 [==============================] - 0s 412us/step - loss: 2.8244 - val_loss: 11.0370\n",
      "Epoch 1059/1500\n",
      "36/36 [==============================] - 0s 510us/step - loss: 2.0124 - val_loss: 13.9362\n",
      "Epoch 1060/1500\n",
      "36/36 [==============================] - 0s 428us/step - loss: 1.6310 - val_loss: 17.3691\n",
      "Epoch 1061/1500\n",
      "36/36 [==============================] - 0s 487us/step - loss: 2.5043 - val_loss: 14.0696\n",
      "Epoch 1062/1500\n",
      "36/36 [==============================] - 0s 472us/step - loss: 1.5546 - val_loss: 11.4646\n",
      "Epoch 1063/1500\n",
      "36/36 [==============================] - 0s 340us/step - loss: 1.5328 - val_loss: 15.4297\n",
      "Epoch 1064/1500\n",
      "36/36 [==============================] - 0s 395us/step - loss: 3.5450 - val_loss: 12.3298\n",
      "Epoch 1065/1500\n",
      "36/36 [==============================] - ETA: 0s - loss: 3.108 - 0s 354us/step - loss: 3.1297 - val_loss: 8.0038\n",
      "Epoch 1066/1500\n",
      "36/36 [==============================] - 0s 399us/step - loss: 2.4216 - val_loss: 14.0484\n",
      "Epoch 1067/1500\n",
      "36/36 [==============================] - 0s 383us/step - loss: 3.0279 - val_loss: 16.5754\n",
      "Epoch 1068/1500\n",
      "36/36 [==============================] - 0s 369us/step - loss: 3.3415 - val_loss: 14.1053\n",
      "Epoch 1069/1500\n",
      "36/36 [==============================] - 0s 349us/step - loss: 3.0538 - val_loss: 12.9517\n",
      "Epoch 1070/1500\n",
      "36/36 [==============================] - 0s 440us/step - loss: 3.1679 - val_loss: 15.0026\n",
      "Epoch 1071/1500\n",
      "36/36 [==============================] - 0s 481us/step - loss: 2.2106 - val_loss: 14.4532\n",
      "Epoch 1072/1500\n",
      "36/36 [==============================] - 0s 500us/step - loss: 2.2506 - val_loss: 11.6831\n",
      "Epoch 1073/1500\n",
      "36/36 [==============================] - 0s 491us/step - loss: 2.4600 - val_loss: 8.1829\n",
      "Epoch 1074/1500\n",
      "36/36 [==============================] - 0s 503us/step - loss: 3.3830 - val_loss: 9.9663\n",
      "Epoch 1075/1500\n",
      "36/36 [==============================] - 0s 506us/step - loss: 2.4193 - val_loss: 14.8581\n",
      "Epoch 1076/1500\n",
      "36/36 [==============================] - 0s 489us/step - loss: 2.6299 - val_loss: 10.9020\n",
      "Epoch 1077/1500\n",
      "36/36 [==============================] - 0s 413us/step - loss: 2.2371 - val_loss: 13.0595\n",
      "Epoch 1078/1500\n",
      "36/36 [==============================] - 0s 497us/step - loss: 1.8255 - val_loss: 16.7456\n",
      "Epoch 1079/1500\n",
      "36/36 [==============================] - 0s 428us/step - loss: 2.2847 - val_loss: 12.7659\n",
      "Epoch 1080/1500\n",
      "36/36 [==============================] - 0s 468us/step - loss: 1.6835 - val_loss: 12.2525\n",
      "Epoch 1081/1500\n",
      "36/36 [==============================] - 0s 533us/step - loss: 1.5447 - val_loss: 12.9110\n",
      "Epoch 1082/1500\n",
      "36/36 [==============================] - 0s 462us/step - loss: 1.1415 - val_loss: 12.2015\n",
      "Epoch 1083/1500\n",
      "36/36 [==============================] - 0s 423us/step - loss: 1.5824 - val_loss: 15.0388\n",
      "Epoch 1084/1500\n",
      "36/36 [==============================] - 0s 537us/step - loss: 1.3773 - val_loss: 13.6768\n",
      "Epoch 1085/1500\n",
      "36/36 [==============================] - 0s 426us/step - loss: 2.0728 - val_loss: 15.8204\n",
      "Epoch 1086/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.7443 - val_loss: 17.7225\n",
      "Epoch 1087/1500\n",
      "36/36 [==============================] - 0s 515us/step - loss: 2.5151 - val_loss: 10.6727\n",
      "Epoch 1088/1500\n",
      "36/36 [==============================] - 0s 737us/step - loss: 2.0658 - val_loss: 9.4635\n",
      "Epoch 1089/1500\n",
      "36/36 [==============================] - 0s 674us/step - loss: 1.4850 - val_loss: 12.3966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1090/1500\n",
      "36/36 [==============================] - 0s 398us/step - loss: 2.7429 - val_loss: 10.9167\n",
      "Epoch 1091/1500\n",
      "36/36 [==============================] - 0s 468us/step - loss: 1.5502 - val_loss: 8.9923\n",
      "Epoch 1092/1500\n",
      "36/36 [==============================] - 0s 377us/step - loss: 2.8876 - val_loss: 13.2867\n",
      "Epoch 1093/1500\n",
      "36/36 [==============================] - 0s 453us/step - loss: 1.8155 - val_loss: 20.2862\n",
      "Epoch 1094/1500\n",
      "36/36 [==============================] - 0s 318us/step - loss: 5.1190 - val_loss: 16.8489\n",
      "Epoch 1095/1500\n",
      "36/36 [==============================] - 0s 330us/step - loss: 3.1391 - val_loss: 7.4730\n",
      "Epoch 1096/1500\n",
      "36/36 [==============================] - 0s 334us/step - loss: 3.3112 - val_loss: 9.1180\n",
      "Epoch 1097/1500\n",
      "36/36 [==============================] - 0s 699us/step - loss: 1.4817 - val_loss: 13.1950\n",
      "Epoch 1098/1500\n",
      "36/36 [==============================] - 0s 788us/step - loss: 3.3424 - val_loss: 13.9764\n",
      "Epoch 1099/1500\n",
      "36/36 [==============================] - 0s 708us/step - loss: 2.8880 - val_loss: 9.9374\n",
      "Epoch 1100/1500\n",
      "36/36 [==============================] - 0s 729us/step - loss: 2.0927 - val_loss: 9.4326\n",
      "Epoch 1101/1500\n",
      "36/36 [==============================] - 0s 402us/step - loss: 2.8179 - val_loss: 15.8810\n",
      "Epoch 1102/1500\n",
      "36/36 [==============================] - 0s 477us/step - loss: 1.4417 - val_loss: 15.4313\n",
      "Epoch 1103/1500\n",
      "36/36 [==============================] - 0s 879us/step - loss: 1.4377 - val_loss: 16.5796\n",
      "Epoch 1104/1500\n",
      "36/36 [==============================] - 0s 677us/step - loss: 1.5290 - val_loss: 16.4659\n",
      "Epoch 1105/1500\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4430 - val_loss: 10.4453\n",
      "Epoch 1106/1500\n",
      "36/36 [==============================] - 0s 356us/step - loss: 3.1544 - val_loss: 11.8383\n",
      "Epoch 1107/1500\n",
      "36/36 [==============================] - 0s 366us/step - loss: 2.0047 - val_loss: 17.1727\n",
      "Epoch 1108/1500\n",
      "36/36 [==============================] - 0s 336us/step - loss: 2.8629 - val_loss: 14.2550\n",
      "Epoch 1109/1500\n",
      "36/36 [==============================] - 0s 355us/step - loss: 1.9719 - val_loss: 14.2737\n",
      "Epoch 1110/1500\n",
      "36/36 [==============================] - 0s 341us/step - loss: 3.1090 - val_loss: 18.9522\n",
      "Epoch 1111/1500\n",
      "36/36 [==============================] - 0s 387us/step - loss: 3.1320 - val_loss: 17.8517\n",
      "Epoch 1112/1500\n",
      "36/36 [==============================] - 0s 350us/step - loss: 2.6304 - val_loss: 10.3100\n",
      "Epoch 1113/1500\n",
      "36/36 [==============================] - 0s 388us/step - loss: 2.0981 - val_loss: 10.9359\n",
      "Epoch 1114/1500\n",
      "36/36 [==============================] - 0s 418us/step - loss: 1.9558 - val_loss: 12.2459\n",
      "Epoch 1115/1500\n",
      "36/36 [==============================] - 0s 592us/step - loss: 2.4525 - val_loss: 8.5339\n",
      "Epoch 1116/1500\n",
      "36/36 [==============================] - 0s 478us/step - loss: 2.5855 - val_loss: 10.3408\n",
      "Epoch 1117/1500\n",
      "36/36 [==============================] - 0s 434us/step - loss: 2.3433 - val_loss: 15.9949\n",
      "Epoch 1118/1500\n",
      "36/36 [==============================] - 0s 519us/step - loss: 2.5537 - val_loss: 10.7141\n",
      "Epoch 1119/1500\n",
      "36/36 [==============================] - 0s 379us/step - loss: 1.7980 - val_loss: 7.9324\n",
      "Epoch 1120/1500\n",
      "36/36 [==============================] - 0s 361us/step - loss: 2.7240 - val_loss: 13.9254\n",
      "Epoch 1121/1500\n",
      "36/36 [==============================] - 0s 368us/step - loss: 2.1460 - val_loss: 13.2389\n",
      "Epoch 1122/1500\n",
      "36/36 [==============================] - 0s 347us/step - loss: 1.2047 - val_loss: 8.4888\n",
      "Epoch 1123/1500\n",
      "36/36 [==============================] - 0s 343us/step - loss: 4.1610 - val_loss: 11.7896\n",
      "Epoch 1124/1500\n",
      "36/36 [==============================] - 0s 353us/step - loss: 1.9105 - val_loss: 17.3968\n",
      "Epoch 1125/1500\n",
      "36/36 [==============================] - 0s 353us/step - loss: 2.8709 - val_loss: 13.4862\n",
      "Epoch 1126/1500\n",
      "36/36 [==============================] - 0s 330us/step - loss: 1.4498 - val_loss: 10.0999\n",
      "Epoch 1127/1500\n",
      "36/36 [==============================] - 0s 335us/step - loss: 2.5925 - val_loss: 13.0263\n",
      "Epoch 1128/1500\n",
      "36/36 [==============================] - 0s 360us/step - loss: 1.5575 - val_loss: 12.4506\n",
      "Epoch 1129/1500\n",
      "36/36 [==============================] - 0s 347us/step - loss: 1.6103 - val_loss: 8.7724\n",
      "Epoch 1130/1500\n",
      "36/36 [==============================] - 0s 384us/step - loss: 2.4383 - val_loss: 12.7392\n",
      "Epoch 1131/1500\n",
      "36/36 [==============================] - 0s 343us/step - loss: 1.5641 - val_loss: 13.3477\n",
      "Epoch 1132/1500\n",
      "36/36 [==============================] - 0s 369us/step - loss: 1.3570 - val_loss: 11.3050\n",
      "Epoch 1133/1500\n",
      "36/36 [==============================] - 0s 565us/step - loss: 1.3783 - val_loss: 14.7466\n",
      "Epoch 1134/1500\n",
      "36/36 [==============================] - 0s 588us/step - loss: 1.9703 - val_loss: 14.9177\n",
      "Epoch 1135/1500\n",
      "36/36 [==============================] - ETA: 0s - loss: 1.549 - 0s 642us/step - loss: 1.5171 - val_loss: 13.5101\n",
      "Epoch 1136/1500\n",
      "36/36 [==============================] - 0s 517us/step - loss: 1.3110 - val_loss: 16.1532\n",
      "Epoch 1137/1500\n",
      "36/36 [==============================] - 0s 444us/step - loss: 1.8600 - val_loss: 12.8500\n",
      "Epoch 1138/1500\n",
      "36/36 [==============================] - 0s 529us/step - loss: 1.4785 - val_loss: 12.8210\n",
      "Epoch 1139/1500\n",
      "36/36 [==============================] - 0s 505us/step - loss: 1.1618 - val_loss: 12.1469\n",
      "Epoch 1140/1500\n",
      "36/36 [==============================] - 0s 624us/step - loss: 0.9414 - val_loss: 12.9026\n",
      "Epoch 1141/1500\n",
      "36/36 [==============================] - 0s 338us/step - loss: 1.0774 - val_loss: 11.3673\n",
      "Epoch 1142/1500\n",
      "36/36 [==============================] - 0s 515us/step - loss: 1.4965 - val_loss: 14.9053\n",
      "Epoch 1143/1500\n",
      "36/36 [==============================] - 0s 483us/step - loss: 1.4731 - val_loss: 15.1669\n",
      "Epoch 1144/1500\n",
      "36/36 [==============================] - ETA: 0s - loss: 1.271 - 0s 607us/step - loss: 1.3502 - val_loss: 13.5596\n",
      "Epoch 1145/1500\n",
      "36/36 [==============================] - 0s 386us/step - loss: 1.9807 - val_loss: 14.4012\n",
      "Epoch 1146/1500\n",
      "36/36 [==============================] - 0s 595us/step - loss: 1.3627 - val_loss: 14.4952\n",
      "Epoch 1147/1500\n",
      "36/36 [==============================] - 0s 416us/step - loss: 1.4945 - val_loss: 11.5388\n",
      "Epoch 1148/1500\n",
      "36/36 [==============================] - 0s 543us/step - loss: 1.7578 - val_loss: 12.6867\n",
      "Epoch 1149/1500\n",
      "36/36 [==============================] - 0s 564us/step - loss: 1.9480 - val_loss: 11.9549\n",
      "Epoch 1150/1500\n",
      "36/36 [==============================] - 0s 506us/step - loss: 2.1849 - val_loss: 12.0668\n",
      "Epoch 1151/1500\n",
      "36/36 [==============================] - 0s 368us/step - loss: 2.1891 - val_loss: 12.7706\n",
      "Epoch 1152/1500\n",
      "36/36 [==============================] - ETA: 0s - loss: 2.032 - 0s 424us/step - loss: 1.9116 - val_loss: 9.7826\n",
      "Epoch 1153/1500\n",
      "36/36 [==============================] - 0s 517us/step - loss: 1.8632 - val_loss: 12.5344\n",
      "Epoch 1154/1500\n",
      "36/36 [==============================] - 0s 516us/step - loss: 1.5770 - val_loss: 13.1842\n",
      "Epoch 1155/1500\n",
      "36/36 [==============================] - 0s 542us/step - loss: 1.5062 - val_loss: 11.4792\n",
      "Epoch 1156/1500\n",
      "36/36 [==============================] - 0s 548us/step - loss: 2.2080 - val_loss: 12.8910\n",
      "Epoch 1157/1500\n",
      "36/36 [==============================] - 0s 613us/step - loss: 1.1553 - val_loss: 10.5008\n",
      "Epoch 1158/1500\n",
      "36/36 [==============================] - 0s 436us/step - loss: 1.3200 - val_loss: 8.6961\n",
      "Epoch 1159/1500\n",
      "36/36 [==============================] - 0s 600us/step - loss: 1.7276 - val_loss: 8.1516\n",
      "Epoch 1160/1500\n",
      "36/36 [==============================] - 0s 416us/step - loss: 1.6962 - val_loss: 12.2586\n",
      "Epoch 1161/1500\n",
      "36/36 [==============================] - 0s 575us/step - loss: 2.2229 - val_loss: 14.7344\n",
      "Epoch 1162/1500\n",
      "36/36 [==============================] - 0s 425us/step - loss: 2.5142 - val_loss: 12.6934\n",
      "Epoch 1163/1500\n",
      "36/36 [==============================] - 0s 730us/step - loss: 1.8757 - val_loss: 11.5563\n",
      "Epoch 1164/1500\n",
      "36/36 [==============================] - 0s 489us/step - loss: 1.4539 - val_loss: 16.2254\n",
      "Epoch 1165/1500\n",
      "36/36 [==============================] - 0s 573us/step - loss: 2.7042 - val_loss: 11.9964\n",
      "Epoch 1166/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 342us/step - loss: 1.3576 - val_loss: 10.2639\n",
      "Epoch 1167/1500\n",
      "36/36 [==============================] - 0s 330us/step - loss: 1.9920 - val_loss: 16.3381\n",
      "Epoch 1168/1500\n",
      "36/36 [==============================] - 0s 389us/step - loss: 2.3880 - val_loss: 15.1647\n",
      "Epoch 1169/1500\n",
      "36/36 [==============================] - 0s 389us/step - loss: 2.0243 - val_loss: 15.3949\n",
      "Epoch 1170/1500\n",
      "36/36 [==============================] - 0s 354us/step - loss: 2.1385 - val_loss: 21.2443\n",
      "Epoch 1171/1500\n",
      "36/36 [==============================] - 0s 407us/step - loss: 2.7341 - val_loss: 18.6379\n",
      "Epoch 1172/1500\n",
      "36/36 [==============================] - 0s 365us/step - loss: 1.6075 - val_loss: 17.1407\n",
      "Epoch 1173/1500\n",
      "36/36 [==============================] - 0s 452us/step - loss: 1.6263 - val_loss: 15.7244\n",
      "Epoch 1174/1500\n",
      "36/36 [==============================] - ETA: 0s - loss: 1.193 - 0s 362us/step - loss: 1.3189 - val_loss: 14.9856\n",
      "Epoch 1175/1500\n",
      "36/36 [==============================] - 0s 355us/step - loss: 1.7831 - val_loss: 17.5320\n",
      "Epoch 1176/1500\n",
      "36/36 [==============================] - 0s 385us/step - loss: 2.1802 - val_loss: 13.0908\n",
      "Epoch 1177/1500\n",
      "36/36 [==============================] - 0s 499us/step - loss: 2.7451 - val_loss: 15.4681\n",
      "Epoch 1178/1500\n",
      "36/36 [==============================] - 0s 520us/step - loss: 1.8883 - val_loss: 20.8086\n",
      "Epoch 1179/1500\n",
      "36/36 [==============================] - 0s 455us/step - loss: 2.4956 - val_loss: 16.7920\n",
      "Epoch 1180/1500\n",
      "36/36 [==============================] - 0s 519us/step - loss: 2.4413 - val_loss: 15.1854\n",
      "Epoch 1181/1500\n",
      "36/36 [==============================] - 0s 545us/step - loss: 2.7060 - val_loss: 16.5142\n",
      "Epoch 1182/1500\n",
      "36/36 [==============================] - 0s 349us/step - loss: 1.4939 - val_loss: 13.3754\n",
      "Epoch 1183/1500\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.1977 - val_loss: 14.9125\n",
      "Epoch 1184/1500\n",
      "36/36 [==============================] - 0s 469us/step - loss: 1.7069 - val_loss: 12.2450\n",
      "Epoch 1185/1500\n",
      "36/36 [==============================] - 0s 525us/step - loss: 1.4333 - val_loss: 12.0699\n",
      "Epoch 1186/1500\n",
      "36/36 [==============================] - 0s 697us/step - loss: 1.7922 - val_loss: 16.0116\n",
      "Epoch 1187/1500\n",
      "36/36 [==============================] - 0s 531us/step - loss: 1.9931 - val_loss: 14.3364\n",
      "Epoch 1188/1500\n",
      "36/36 [==============================] - 0s 864us/step - loss: 2.2619 - val_loss: 15.7978\n",
      "Epoch 1189/1500\n",
      "36/36 [==============================] - 0s 558us/step - loss: 1.8792 - val_loss: 16.7581\n",
      "Epoch 1190/1500\n",
      "36/36 [==============================] - 0s 551us/step - loss: 2.1103 - val_loss: 12.3510\n",
      "Epoch 1191/1500\n",
      "36/36 [==============================] - 0s 368us/step - loss: 1.6027 - val_loss: 12.0907\n",
      "Epoch 1192/1500\n",
      "36/36 [==============================] - 0s 572us/step - loss: 2.3708 - val_loss: 10.1553\n",
      "Epoch 1193/1500\n",
      "36/36 [==============================] - 0s 660us/step - loss: 1.8684 - val_loss: 9.4884\n",
      "Epoch 1194/1500\n",
      "36/36 [==============================] - 0s 450us/step - loss: 2.0381 - val_loss: 11.9229\n",
      "Epoch 1195/1500\n",
      "36/36 [==============================] - 0s 734us/step - loss: 2.5186 - val_loss: 13.4237\n",
      "Epoch 1196/1500\n",
      "36/36 [==============================] - 0s 614us/step - loss: 2.1961 - val_loss: 12.8213\n",
      "Epoch 1197/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.4726 - val_loss: 12.6316\n",
      "Epoch 1198/1500\n",
      "36/36 [==============================] - 0s 808us/step - loss: 1.3596 - val_loss: 13.5392\n",
      "Epoch 1199/1500\n",
      "36/36 [==============================] - 0s 752us/step - loss: 2.0799 - val_loss: 14.0095\n",
      "Epoch 1200/1500\n",
      "36/36 [==============================] - 0s 839us/step - loss: 2.0748 - val_loss: 12.9275\n",
      "Epoch 1201/1500\n",
      "36/36 [==============================] - 0s 348us/step - loss: 1.4711 - val_loss: 13.9076\n",
      "Epoch 1202/1500\n",
      "36/36 [==============================] - 0s 795us/step - loss: 1.5390 - val_loss: 20.5102\n",
      "Epoch 1203/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 3.3472 - val_loss: 18.9525\n",
      "Epoch 1204/1500\n",
      "36/36 [==============================] - 0s 734us/step - loss: 2.4345 - val_loss: 9.4153\n",
      "Epoch 1205/1500\n",
      "36/36 [==============================] - 0s 439us/step - loss: 5.7376 - val_loss: 7.5018\n",
      "Epoch 1206/1500\n",
      "36/36 [==============================] - 0s 655us/step - loss: 6.9454 - val_loss: 14.5526\n",
      "Epoch 1207/1500\n",
      "36/36 [==============================] - 0s 407us/step - loss: 2.7273 - val_loss: 22.3899\n",
      "Epoch 1208/1500\n",
      "36/36 [==============================] - 0s 385us/step - loss: 6.4017 - val_loss: 18.9887\n",
      "Epoch 1209/1500\n",
      "36/36 [==============================] - 0s 458us/step - loss: 3.8823 - val_loss: 10.1320\n",
      "Epoch 1210/1500\n",
      "36/36 [==============================] - 0s 435us/step - loss: 3.7241 - val_loss: 11.5684\n",
      "Epoch 1211/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 2.2327 - val_loss: 19.0726\n",
      "Epoch 1212/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 3.6005 - val_loss: 16.5606\n",
      "Epoch 1213/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 2.0877 - val_loss: 9.8698\n",
      "Epoch 1214/1500\n",
      "36/36 [==============================] - 0s 750us/step - loss: 2.9966 - val_loss: 13.2381\n",
      "Epoch 1215/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.5879 - val_loss: 14.9824\n",
      "Epoch 1216/1500\n",
      "36/36 [==============================] - 0s 641us/step - loss: 2.3265 - val_loss: 9.9628\n",
      "Epoch 1217/1500\n",
      "36/36 [==============================] - 0s 387us/step - loss: 3.2011 - val_loss: 10.6840\n",
      "Epoch 1218/1500\n",
      "36/36 [==============================] - 0s 369us/step - loss: 2.4496 - val_loss: 16.7919\n",
      "Epoch 1219/1500\n",
      "36/36 [==============================] - 0s 359us/step - loss: 2.8424 - val_loss: 15.7408\n",
      "Epoch 1220/1500\n",
      "36/36 [==============================] - 0s 537us/step - loss: 1.3066 - val_loss: 13.4106\n",
      "Epoch 1221/1500\n",
      "36/36 [==============================] - 0s 382us/step - loss: 1.8644 - val_loss: 18.2308\n",
      "Epoch 1222/1500\n",
      "36/36 [==============================] - 0s 575us/step - loss: 4.0523 - val_loss: 17.7646\n",
      "Epoch 1223/1500\n",
      "36/36 [==============================] - 0s 364us/step - loss: 3.8088 - val_loss: 10.9451\n",
      "Epoch 1224/1500\n",
      "36/36 [==============================] - 0s 353us/step - loss: 3.6677 - val_loss: 10.4973\n",
      "Epoch 1225/1500\n",
      "36/36 [==============================] - 0s 320us/step - loss: 3.9780 - val_loss: 17.1115\n",
      "Epoch 1226/1500\n",
      "36/36 [==============================] - 0s 337us/step - loss: 3.6534 - val_loss: 19.8222\n",
      "Epoch 1227/1500\n",
      "36/36 [==============================] - 0s 326us/step - loss: 4.7404 - val_loss: 13.4738\n",
      "Epoch 1228/1500\n",
      "36/36 [==============================] - 0s 488us/step - loss: 1.7784 - val_loss: 10.7094\n",
      "Epoch 1229/1500\n",
      "36/36 [==============================] - 0s 530us/step - loss: 3.0876 - val_loss: 13.7900\n",
      "Epoch 1230/1500\n",
      "36/36 [==============================] - 0s 384us/step - loss: 1.6942 - val_loss: 13.2709\n",
      "Epoch 1231/1500\n",
      "36/36 [==============================] - 0s 304us/step - loss: 1.9398 - val_loss: 10.5658\n",
      "Epoch 1232/1500\n",
      "36/36 [==============================] - 0s 451us/step - loss: 1.7124 - val_loss: 12.0502\n",
      "Epoch 1233/1500\n",
      "36/36 [==============================] - 0s 575us/step - loss: 1.2006 - val_loss: 12.6506\n",
      "Epoch 1234/1500\n",
      "36/36 [==============================] - 0s 404us/step - loss: 1.6621 - val_loss: 17.7227\n",
      "Epoch 1235/1500\n",
      "36/36 [==============================] - 0s 364us/step - loss: 2.5234 - val_loss: 16.2146\n",
      "Epoch 1236/1500\n",
      "36/36 [==============================] - 0s 406us/step - loss: 1.8484 - val_loss: 10.7919\n",
      "Epoch 1237/1500\n",
      "36/36 [==============================] - 0s 364us/step - loss: 1.5881 - val_loss: 11.2999\n",
      "Epoch 1238/1500\n",
      "36/36 [==============================] - 0s 347us/step - loss: 1.0167 - val_loss: 13.2230\n",
      "Epoch 1239/1500\n",
      "36/36 [==============================] - 0s 378us/step - loss: 1.3143 - val_loss: 11.5091\n",
      "Epoch 1240/1500\n",
      "36/36 [==============================] - 0s 369us/step - loss: 1.1028 - val_loss: 12.3040\n",
      "Epoch 1241/1500\n",
      "36/36 [==============================] - 0s 353us/step - loss: 0.9164 - val_loss: 15.0619\n",
      "Epoch 1242/1500\n",
      "36/36 [==============================] - 0s 380us/step - loss: 1.2905 - val_loss: 13.1126\n",
      "Epoch 1243/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 438us/step - loss: 1.4116 - val_loss: 16.5274\n",
      "Epoch 1244/1500\n",
      "36/36 [==============================] - 0s 428us/step - loss: 1.8333 - val_loss: 14.4800\n",
      "Epoch 1245/1500\n",
      "36/36 [==============================] - 0s 554us/step - loss: 1.3760 - val_loss: 13.6854\n",
      "Epoch 1246/1500\n",
      "36/36 [==============================] - 0s 464us/step - loss: 1.3708 - val_loss: 15.2284\n",
      "Epoch 1247/1500\n",
      "36/36 [==============================] - 0s 470us/step - loss: 0.9128 - val_loss: 13.1627\n",
      "Epoch 1248/1500\n",
      "36/36 [==============================] - 0s 493us/step - loss: 1.3455 - val_loss: 16.7378\n",
      "Epoch 1249/1500\n",
      "36/36 [==============================] - 0s 498us/step - loss: 1.7979 - val_loss: 13.6541\n",
      "Epoch 1250/1500\n",
      "36/36 [==============================] - 0s 638us/step - loss: 1.4670 - val_loss: 12.2331\n",
      "Epoch 1251/1500\n",
      "36/36 [==============================] - 0s 444us/step - loss: 1.7705 - val_loss: 14.6315\n",
      "Epoch 1252/1500\n",
      "36/36 [==============================] - 0s 266us/step - loss: 1.6288 - val_loss: 9.4140\n",
      "Epoch 1253/1500\n",
      "36/36 [==============================] - 0s 380us/step - loss: 2.7461 - val_loss: 12.2810\n",
      "Epoch 1254/1500\n",
      "36/36 [==============================] - 0s 307us/step - loss: 1.0340 - val_loss: 16.7669\n",
      "Epoch 1255/1500\n",
      "36/36 [==============================] - 0s 373us/step - loss: 2.6378 - val_loss: 12.7364\n",
      "Epoch 1256/1500\n",
      "36/36 [==============================] - 0s 540us/step - loss: 2.0732 - val_loss: 13.1147\n",
      "Epoch 1257/1500\n",
      "36/36 [==============================] - 0s 774us/step - loss: 2.1986 - val_loss: 14.9579\n",
      "Epoch 1258/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 2.5903 - val_loss: 10.9146\n",
      "Epoch 1259/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.6456 - val_loss: 11.9426\n",
      "Epoch 1260/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.3938 - val_loss: 10.3668\n",
      "Epoch 1261/1500\n",
      "36/36 [==============================] - 0s 473us/step - loss: 2.0476 - val_loss: 10.9062\n",
      "Epoch 1262/1500\n",
      "36/36 [==============================] - 0s 884us/step - loss: 1.8921 - val_loss: 11.9241\n",
      "Epoch 1263/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.3326 - val_loss: 10.2954\n",
      "Epoch 1264/1500\n",
      "36/36 [==============================] - 0s 492us/step - loss: 1.0519 - val_loss: 9.8768\n",
      "Epoch 1265/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.2329 - val_loss: 9.6985\n",
      "Epoch 1266/1500\n",
      "36/36 [==============================] - 0s 537us/step - loss: 1.2364 - val_loss: 12.2912\n",
      "Epoch 1267/1500\n",
      "36/36 [==============================] - 0s 550us/step - loss: 1.7746 - val_loss: 8.8866\n",
      "Epoch 1268/1500\n",
      "36/36 [==============================] - 0s 497us/step - loss: 2.0971 - val_loss: 10.4100\n",
      "Epoch 1269/1500\n",
      "36/36 [==============================] - 0s 487us/step - loss: 1.4561 - val_loss: 12.2480\n",
      "Epoch 1270/1500\n",
      "36/36 [==============================] - 0s 581us/step - loss: 1.8736 - val_loss: 9.0330\n",
      "Epoch 1271/1500\n",
      "36/36 [==============================] - 0s 584us/step - loss: 2.1894 - val_loss: 12.6547\n",
      "Epoch 1272/1500\n",
      "36/36 [==============================] - 0s 719us/step - loss: 2.2930 - val_loss: 11.3491\n",
      "Epoch 1273/1500\n",
      "36/36 [==============================] - 0s 874us/step - loss: 1.8487 - val_loss: 10.6513\n",
      "Epoch 1274/1500\n",
      "36/36 [==============================] - 0s 481us/step - loss: 1.9469 - val_loss: 13.0359\n",
      "Epoch 1275/1500\n",
      "36/36 [==============================] - 0s 889us/step - loss: 1.5704 - val_loss: 13.1357\n",
      "Epoch 1276/1500\n",
      "36/36 [==============================] - 0s 372us/step - loss: 1.7342 - val_loss: 11.8648\n",
      "Epoch 1277/1500\n",
      "36/36 [==============================] - 0s 407us/step - loss: 1.7317 - val_loss: 11.3951\n",
      "Epoch 1278/1500\n",
      "36/36 [==============================] - 0s 606us/step - loss: 1.2310 - val_loss: 12.4130\n",
      "Epoch 1279/1500\n",
      "36/36 [==============================] - 0s 699us/step - loss: 1.2204 - val_loss: 13.7837\n",
      "Epoch 1280/1500\n",
      "36/36 [==============================] - 0s 831us/step - loss: 1.7153 - val_loss: 17.1376\n",
      "Epoch 1281/1500\n",
      "36/36 [==============================] - 0s 649us/step - loss: 2.8555 - val_loss: 15.2823\n",
      "Epoch 1282/1500\n",
      "36/36 [==============================] - 0s 624us/step - loss: 2.2065 - val_loss: 11.5532\n",
      "Epoch 1283/1500\n",
      "36/36 [==============================] - 0s 441us/step - loss: 1.6507 - val_loss: 13.2733\n",
      "Epoch 1284/1500\n",
      "36/36 [==============================] - 0s 520us/step - loss: 1.3895 - val_loss: 10.4868\n",
      "Epoch 1285/1500\n",
      "36/36 [==============================] - 0s 723us/step - loss: 1.8539 - val_loss: 13.3370\n",
      "Epoch 1286/1500\n",
      "36/36 [==============================] - 0s 448us/step - loss: 1.3845 - val_loss: 13.7670\n",
      "Epoch 1287/1500\n",
      "36/36 [==============================] - 0s 696us/step - loss: 1.1583 - val_loss: 12.6050\n",
      "Epoch 1288/1500\n",
      "36/36 [==============================] - 0s 579us/step - loss: 1.2696 - val_loss: 13.9196\n",
      "Epoch 1289/1500\n",
      "36/36 [==============================] - 0s 582us/step - loss: 1.3046 - val_loss: 10.6021\n",
      "Epoch 1290/1500\n",
      "36/36 [==============================] - 0s 449us/step - loss: 1.1932 - val_loss: 12.6422\n",
      "Epoch 1291/1500\n",
      "36/36 [==============================] - 0s 621us/step - loss: 1.1576 - val_loss: 11.2433\n",
      "Epoch 1292/1500\n",
      "36/36 [==============================] - 0s 609us/step - loss: 1.6826 - val_loss: 14.1996\n",
      "Epoch 1293/1500\n",
      "36/36 [==============================] - 0s 506us/step - loss: 1.5143 - val_loss: 17.4139\n",
      "Epoch 1294/1500\n",
      "36/36 [==============================] - ETA: 0s - loss: 3.510 - 0s 630us/step - loss: 3.4441 - val_loss: 13.1640\n",
      "Epoch 1295/1500\n",
      "36/36 [==============================] - 0s 639us/step - loss: 1.7932 - val_loss: 7.0952\n",
      "Epoch 1296/1500\n",
      "36/36 [==============================] - 0s 707us/step - loss: 2.8083 - val_loss: 7.8486\n",
      "Epoch 1297/1500\n",
      "36/36 [==============================] - 0s 617us/step - loss: 1.8054 - val_loss: 7.6900\n",
      "Epoch 1298/1500\n",
      "36/36 [==============================] - 0s 665us/step - loss: 1.8695 - val_loss: 7.7664\n",
      "Epoch 1299/1500\n",
      "36/36 [==============================] - 0s 705us/step - loss: 1.6055 - val_loss: 9.3741\n",
      "Epoch 1300/1500\n",
      "36/36 [==============================] - 0s 711us/step - loss: 1.1940 - val_loss: 10.8067\n",
      "Epoch 1301/1500\n",
      "36/36 [==============================] - 0s 540us/step - loss: 1.5024 - val_loss: 14.1331\n",
      "Epoch 1302/1500\n",
      "36/36 [==============================] - 0s 711us/step - loss: 1.7927 - val_loss: 12.7154\n",
      "Epoch 1303/1500\n",
      "36/36 [==============================] - 0s 609us/step - loss: 1.7219 - val_loss: 13.6034\n",
      "Epoch 1304/1500\n",
      "36/36 [==============================] - 0s 598us/step - loss: 1.9192 - val_loss: 14.5567\n",
      "Epoch 1305/1500\n",
      "36/36 [==============================] - 0s 461us/step - loss: 2.0623 - val_loss: 11.9577\n",
      "Epoch 1306/1500\n",
      "36/36 [==============================] - 0s 606us/step - loss: 1.4557 - val_loss: 13.4599\n",
      "Epoch 1307/1500\n",
      "36/36 [==============================] - 0s 587us/step - loss: 1.1842 - val_loss: 12.2879\n",
      "Epoch 1308/1500\n",
      "36/36 [==============================] - 0s 664us/step - loss: 0.8869 - val_loss: 11.4488\n",
      "Epoch 1309/1500\n",
      "36/36 [==============================] - 0s 827us/step - loss: 0.9327 - val_loss: 11.6348\n",
      "Epoch 1310/1500\n",
      "36/36 [==============================] - 0s 685us/step - loss: 1.1012 - val_loss: 10.6966\n",
      "Epoch 1311/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 2.3117 - val_loss: 12.1733\n",
      "Epoch 1312/1500\n",
      "36/36 [==============================] - 0s 452us/step - loss: 1.1314 - val_loss: 11.1950\n",
      "Epoch 1313/1500\n",
      "36/36 [==============================] - 0s 875us/step - loss: 1.0494 - val_loss: 9.8241\n",
      "Epoch 1314/1500\n",
      "36/36 [==============================] - 0s 579us/step - loss: 1.2307 - val_loss: 11.6077\n",
      "Epoch 1315/1500\n",
      "36/36 [==============================] - 0s 678us/step - loss: 1.6658 - val_loss: 8.8278\n",
      "Epoch 1316/1500\n",
      "36/36 [==============================] - 0s 468us/step - loss: 1.5039 - val_loss: 12.3542\n",
      "Epoch 1317/1500\n",
      "36/36 [==============================] - 0s 886us/step - loss: 1.4085 - val_loss: 9.2075\n",
      "Epoch 1318/1500\n",
      "36/36 [==============================] - 0s 850us/step - loss: 1.7305 - val_loss: 11.5085\n",
      "Epoch 1319/1500\n",
      "36/36 [==============================] - 0s 426us/step - loss: 0.9758 - val_loss: 12.5439\n",
      "Epoch 1320/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 513us/step - loss: 1.4203 - val_loss: 8.4793\n",
      "Epoch 1321/1500\n",
      "36/36 [==============================] - 0s 526us/step - loss: 1.9870 - val_loss: 12.9596\n",
      "Epoch 1322/1500\n",
      "36/36 [==============================] - 0s 508us/step - loss: 1.6621 - val_loss: 11.9611\n",
      "Epoch 1323/1500\n",
      "36/36 [==============================] - 0s 432us/step - loss: 1.5098 - val_loss: 11.5074\n",
      "Epoch 1324/1500\n",
      "36/36 [==============================] - 0s 499us/step - loss: 1.7388 - val_loss: 12.8331\n",
      "Epoch 1325/1500\n",
      "36/36 [==============================] - 0s 935us/step - loss: 1.9275 - val_loss: 7.7516\n",
      "Epoch 1326/1500\n",
      "36/36 [==============================] - 0s 785us/step - loss: 2.5365 - val_loss: 9.3190\n",
      "Epoch 1327/1500\n",
      "36/36 [==============================] - 0s 585us/step - loss: 1.9161 - val_loss: 12.6202\n",
      "Epoch 1328/1500\n",
      "36/36 [==============================] - 0s 478us/step - loss: 3.0712 - val_loss: 9.6944\n",
      "Epoch 1329/1500\n",
      "36/36 [==============================] - 0s 875us/step - loss: 1.7026 - val_loss: 11.0981\n",
      "Epoch 1330/1500\n",
      "36/36 [==============================] - 0s 799us/step - loss: 1.7633 - val_loss: 13.4282\n",
      "Epoch 1331/1500\n",
      "36/36 [==============================] - 0s 794us/step - loss: 1.5884 - val_loss: 14.9039\n",
      "Epoch 1332/1500\n",
      "36/36 [==============================] - 0s 715us/step - loss: 1.8920 - val_loss: 12.4522\n",
      "Epoch 1333/1500\n",
      "36/36 [==============================] - 0s 731us/step - loss: 1.8800 - val_loss: 12.7427\n",
      "Epoch 1334/1500\n",
      "36/36 [==============================] - 0s 967us/step - loss: 1.2505 - val_loss: 11.2298\n",
      "Epoch 1335/1500\n",
      "36/36 [==============================] - 0s 593us/step - loss: 1.1214 - val_loss: 8.9919\n",
      "Epoch 1336/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.5020 - val_loss: 8.3403\n",
      "Epoch 1337/1500\n",
      "36/36 [==============================] - 0s 708us/step - loss: 1.7525 - val_loss: 8.7756\n",
      "Epoch 1338/1500\n",
      "36/36 [==============================] - 0s 456us/step - loss: 1.6047 - val_loss: 9.9756\n",
      "Epoch 1339/1500\n",
      "36/36 [==============================] - 0s 724us/step - loss: 1.4500 - val_loss: 11.4820\n",
      "Epoch 1340/1500\n",
      "36/36 [==============================] - 0s 321us/step - loss: 1.3147 - val_loss: 13.6970\n",
      "Epoch 1341/1500\n",
      "36/36 [==============================] - 0s 515us/step - loss: 2.1532 - val_loss: 10.0538\n",
      "Epoch 1342/1500\n",
      "36/36 [==============================] - 0s 345us/step - loss: 1.9443 - val_loss: 10.7432\n",
      "Epoch 1343/1500\n",
      "36/36 [==============================] - 0s 712us/step - loss: 3.1202 - val_loss: 11.7871\n",
      "Epoch 1344/1500\n",
      "36/36 [==============================] - 0s 589us/step - loss: 2.3997 - val_loss: 15.0495\n",
      "Epoch 1345/1500\n",
      "36/36 [==============================] - 0s 308us/step - loss: 2.7914 - val_loss: 9.5299\n",
      "Epoch 1346/1500\n",
      "36/36 [==============================] - 0s 342us/step - loss: 3.1747 - val_loss: 8.3980\n",
      "Epoch 1347/1500\n",
      "36/36 [==============================] - 0s 489us/step - loss: 3.5142 - val_loss: 16.0858\n",
      "Epoch 1348/1500\n",
      "36/36 [==============================] - 0s 611us/step - loss: 3.9987 - val_loss: 17.7537\n",
      "Epoch 1349/1500\n",
      "36/36 [==============================] - 0s 416us/step - loss: 4.7251 - val_loss: 8.8461\n",
      "Epoch 1350/1500\n",
      "36/36 [==============================] - 0s 578us/step - loss: 2.2074 - val_loss: 8.4892\n",
      "Epoch 1351/1500\n",
      "36/36 [==============================] - 0s 547us/step - loss: 2.5717 - val_loss: 14.1878\n",
      "Epoch 1352/1500\n",
      "36/36 [==============================] - 0s 392us/step - loss: 2.8098 - val_loss: 11.4606\n",
      "Epoch 1353/1500\n",
      "36/36 [==============================] - 0s 358us/step - loss: 1.5772 - val_loss: 8.6857\n",
      "Epoch 1354/1500\n",
      "36/36 [==============================] - 0s 308us/step - loss: 2.2078 - val_loss: 8.9166\n",
      "Epoch 1355/1500\n",
      "36/36 [==============================] - 0s 483us/step - loss: 1.9362 - val_loss: 10.0613\n",
      "Epoch 1356/1500\n",
      "36/36 [==============================] - 0s 389us/step - loss: 1.5920 - val_loss: 11.7566\n",
      "Epoch 1357/1500\n",
      "36/36 [==============================] - 0s 530us/step - loss: 1.3220 - val_loss: 12.0926\n",
      "Epoch 1358/1500\n",
      "36/36 [==============================] - 0s 344us/step - loss: 1.5008 - val_loss: 11.5552\n",
      "Epoch 1359/1500\n",
      "36/36 [==============================] - 0s 390us/step - loss: 2.3088 - val_loss: 13.7911\n",
      "Epoch 1360/1500\n",
      "36/36 [==============================] - 0s 348us/step - loss: 1.7825 - val_loss: 13.9521\n",
      "Epoch 1361/1500\n",
      "36/36 [==============================] - 0s 519us/step - loss: 1.8665 - val_loss: 12.6105\n",
      "Epoch 1362/1500\n",
      "36/36 [==============================] - 0s 508us/step - loss: 2.0966 - val_loss: 14.6110\n",
      "Epoch 1363/1500\n",
      "36/36 [==============================] - 0s 497us/step - loss: 1.5195 - val_loss: 13.8054\n",
      "Epoch 1364/1500\n",
      "36/36 [==============================] - 0s 551us/step - loss: 1.4426 - val_loss: 10.5123\n",
      "Epoch 1365/1500\n",
      "36/36 [==============================] - 0s 538us/step - loss: 1.2328 - val_loss: 11.1844\n",
      "Epoch 1366/1500\n",
      "36/36 [==============================] - 0s 353us/step - loss: 1.2818 - val_loss: 9.6681\n",
      "Epoch 1367/1500\n",
      "36/36 [==============================] - 0s 503us/step - loss: 1.3430 - val_loss: 9.3166\n",
      "Epoch 1368/1500\n",
      "36/36 [==============================] - 0s 353us/step - loss: 1.4567 - val_loss: 11.9056\n",
      "Epoch 1369/1500\n",
      "36/36 [==============================] - 0s 354us/step - loss: 1.0913 - val_loss: 9.5444\n",
      "Epoch 1370/1500\n",
      "36/36 [==============================] - 0s 404us/step - loss: 1.1711 - val_loss: 10.3540\n",
      "Epoch 1371/1500\n",
      "36/36 [==============================] - 0s 479us/step - loss: 1.0854 - val_loss: 9.0604\n",
      "Epoch 1372/1500\n",
      "36/36 [==============================] - 0s 535us/step - loss: 1.8177 - val_loss: 12.9276\n",
      "Epoch 1373/1500\n",
      "36/36 [==============================] - 0s 391us/step - loss: 0.9492 - val_loss: 14.3696\n",
      "Epoch 1374/1500\n",
      "36/36 [==============================] - 0s 807us/step - loss: 1.1337 - val_loss: 9.3662\n",
      "Epoch 1375/1500\n",
      "36/36 [==============================] - 0s 421us/step - loss: 3.6902 - val_loss: 11.2011\n",
      "Epoch 1376/1500\n",
      "36/36 [==============================] - 0s 378us/step - loss: 2.3032 - val_loss: 17.2015\n",
      "Epoch 1377/1500\n",
      "36/36 [==============================] - 0s 510us/step - loss: 2.8566 - val_loss: 14.9779\n",
      "Epoch 1378/1500\n",
      "36/36 [==============================] - 0s 453us/step - loss: 1.4717 - val_loss: 14.1840\n",
      "Epoch 1379/1500\n",
      "36/36 [==============================] - 0s 342us/step - loss: 1.3708 - val_loss: 15.0676\n",
      "Epoch 1380/1500\n",
      "36/36 [==============================] - 0s 378us/step - loss: 1.4639 - val_loss: 11.5094\n",
      "Epoch 1381/1500\n",
      "36/36 [==============================] - 0s 335us/step - loss: 2.0760 - val_loss: 11.6013\n",
      "Epoch 1382/1500\n",
      "36/36 [==============================] - 0s 388us/step - loss: 2.0990 - val_loss: 14.0465\n",
      "Epoch 1383/1500\n",
      "36/36 [==============================] - 0s 379us/step - loss: 2.4594 - val_loss: 11.8344\n",
      "Epoch 1384/1500\n",
      "36/36 [==============================] - 0s 360us/step - loss: 1.4057 - val_loss: 13.0432\n",
      "Epoch 1385/1500\n",
      "36/36 [==============================] - 0s 677us/step - loss: 1.4928 - val_loss: 15.6896\n",
      "Epoch 1386/1500\n",
      "36/36 [==============================] - 0s 420us/step - loss: 2.0433 - val_loss: 15.3843\n",
      "Epoch 1387/1500\n",
      "36/36 [==============================] - 0s 400us/step - loss: 1.8071 - val_loss: 10.7154\n",
      "Epoch 1388/1500\n",
      "36/36 [==============================] - 0s 328us/step - loss: 2.2516 - val_loss: 12.4324\n",
      "Epoch 1389/1500\n",
      "36/36 [==============================] - 0s 333us/step - loss: 1.1050 - val_loss: 15.7800\n",
      "Epoch 1390/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 2.3465 - val_loss: 12.4872\n",
      "Epoch 1391/1500\n",
      "36/36 [==============================] - 0s 543us/step - loss: 1.4027 - val_loss: 14.8684\n",
      "Epoch 1392/1500\n",
      "36/36 [==============================] - 0s 492us/step - loss: 1.3218 - val_loss: 13.0233\n",
      "Epoch 1393/1500\n",
      "36/36 [==============================] - 0s 443us/step - loss: 0.9819 - val_loss: 9.2430\n",
      "Epoch 1394/1500\n",
      "36/36 [==============================] - 0s 400us/step - loss: 1.8863 - val_loss: 12.1486\n",
      "Epoch 1395/1500\n",
      "36/36 [==============================] - 0s 566us/step - loss: 1.4202 - val_loss: 10.3947\n",
      "Epoch 1396/1500\n",
      "36/36 [==============================] - 0s 773us/step - loss: 1.2468 - val_loss: 12.0630\n",
      "Epoch 1397/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 637us/step - loss: 0.8645 - val_loss: 13.1571\n",
      "Epoch 1398/1500\n",
      "36/36 [==============================] - 0s 527us/step - loss: 1.2668 - val_loss: 11.5145\n",
      "Epoch 1399/1500\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.777 - 0s 489us/step - loss: 0.7464 - val_loss: 10.8186\n",
      "Epoch 1400/1500\n",
      "36/36 [==============================] - 0s 489us/step - loss: 0.8251 - val_loss: 10.8826\n",
      "Epoch 1401/1500\n",
      "36/36 [==============================] - 0s 300us/step - loss: 0.9570 - val_loss: 9.1779\n",
      "Epoch 1402/1500\n",
      "36/36 [==============================] - 0s 635us/step - loss: 1.1308 - val_loss: 9.0997\n",
      "Epoch 1403/1500\n",
      "36/36 [==============================] - 0s 551us/step - loss: 1.2165 - val_loss: 10.4128\n",
      "Epoch 1404/1500\n",
      "36/36 [==============================] - 0s 514us/step - loss: 1.1461 - val_loss: 10.3133\n",
      "Epoch 1405/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.0725 - val_loss: 11.5510\n",
      "Epoch 1406/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.0077 - val_loss: 15.4470\n",
      "Epoch 1407/1500\n",
      "36/36 [==============================] - 0s 471us/step - loss: 2.5195 - val_loss: 13.9766\n",
      "Epoch 1408/1500\n",
      "36/36 [==============================] - 0s 544us/step - loss: 1.6101 - val_loss: 10.4963\n",
      "Epoch 1409/1500\n",
      "36/36 [==============================] - 0s 654us/step - loss: 2.0435 - val_loss: 17.4211\n",
      "Epoch 1410/1500\n",
      "36/36 [==============================] - 0s 557us/step - loss: 3.8984 - val_loss: 17.9749\n",
      "Epoch 1411/1500\n",
      "36/36 [==============================] - 0s 572us/step - loss: 3.5076 - val_loss: 9.5536\n",
      "Epoch 1412/1500\n",
      "36/36 [==============================] - 0s 402us/step - loss: 4.5058 - val_loss: 9.6528\n",
      "Epoch 1413/1500\n",
      "36/36 [==============================] - 0s 608us/step - loss: 6.3401 - val_loss: 12.5952\n",
      "Epoch 1414/1500\n",
      "36/36 [==============================] - 0s 386us/step - loss: 1.4811 - val_loss: 23.0288\n",
      "Epoch 1415/1500\n",
      "36/36 [==============================] - 0s 439us/step - loss: 6.8626 - val_loss: 21.9202\n",
      "Epoch 1416/1500\n",
      "36/36 [==============================] - 0s 403us/step - loss: 4.6842 - val_loss: 12.5616\n",
      "Epoch 1417/1500\n",
      "36/36 [==============================] - ETA: 0s - loss: 4.778 - 0s 474us/step - loss: 5.3101 - val_loss: 11.7840\n",
      "Epoch 1418/1500\n",
      "36/36 [==============================] - 0s 470us/step - loss: 8.5699 - val_loss: 11.6081\n",
      "Epoch 1419/1500\n",
      "36/36 [==============================] - 0s 467us/step - loss: 4.7919 - val_loss: 19.8345\n",
      "Epoch 1420/1500\n",
      "36/36 [==============================] - 0s 390us/step - loss: 4.9248 - val_loss: 20.3824\n",
      "Epoch 1421/1500\n",
      "36/36 [==============================] - 0s 359us/step - loss: 6.1952 - val_loss: 11.2730\n",
      "Epoch 1422/1500\n",
      "36/36 [==============================] - 0s 373us/step - loss: 2.3620 - val_loss: 9.1118\n",
      "Epoch 1423/1500\n",
      "36/36 [==============================] - 0s 305us/step - loss: 4.5426 - val_loss: 10.0452\n",
      "Epoch 1424/1500\n",
      "36/36 [==============================] - 0s 460us/step - loss: 1.9250 - val_loss: 13.7454\n",
      "Epoch 1425/1500\n",
      "36/36 [==============================] - 0s 443us/step - loss: 2.8865 - val_loss: 11.2265\n",
      "Epoch 1426/1500\n",
      "36/36 [==============================] - 0s 470us/step - loss: 1.7839 - val_loss: 11.9780\n",
      "Epoch 1427/1500\n",
      "36/36 [==============================] - 0s 484us/step - loss: 1.2782 - val_loss: 15.6698\n",
      "Epoch 1428/1500\n",
      "36/36 [==============================] - 0s 449us/step - loss: 2.0466 - val_loss: 12.9782\n",
      "Epoch 1429/1500\n",
      "36/36 [==============================] - 0s 546us/step - loss: 1.4968 - val_loss: 11.4728\n",
      "Epoch 1430/1500\n",
      "36/36 [==============================] - 0s 483us/step - loss: 2.0840 - val_loss: 12.9453\n",
      "Epoch 1431/1500\n",
      "36/36 [==============================] - 0s 372us/step - loss: 1.6703 - val_loss: 10.9747\n",
      "Epoch 1432/1500\n",
      "36/36 [==============================] - 0s 412us/step - loss: 1.0067 - val_loss: 9.1499\n",
      "Epoch 1433/1500\n",
      "36/36 [==============================] - 0s 444us/step - loss: 1.6241 - val_loss: 12.8357\n",
      "Epoch 1434/1500\n",
      "36/36 [==============================] - 0s 367us/step - loss: 2.0246 - val_loss: 12.4302\n",
      "Epoch 1435/1500\n",
      "36/36 [==============================] - 0s 492us/step - loss: 2.1686 - val_loss: 9.6458\n",
      "Epoch 1436/1500\n",
      "36/36 [==============================] - 0s 373us/step - loss: 1.6365 - val_loss: 7.7847\n",
      "Epoch 1437/1500\n",
      "36/36 [==============================] - 0s 485us/step - loss: 1.4085 - val_loss: 9.1235\n",
      "Epoch 1438/1500\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4273 - val_loss: 9.6652\n",
      "Epoch 1439/1500\n",
      "36/36 [==============================] - 0s 374us/step - loss: 1.3049 - val_loss: 11.3049\n",
      "Epoch 1440/1500\n",
      "36/36 [==============================] - 0s 424us/step - loss: 1.0134 - val_loss: 12.5195\n",
      "Epoch 1441/1500\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.2006 - val_loss: 14.4006\n",
      "Epoch 1442/1500\n",
      "36/36 [==============================] - 0s 528us/step - loss: 1.5995 - val_loss: 9.5098\n",
      "Epoch 1443/1500\n",
      "36/36 [==============================] - 0s 511us/step - loss: 1.8588 - val_loss: 9.7111\n",
      "Epoch 1444/1500\n",
      "36/36 [==============================] - 0s 495us/step - loss: 1.6685 - val_loss: 10.5653\n",
      "Epoch 1445/1500\n",
      "36/36 [==============================] - 0s 546us/step - loss: 2.0943 - val_loss: 9.2788\n",
      "Epoch 1446/1500\n",
      "36/36 [==============================] - 0s 389us/step - loss: 1.8744 - val_loss: 13.7730\n",
      "Epoch 1447/1500\n",
      "36/36 [==============================] - 0s 569us/step - loss: 1.6942 - val_loss: 11.6834\n",
      "Epoch 1448/1500\n",
      "36/36 [==============================] - 0s 412us/step - loss: 1.3508 - val_loss: 13.9961\n",
      "Epoch 1449/1500\n",
      "36/36 [==============================] - 0s 380us/step - loss: 1.7754 - val_loss: 12.5160\n",
      "Epoch 1450/1500\n",
      "36/36 [==============================] - 0s 413us/step - loss: 1.4618 - val_loss: 10.5984\n",
      "Epoch 1451/1500\n",
      "36/36 [==============================] - 0s 503us/step - loss: 1.8275 - val_loss: 13.2381\n",
      "Epoch 1452/1500\n",
      "36/36 [==============================] - 0s 498us/step - loss: 1.4837 - val_loss: 10.9272\n",
      "Epoch 1453/1500\n",
      "36/36 [==============================] - 0s 520us/step - loss: 1.2716 - val_loss: 16.1945\n",
      "Epoch 1454/1500\n",
      "36/36 [==============================] - 0s 526us/step - loss: 2.5836 - val_loss: 15.8916\n",
      "Epoch 1455/1500\n",
      "36/36 [==============================] - 0s 382us/step - loss: 2.1120 - val_loss: 11.2946\n",
      "Epoch 1456/1500\n",
      "36/36 [==============================] - 0s 411us/step - loss: 1.6588 - val_loss: 15.0551\n",
      "Epoch 1457/1500\n",
      "36/36 [==============================] - ETA: 0s - loss: 2.309 - 0s 376us/step - loss: 2.3691 - val_loss: 12.5927\n",
      "Epoch 1458/1500\n",
      "36/36 [==============================] - 0s 501us/step - loss: 1.2646 - val_loss: 10.5088\n",
      "Epoch 1459/1500\n",
      "36/36 [==============================] - 0s 406us/step - loss: 2.3885 - val_loss: 16.9344\n",
      "Epoch 1460/1500\n",
      "36/36 [==============================] - 0s 336us/step - loss: 2.7814 - val_loss: 15.7815\n",
      "Epoch 1461/1500\n",
      "36/36 [==============================] - 0s 345us/step - loss: 1.5611 - val_loss: 12.4822\n",
      "Epoch 1462/1500\n",
      "36/36 [==============================] - 0s 384us/step - loss: 1.9928 - val_loss: 15.9531\n",
      "Epoch 1463/1500\n",
      "36/36 [==============================] - 0s 352us/step - loss: 2.2859 - val_loss: 11.9556\n",
      "Epoch 1464/1500\n",
      "36/36 [==============================] - 0s 362us/step - loss: 1.4716 - val_loss: 9.6855\n",
      "Epoch 1465/1500\n",
      "36/36 [==============================] - 0s 381us/step - loss: 2.7029 - val_loss: 13.9158\n",
      "Epoch 1466/1500\n",
      "36/36 [==============================] - 0s 364us/step - loss: 2.3456 - val_loss: 12.9564\n",
      "Epoch 1467/1500\n",
      "36/36 [==============================] - 0s 347us/step - loss: 2.0562 - val_loss: 6.6155\n",
      "Epoch 1468/1500\n",
      "36/36 [==============================] - 0s 380us/step - loss: 2.8501 - val_loss: 10.4896\n",
      "Epoch 1469/1500\n",
      "36/36 [==============================] - 0s 382us/step - loss: 2.1886 - val_loss: 12.9239\n",
      "Epoch 1470/1500\n",
      "36/36 [==============================] - 0s 382us/step - loss: 3.0255 - val_loss: 11.4485\n",
      "Epoch 1471/1500\n",
      "36/36 [==============================] - 0s 324us/step - loss: 2.2451 - val_loss: 8.0163\n",
      "Epoch 1472/1500\n",
      "36/36 [==============================] - 0s 320us/step - loss: 2.4165 - val_loss: 6.5272\n",
      "Epoch 1473/1500\n",
      "36/36 [==============================] - 0s 373us/step - loss: 2.0413 - val_loss: 9.3238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1474/1500\n",
      "36/36 [==============================] - ETA: 0s - loss: 2.575 - 0s 392us/step - loss: 2.5114 - val_loss: 6.2048\n",
      "Epoch 1475/1500\n",
      "36/36 [==============================] - 0s 459us/step - loss: 2.3698 - val_loss: 7.0281\n",
      "Epoch 1476/1500\n",
      "36/36 [==============================] - 0s 451us/step - loss: 2.0669 - val_loss: 11.0622\n",
      "Epoch 1477/1500\n",
      "36/36 [==============================] - 0s 525us/step - loss: 1.6838 - val_loss: 9.4519\n",
      "Epoch 1478/1500\n",
      "36/36 [==============================] - 0s 524us/step - loss: 1.3582 - val_loss: 11.5363\n",
      "Epoch 1479/1500\n",
      "36/36 [==============================] - 0s 534us/step - loss: 1.2035 - val_loss: 9.5767\n",
      "Epoch 1480/1500\n",
      "36/36 [==============================] - 0s 431us/step - loss: 1.2816 - val_loss: 8.5457\n",
      "Epoch 1481/1500\n",
      "36/36 [==============================] - 0s 539us/step - loss: 1.1384 - val_loss: 11.3211\n",
      "Epoch 1482/1500\n",
      "36/36 [==============================] - 0s 427us/step - loss: 3.2327 - val_loss: 9.9597\n",
      "Epoch 1483/1500\n",
      "36/36 [==============================] - 0s 513us/step - loss: 1.6938 - val_loss: 9.6032\n",
      "Epoch 1484/1500\n",
      "36/36 [==============================] - 0s 584us/step - loss: 2.5869 - val_loss: 11.9115\n",
      "Epoch 1485/1500\n",
      "36/36 [==============================] - 0s 424us/step - loss: 1.2631 - val_loss: 13.5728\n",
      "Epoch 1486/1500\n",
      "36/36 [==============================] - 0s 391us/step - loss: 1.4600 - val_loss: 12.5542\n",
      "Epoch 1487/1500\n",
      "36/36 [==============================] - 0s 305us/step - loss: 1.0327 - val_loss: 13.4431\n",
      "Epoch 1488/1500\n",
      "36/36 [==============================] - 0s 345us/step - loss: 1.2070 - val_loss: 12.3793\n",
      "Epoch 1489/1500\n",
      "36/36 [==============================] - 0s 338us/step - loss: 1.0186 - val_loss: 11.2267\n",
      "Epoch 1490/1500\n",
      "36/36 [==============================] - 0s 610us/step - loss: 1.4645 - val_loss: 12.6967\n",
      "Epoch 1491/1500\n",
      "36/36 [==============================] - ETA: 0s - loss: 1.063 - 0s 487us/step - loss: 1.0902 - val_loss: 12.1005\n",
      "Epoch 1492/1500\n",
      "36/36 [==============================] - 0s 388us/step - loss: 1.3183 - val_loss: 12.5376\n",
      "Epoch 1493/1500\n",
      "36/36 [==============================] - 0s 505us/step - loss: 0.8833 - val_loss: 11.9114\n",
      "Epoch 1494/1500\n",
      "36/36 [==============================] - 0s 366us/step - loss: 0.8547 - val_loss: 7.7916\n",
      "Epoch 1495/1500\n",
      "36/36 [==============================] - 0s 349us/step - loss: 3.4294 - val_loss: 6.8860\n",
      "Epoch 1496/1500\n",
      "36/36 [==============================] - 0s 353us/step - loss: 2.3497 - val_loss: 12.2152\n",
      "Epoch 1497/1500\n",
      "36/36 [==============================] - 0s 464us/step - loss: 2.8917 - val_loss: 10.8402\n",
      "Epoch 1498/1500\n",
      "36/36 [==============================] - 0s 406us/step - loss: 1.3200 - val_loss: 9.6409\n",
      "Epoch 1499/1500\n",
      "36/36 [==============================] - 0s 589us/step - loss: 2.2451 - val_loss: 16.7959\n",
      "Epoch 1500/1500\n",
      "36/36 [==============================] - 0s 462us/step - loss: 2.6306 - val_loss: 16.3812\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=1500, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos predicciones del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 70008.81],\n",
       "       [100638.24]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Iran</th>\n",
       "      <td>64586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>China</th>\n",
       "      <td>80921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          50\n",
       "Iran   64586\n",
       "China  80921"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_prediction_namelist = ['Mexico']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = open('database_prediction.csv','w')\n",
    "\n",
    "for value in range(dias_a_usar):\n",
    "    print(value+1,file=db,end = ',')\n",
    "print(dia_a_predecir,file=db)\n",
    "\n",
    "for country in country_prediction_namelist:\n",
    "    \n",
    "    f = open('database_confirmed.csv')\n",
    "    infected = []\n",
    "    dia_inicio = 0\n",
    "    for line in f:\n",
    "        if(country == line.split(',')[1]):\n",
    "            lat = float(line.split(',')[2])\n",
    "            long = float(line.split(',')[3])\n",
    "            country_data = []\n",
    "            data = line.replace('\\n','').split(',')[4:]\n",
    "            for number in data:\n",
    "                if(number != '0'):\n",
    "                    infected.append(int(number))\n",
    "                else:\n",
    "                    dia_inicio += 1                    \n",
    "    f.close()\n",
    "\n",
    "    if(len(infected)<dias_a_usar):\n",
    "        continue    \n",
    "    print(country,end=',',file=db,sep=',')\n",
    "    for value in infected[:dias_a_usar]:\n",
    "        print(value,file=db,end = ',')\n",
    "    if(len(infected)>=dia_a_predecir):\n",
    "        print(infected[dia_a_predecir-1],file=db)\n",
    "    else:\n",
    "        print(-1,file=db)\n",
    "db.close()          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prediction = pd.read_csv(\"database_prediction.csv\",sep=',') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mexico</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>2785</td>\n",
       "      <td>3181</td>\n",
       "      <td>3441</td>\n",
       "      <td>3844</td>\n",
       "      <td>4219</td>\n",
       "      <td>4661</td>\n",
       "      <td>5014</td>\n",
       "      <td>5399</td>\n",
       "      <td>5847</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        1  2  3  4  5  6  7  8  9  10  ...    41    42    43    44    45  \\\n",
       "Mexico  1  4  5  5  5  5  5  6  6   7  ...  2785  3181  3441  3844  4219   \n",
       "\n",
       "          46    47    48    49  50  \n",
       "Mexico  4661  5014  5399  5847  -1  \n",
       "\n",
       "[1 rows x 50 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 49) (1, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_prediction = pd.DataFrame()\n",
    "for i in range(1,dias_a_usar+1):\n",
    "    X_prediction[str(i)] = data_prediction[str(i)]\n",
    "Y_prediction = pd.DataFrame()\n",
    "Y_prediction[str(dia_a_predecir)] = data_prediction[str(dia_a_predecir)]\n",
    "print(X_prediction.shape, Y_prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6502.965]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
